{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "url": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "title": "LLaMA&混合专家模型",
            "date_published": "2025-05-19T10:50:11.000Z",
            "content_html": "<h2 id=\"LLaMA-model-structure\"><a href=\"#LLaMA-model-structure\" class=\"headerlink\" title=\"LLaMA  model  structure\"></a>LLaMA  model  structure</h2><p>​\tLLaMA是基于transformer的decoder部分构建的，采用前置层归一化、使用RMSNorm规划函数，激活函数更改为SwiGLU，使用旋转位置嵌入更改的decoder模型。更改的位置如下所示：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ul>\n<li><p>RMSNorm函数：</p>\n<p><img src=\"/image2.png\" alt=\"image2\"></p>\n<p>原始层归一化函数：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><strong>对比</strong></th>\n<th align=\"center\"><strong>LayerNorm</strong></th>\n<th align=\"center\"><strong>RMSNorm</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>归一化目标</strong></td>\n<td align=\"center\">均值中心化 + 方差缩放</td>\n<td align=\"center\">仅均方根（RMS）缩放</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>计算复杂度</strong></td>\n<td align=\"center\">较高（需计算均值和方差）</td>\n<td align=\"center\">较低（仅需均方值）</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>参数数量</strong></td>\n<td align=\"center\"><em>γ</em>+<em>β</em>（2d 参数）</td>\n<td align=\"center\">仅 <em>γ</em>（d 参数）</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>SwiGLU</p>\n<p>SwiGLU是门控线性单元（GLU）的变体，公式如下：<br><img src=\"/image4.png\" alt=\"image2\"></p>\n<p><img src=\"/image5.png\" alt=\"image2\"></p>\n<p>第二个公式的激活函数是sigmoid，sigmoid函数特点：</p>\n</li>\n</ul>\n<p><img src=\"/image6.png\" alt=\"image2\"></p>\n<p>当β趋向于0时，相当于y&#x3D;x&#x2F;2，线性函数，当β趋向于无穷时（x&gt;0,x&lt;0,x&#x3D;0)，相当于ReLU函数，当β&#x3D;1，swish光滑且非单调。</p>\n<p>Swish(xW)为门控权重（相当于选择遗忘比例），用权重对xV逐元素加权，用W2映射回原维度。</p>\n<ul>\n<li><p>RoPE（待）</p>\n<p>传统PE，model需要学习隐式位置关系，而RoPE通过旋转矩阵直接编码位置，即将位置信息通过旋转矩阵融合key\\query向量中，直接建模相对位置依赖关系，value是不需要旋转的:</p>\n<p><img src=\"/image7.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<p>以上三个改变与原decoder结合实现了LLaMA</p>\n<h2 id=\"注意力机制优化\"><a href=\"#注意力机制优化\" class=\"headerlink\" title=\"注意力机制优化\"></a>注意力机制优化</h2><p>在 Transformer 结构中，自注意力机制的时间和存储复杂度与序列的长度呈平方的关系，因此占用了大量的计算设备内存并消耗了大量的计算资源，比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>: Q K </span><br><span class=\"line\">Q = [](seq_len*vector_len)</span><br><span class=\"line\">K = [](seq_len*vector_len)</span><br><span class=\"line\">Q*K(转置) = seq_len * seq_len</span><br></pre></td></tr></table></figure>\n\n<p>那么就有这种情况：</p>\n<ul>\n<li><strong>显存占用</strong>：</li>\n<li><strong>计算时间</strong>：<br>每次注意力计算需 seq_len方 * d 次操作，<em>seq_len&#x3D;32</em>k*, <em>d</em>&#x3D;1024 时约为 10的12 次方次操作。</li>\n</ul>\n<p>所以需要方法去优化这一问题，以下举例两种方法：</p>\n<ul>\n<li><p>稀疏注意力</p>\n<ul>\n<li>全局注意力：在稀疏注意力中保留少量全局节点（如 [CLS] token 或特定位置），这些节点可以与序列中<strong>所有其他位置</strong>交互。</li>\n<li>带状注意力：每个 Query 只与<strong>固定宽度邻域内</strong>的 Key 交互（类似对角带状矩阵）</li>\n<li>膨胀注意力：以<strong>固定间隔跳跃采样</strong> Key</li>\n<li>随机注意力：每个 Query 随机选择 <em>r</em> 个位置进行交互</li>\n<li>局部块注意力：多个不重叠块交互</li>\n</ul>\n<p>一般利用上述的复合模式s</p>\n</li>\n<li><p>低秩注意力:</p>\n<p><img src=\"/image8.jpg\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"混合专家模型\"><a href=\"#混合专家模型\" class=\"headerlink\" title=\"混合专家模型\"></a>混合专家模型</h2><p>​\t混合专家模型 (MixedExpert Models，MoEs) 日益受到关注。依据大模型缩放法则，模型规模是提升性能的关键，然而规模扩大必然使计算资源大幅增加。因此，在有限计算资源预算下，如何用更少训练步数训练更大模型成为关键问题。为解决该问题，混合专家模型基于一个简洁的思想：模型不同部分（即“专家”）专注不同任务或数据层面。</p>\n<p>其实就是把模型内部的一组专用子网络，每个子网络负责处理数据中特定类型的任务，如：</p>\n<ul>\n<li>输入句子是数学问题 → 激活“数学专家”</li>\n<li>输入是诗歌 → 激活“文学专家”</li>\n</ul>\n<p>优势：若model有100个专家，每次输入仅用2个，计算量少，而且每个专家通过训练集中学习特定模式，比通用模块更高效。</p>\n<p>混合专家模型按照门控网络（Gate）类型，可以从广义上讲可以分为三个大类：稀疏混合专家模型（Sparse MoE）、稠密混合专家模型（Dense MoE）、软混合专家模型（Soft MoE）：</p>\n<ul>\n<li><p>稀疏混合专家模型：input之后，门控网络仅激活少数专家</p>\n</li>\n<li><p>稠密混合专家模型：所有专家激活，甲醛组合输出，这个是要计算每个wi,bi与x的结果</p>\n</li>\n<li><p>软混合专家模型：门控网络分配的权重直接融合不同专家的参数，得到w(融合)，b(融合)，融合与x计算&#x3D;w*x+b</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><strong>类型</strong></th>\n<th align=\"center\"><strong>专家激活方式</strong></th>\n<th align=\"center\"><strong>计算量</strong></th>\n<th align=\"center\"><strong>参数量扩展性</strong></th>\n<th align=\"center\"><strong>典型场景</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>稀疏MoE</strong></td>\n<td align=\"center\">硬性Top-k选择（如k&#x3D;2）</td>\n<td align=\"center\"><em>O</em>(<em>k</em>⋅FFN)</td>\n<td align=\"center\">极高（万亿级）</td>\n<td align=\"center\">大规模预训练（Mixtral, GPT-4）</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>稠密MoE</strong></td>\n<td align=\"center\">所有专家加权求和</td>\n<td align=\"center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td align=\"center\">低（十亿级）</td>\n<td align=\"center\">小规模多任务学习</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>软MoE</strong></td>\n<td align=\"center\">软性稀疏权重</td>\n<td align=\"center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td align=\"center\">中（百亿级）</td>\n<td align=\"center\">平衡效率与稳定性需求</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n",
            "tags": [
                "混合专家模型",
                "LLaMA"
            ]
        },
        {
            "id": "http://example.com/2025/05/17/GPT-BertBuild/",
            "url": "http://example.com/2025/05/17/GPT-BertBuild/",
            "title": "GPT&BertBuild",
            "date_published": "2025-05-16T16:37:02.000Z",
            "content_html": "<h2 id=\"生成式预训练语言模型GPT\"><a href=\"#生成式预训练语言模型GPT\" class=\"headerlink\" title=\"生成式预训练语言模型GPT\"></a>生成式预训练语言模型GPT</h2><p>GPT的模型结构，由多个transformer的decoder组成，结构如下：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<p>该模型利用的是transformer的解码器部分，训练和推理的过程是类似的，12层transformer模块都在做类似的事情，到最后一层后输出预测的分数（映射到词分类中，得到预测的分数&#x2F;置信度）</p>\n<ul>\n<li><p>有监督下游任务微调：</p>\n<p>在进行下游任务微调时，仅使用GPT的最后一层的最后一个词的隐藏状态，同个全连接层映射到标签空间。因为每个词的隐藏状态都聚合了左侧所有历史词的信息，因此最后一个词的隐藏状态天然编码了整个序列的全局语义。</p>\n<p>同时再进行微调的时候，可能会出现模型遗忘预训练阶段学习的通用只是表示，损失通用与泛化能力，出现灾难性遗忘的问题，所以通常采用混合预训练任务损失和下游微调损失缓解这一问题，Loss Function如下:<br><img src=\"/image2.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"Bert模型构建\"><a href=\"#Bert模型构建\" class=\"headerlink\" title=\"Bert模型构建\"></a>Bert模型构建</h2><p>Bert模型一般是基于Transformer的编码器部分构建的，是一个相当于完形填空(mask)的模型，是考虑双向的模型（同时利用两侧信息）</p>\n<p>例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>:the cat site on the mat</span><br><span class=\"line\">tokenizer(分词):（假设不加入CLS标记和SEP标记）</span><br><span class=\"line\">    code:[<span class=\"number\">1996</span>,<span class=\"number\">4248</span>,<span class=\"number\">2825</span>,<span class=\"number\">2007</span>,<span class=\"number\">1996</span>,<span class=\"number\">3829</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>] <span class=\"comment\"># 0为pad的编码，max假设为8</span></span><br><span class=\"line\">    attention_mask:[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>] <span class=\"comment\"># 0表示填充的位置</span></span><br><span class=\"line\">random mask:选取到了site作为掩码，替换为mask的编码-&gt;<span class=\"number\">103</span></span><br><span class=\"line\">\tcode:[<span class=\"number\">1996</span>,<span class=\"number\">4248</span>,<span class=\"number\">103</span>,<span class=\"number\">2007</span>,<span class=\"number\">1996</span>,<span class=\"number\">3829</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    attention_mask:[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    label:[-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,<span class=\"number\">2825</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>] <span class=\"comment\">#这个是用于计算掩码的真实标签，-100表示不需要计算，只保留被掩码的位置</span></span><br><span class=\"line\">train:</span><br><span class=\"line\">    将上述数据送入Bert模型中</span><br><span class=\"line\">output:</span><br><span class=\"line\">    [</span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        [..,..,..,..,], <span class=\"comment\">#取第二个位置进行计算交叉熵</span></span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        ........</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>训练过程：加载dataset,切分traindataset和testdataset –&gt; 用traindataset训练词元分析器，形成vocab词表 –&gt; 加载预训练词元分析器 –&gt; 分词序列 –&gt; 加载随即权重的BERT模型，设置掩码比例，进行训练，下面是代码训练过程：</p>\n<p>dataset获取与镜像设置：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#下载数据集：</span></span><br><span class=\"line\">pip install huggingface_hub</span><br><span class=\"line\">huggingface-cli download --repo-<span class=\"built_in\">type</span> dataset legacy-datasets/wikipedia  --local-<span class=\"built_in\">dir</span> wikipedia</span><br><span class=\"line\"><span class=\"comment\"># mirror</span></span><br><span class=\"line\">export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>\n\n<p>environment(有部分可能不需要）:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">dependencies:</span><br><span class=\"line\">  - _libgcc_mutex=<span class=\"number\">0.1</span>=main</span><br><span class=\"line\">  - _openmp_mutex=<span class=\"number\">5.1</span>=1_gnu</span><br><span class=\"line\">  - bzip2=<span class=\"number\">1.0</span><span class=\"number\">.8</span>=h5eee18b_6</span><br><span class=\"line\">  - ca-certificates=<span class=\"number\">2025.2</span><span class=\"number\">.25</span>=h06a4308_0</span><br><span class=\"line\">  - expat=<span class=\"number\">2.7</span><span class=\"number\">.1</span>=h6a678d5_0</span><br><span class=\"line\">  - ld_impl_linux-<span class=\"number\">64</span>=<span class=\"number\">2.40</span>=h12ee557_0</span><br><span class=\"line\">  - libffi=<span class=\"number\">3.4</span><span class=\"number\">.4</span>=h6a678d5_1</span><br><span class=\"line\">  - libgcc-ng=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libgomp=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libstdcxx-ng=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libuuid=<span class=\"number\">1.41</span><span class=\"number\">.5</span>=h5eee18b_0</span><br><span class=\"line\">  - ncurses=<span class=\"number\">6.4</span>=h6a678d5_0</span><br><span class=\"line\">  - openssl=<span class=\"number\">3.0</span><span class=\"number\">.16</span>=h5eee18b_0</span><br><span class=\"line\">  - pip=<span class=\"number\">25.1</span>=pyhc872135_2</span><br><span class=\"line\">  - python=<span class=\"number\">3.12</span><span class=\"number\">.9</span>=h5148396_0</span><br><span class=\"line\">  - readline=<span class=\"number\">8.2</span>=h5eee18b_0</span><br><span class=\"line\">  - setuptools=<span class=\"number\">78.1</span><span class=\"number\">.1</span>=py312h06a4308_0</span><br><span class=\"line\">  - sqlite=<span class=\"number\">3.45</span><span class=\"number\">.3</span>=h5eee18b_0</span><br><span class=\"line\">  - tk=<span class=\"number\">8.6</span><span class=\"number\">.14</span>=h39e8969_0</span><br><span class=\"line\">  - wheel=<span class=\"number\">0.45</span><span class=\"number\">.1</span>=py312h06a4308_0</span><br><span class=\"line\">  - xz=<span class=\"number\">5.6</span><span class=\"number\">.4</span>=h5eee18b_1</span><br><span class=\"line\">  - zlib=<span class=\"number\">1.2</span><span class=\"number\">.13</span>=h5eee18b_1</span><br><span class=\"line\">  - pip:</span><br><span class=\"line\">      - accelerate==<span class=\"number\">0.26</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - aiohappyeyeballs==<span class=\"number\">2.6</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - aiohttp==<span class=\"number\">3.11</span><span class=\"number\">.18</span></span><br><span class=\"line\">      - aiosignal==<span class=\"number\">1.3</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - attrs==<span class=\"number\">25.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - certifi==<span class=\"number\">2025.4</span><span class=\"number\">.26</span></span><br><span class=\"line\">      - charset-normalizer==<span class=\"number\">3.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - datasets==<span class=\"number\">3.6</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - dill==<span class=\"number\">0.3</span><span class=\"number\">.8</span></span><br><span class=\"line\">      - filelock==<span class=\"number\">3.18</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - frozenlist==<span class=\"number\">1.6</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - fsspec==<span class=\"number\">2025.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - huggingface-hub==<span class=\"number\">0.31</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - idna==<span class=\"number\">3.10</span></span><br><span class=\"line\">      - jinja2==<span class=\"number\">3.1</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - markupsafe==<span class=\"number\">3.0</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - mpmath==<span class=\"number\">1.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - multidict==<span class=\"number\">6.4</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - multiprocess==<span class=\"number\">0.70</span><span class=\"number\">.16</span></span><br><span class=\"line\">      - mwparserfromhell==<span class=\"number\">0.6</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - networkx==<span class=\"number\">3.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - numpy==<span class=\"number\">2.2</span><span class=\"number\">.5</span></span><br><span class=\"line\">      - nvidia-cublas-cu12==<span class=\"number\">12.6</span><span class=\"number\">.4</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - nvidia-cuda-cupti-cu12==<span class=\"number\">12.6</span><span class=\"number\">.80</span></span><br><span class=\"line\">      - nvidia-cuda-nvrtc-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cuda-runtime-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cudnn-cu12==<span class=\"number\">9.5</span><span class=\"number\">.1</span><span class=\"number\">.17</span></span><br><span class=\"line\">      - nvidia-cufft-cu12==<span class=\"number\">11.3</span><span class=\"number\">.0</span><span class=\"number\">.4</span></span><br><span class=\"line\">      - nvidia-cufile-cu12==<span class=\"number\">1.11</span><span class=\"number\">.1</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - nvidia-curand-cu12==<span class=\"number\">10.3</span><span class=\"number\">.7</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cusolver-cu12==<span class=\"number\">11.7</span><span class=\"number\">.1</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-cusparse-cu12==<span class=\"number\">12.5</span><span class=\"number\">.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-cusparselt-cu12==<span class=\"number\">0.6</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - nvidia-nccl-cu12==<span class=\"number\">2.26</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-nvjitlink-cu12==<span class=\"number\">12.6</span><span class=\"number\">.85</span></span><br><span class=\"line\">      - nvidia-nvtx-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - packaging==<span class=\"number\">25.0</span></span><br><span class=\"line\">      - pandas==<span class=\"number\">2.2</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - propcache==<span class=\"number\">0.3</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - psutil==<span class=\"number\">7.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - pyarrow==<span class=\"number\">20.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - python-dateutil==<span class=\"number\">2.9</span><span class=\"number\">.0</span>.post0</span><br><span class=\"line\">      - pytz==<span class=\"number\">2025.2</span></span><br><span class=\"line\">      - pyyaml==<span class=\"number\">6.0</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - regex==<span class=\"number\">2024.11</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - requests==<span class=\"number\">2.32</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - safetensors==<span class=\"number\">0.5</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - six==<span class=\"number\">1.17</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - sympy==<span class=\"number\">1.14</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - tokenizers==<span class=\"number\">0.21</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - torch==<span class=\"number\">2.7</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - tqdm==<span class=\"number\">4.67</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - transformers==<span class=\"number\">4.51</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - triton==<span class=\"number\">3.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - typing-extensions==<span class=\"number\">4.13</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - tzdata==<span class=\"number\">2025.2</span></span><br><span class=\"line\">      - urllib3==<span class=\"number\">2.4</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - xxhash==<span class=\"number\">3.5</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - yarl==<span class=\"number\">1.20</span><span class=\"number\">.0</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> BertTokenizerFast,BertConfig,BertForMaskedLM,DataCollatorForLanguageModeling,TrainingArguments,Trainer</span><br><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> tokenizers <span class=\"keyword\">import</span> BertWordPieceTokenizer</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> chain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class=\"string\">&#x27;2&#x27;</span> <span class=\"comment\"># 这个得加，多gpu跑的话会有问题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载本地数据</span></span><br><span class=\"line\">wiki = load_dataset(<span class=\"string\">&quot;/data/hcfeng/learnLLM/wikipedia&quot;</span>, <span class=\"string\">&quot;20220301.en&quot;</span>, split=<span class=\"string\">&quot;train&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#仅保留text列</span></span><br><span class=\"line\">wiki = wiki.remove_columns([col <span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> wiki.column_names <span class=\"keyword\">if</span> col!=<span class=\"string\">&quot;text&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\">#切割数据</span></span><br><span class=\"line\">dataset = wiki</span><br><span class=\"line\">d = dataset.train_test_split(train_size=<span class=\"number\">0.9</span>,test_size=<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dataset_to_text</span>(<span class=\"params\">dataset,filename</span>):</span><br><span class=\"line\">    outpur_path=<span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/&quot;</span>+filename <span class=\"comment\">#拼接地址</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(outpur_path,<span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> dataset[<span class=\"string\">&#x27;text&#x27;</span>]:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(t,file=f)</span><br><span class=\"line\">dataset_to_text(d[<span class=\"string\">&#x27;train&#x27;</span>],<span class=\"string\">&#x27;train.txt&#x27;</span>)</span><br><span class=\"line\">dataset_to_text(d[<span class=\"string\">&#x27;test&#x27;</span>],<span class=\"string\">&#x27;test.txt&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">special_tokens = [</span><br><span class=\"line\"><span class=\"string\">&quot;[PAD]&quot;</span>, <span class=\"string\">&quot;[UNK]&quot;</span>, <span class=\"string\">&quot;[CLS]&quot;</span>, <span class=\"string\">&quot;[SEP]&quot;</span>, <span class=\"string\">&quot;[MASK]&quot;</span>, <span class=\"string\">&quot;&lt;S&gt;&quot;</span>, <span class=\"string\">&quot;&lt;T&gt;&quot;</span></span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果根据训练和测试两个集合训练词元分析器，则需要修改files</span></span><br><span class=\"line\"><span class=\"comment\"># files = [&quot;train.txt&quot;, &quot;test.txt&quot;]</span></span><br><span class=\"line\"><span class=\"comment\"># 仅根据训练集合训练词元分析器</span></span><br><span class=\"line\">files = [<span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/train.txt&quot;</span>]</span><br><span class=\"line\"><span class=\"comment\"># BERT中采用的默认词表大小为30522，可以随意修改</span></span><br><span class=\"line\">vocab_size = <span class=\"number\">30522</span></span><br><span class=\"line\"><span class=\"comment\"># 最大序列长度，该值越小，训练速度越快</span></span><br><span class=\"line\">max_length = <span class=\"number\">512</span></span><br><span class=\"line\"><span class=\"comment\"># 是否将长样本截断</span></span><br><span class=\"line\">truncate_longer_samples = <span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"comment\"># 初始化WordPiece词元分析器</span></span><br><span class=\"line\">tokenizer = BertWordPieceTokenizer()</span><br><span class=\"line\"><span class=\"comment\"># 训练词元分析器，设定的 vocab_size 是最大允许值，但实际生成的词表大小可能更小</span></span><br><span class=\"line\">tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens) </span><br><span class=\"line\"><span class=\"comment\"># 允许截断达到最大512个词元</span></span><br><span class=\"line\">tokenizer.enable_truncation(max_length=max_length)</span><br><span class=\"line\">model_path = <span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果文件夹不存在，则先创建文件夹</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.isdir(model_path):</span><br><span class=\"line\">    os.mkdir(model_path)</span><br><span class=\"line\"><span class=\"comment\"># 保存词元分析器模型</span></span><br><span class=\"line\">tokenizer.save_model(model_path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将一些词元分析器中的配置保存到配置文件，包括特殊词元、转换为小写、最大序列长度等</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(os.path.join(model_path, <span class=\"string\">&quot;config.json&quot;</span>), <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    tokenizer_cfg = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;do_lower_case&quot;</span>: <span class=\"literal\">True</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;unk_token&quot;</span>: <span class=\"string\">&quot;[UNK]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;sep_token&quot;</span>: <span class=\"string\">&quot;[SEP]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;pad_token&quot;</span>: <span class=\"string\">&quot;[PAD]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;cls_token&quot;</span>: <span class=\"string\">&quot;[CLS]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;mask_token&quot;</span>: <span class=\"string\">&quot;[MASK]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;model_max_length&quot;</span>: max_length,</span><br><span class=\"line\">    <span class=\"string\">&quot;max_len&quot;</span>: max_length,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    json.dump(tokenizer_cfg, f)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tokenizer处理序列会自动添加CLS和SEP，但是tokenizer.tokenize()只会进行基础分词，不会添加特殊分词</span></span><br><span class=\"line\">tokenizer = BertTokenizerFast.from_pretrained(<span class=\"string\">&#x27;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;tokenier:<span class=\"subst\">&#123;tokenizer&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">encode_with_truncation</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; 使用词元分析对句子进行处理并截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], truncation=<span class=\"literal\">True</span>, padding=<span class=\"string\">&quot;max_length&quot;</span>,max_length=max_length, return_special_tokens_mask=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">encode_without_truncation</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; 使用词元分析对句子进行处理且不截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], return_special_tokens_mask=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 编码函数将依赖于truncate_longer_samples变量</span></span><br><span class=\"line\">encode = encode_with_truncation <span class=\"keyword\">if</span> truncate_longer_samples <span class=\"keyword\">else</span> encode_without_truncation</span><br><span class=\"line\"><span class=\"comment\"># 对训练数据集进行分词处理</span></span><br><span class=\"line\">train_dataset = d[<span class=\"string\">&quot;train&quot;</span>].<span class=\"built_in\">map</span>(encode, batched=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># 对测试数据集进行分词处理</span></span><br><span class=\"line\">test_dataset = d[<span class=\"string\">&quot;test&quot;</span>].<span class=\"built_in\">map</span>(encode, batched=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> truncate_longer_samples:</span><br><span class=\"line\">    <span class=\"comment\"># 移除其他列，将input_ids和attention_mask设置为PyTorch张量</span></span><br><span class=\"line\">    train_dataset.set_format(<span class=\"built_in\">type</span>=<span class=\"string\">&quot;torch&quot;</span>, columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>])</span><br><span class=\"line\">    test_dataset.set_format(<span class=\"built_in\">type</span>=<span class=\"string\">&quot;torch&quot;</span>, columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>])</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 移除其他列，将它们保留为Python列表</span></span><br><span class=\"line\">    test_dataset.set_format(columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>, <span class=\"string\">&quot;special_tokens_mask&quot;</span>])</span><br><span class=\"line\">    train_dataset.set_format(columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>, <span class=\"string\">&quot;special_tokens_mask&quot;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用于不截断的情况</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">group_texts</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 拼接所有文本</span></span><br><span class=\"line\">    concatenated_examples = &#123;k: <span class=\"built_in\">list</span>(chain(*examples[k])) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> examples.keys()&#125;</span><br><span class=\"line\">    total_length = <span class=\"built_in\">len</span>(concatenated_examples[<span class=\"built_in\">list</span>(examples.keys())[<span class=\"number\">0</span>]])</span><br><span class=\"line\">    <span class=\"comment\"># 舍弃了剩余部分，如果模型支持填充而不是舍弃，则可以根据需要自定义这部分</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> total_length &gt;= max_length:</span><br><span class=\"line\">        total_length = (total_length // max_length) * max_length</span><br><span class=\"line\">    <span class=\"comment\"># 按照最大长度分割成块</span></span><br><span class=\"line\">    result = &#123;</span><br><span class=\"line\">    k: [t[i : i + max_length] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, total_length, max_length)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k, t <span class=\"keyword\">in</span> concatenated_examples.items()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> truncate_longer_samples:</span><br><span class=\"line\">    train_dataset = train_dataset.<span class=\"built_in\">map</span>(group_texts, batched=<span class=\"literal\">True</span>,desc=<span class=\"string\">f&quot;Grouping texts in chunks of <span class=\"subst\">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class=\"line\">    test_dataset = test_dataset.<span class=\"built_in\">map</span>(group_texts, batched=<span class=\"literal\">True</span>,desc=<span class=\"string\">f&quot;Grouping texts in chunks of <span class=\"subst\">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 将它们从列表转换为PyTorch张量</span></span><br><span class=\"line\">    train_dataset.set_format(<span class=\"string\">&quot;torch&quot;</span>)</span><br><span class=\"line\">    test_dataset.set_format(<span class=\"string\">&quot;torch&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;tokenizer.vocab_size:<span class=\"subst\">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#这里vocab_size一般设为tokenizer.vocab_size，其实只要大于这个数字都是可以的，只是会占用了显存空间（training)，但是不能设小，会出现索引越界的问题</span></span><br><span class=\"line\">model_config = BertConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=max_length) </span><br><span class=\"line\">model = BertForMaskedLM(config=model_config)</span><br><span class=\"line\"></span><br><span class=\"line\">data_collator = DataCollatorForLanguageModeling(</span><br><span class=\"line\">    tokenizer=tokenizer, mlm=<span class=\"literal\">True</span>, mlm_probability=<span class=\"number\">0.2</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">training_args = TrainingArguments(</span><br><span class=\"line\">    output_dir=model_path, <span class=\"comment\"># 输出目录，用于保存模型检查点</span></span><br><span class=\"line\">    eval_strategy=<span class=\"string\">&quot;steps&quot;</span>, <span class=\"comment\"># 每隔`logging_steps`步进行一次评估</span></span><br><span class=\"line\">    overwrite_output_dir=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    num_train_epochs=<span class=\"number\">100</span>, <span class=\"comment\"># 训练时的轮数，可以根据需要进行调整</span></span><br><span class=\"line\">    per_device_train_batch_size=<span class=\"number\">10</span>, <span class=\"comment\"># 训练批量大小，可以根据GPU内存容量将其设置得尽可能大</span></span><br><span class=\"line\">    gradient_accumulation_steps=<span class=\"number\">8</span>, <span class=\"comment\"># 在更新权重之前累积梯度</span></span><br><span class=\"line\">    per_device_eval_batch_size=<span class=\"number\">64</span>, <span class=\"comment\"># 评估批量大小</span></span><br><span class=\"line\">    logging_steps=<span class=\"number\">1000</span>, <span class=\"comment\"># 每隔1000步进行一次评估，记录并保存模型检查点</span></span><br><span class=\"line\">    save_steps=<span class=\"number\">1000</span>,</span><br><span class=\"line\">    <span class=\"comment\"># load_best_model_at_end=True, # 是否在训练结束时加载最佳模型（根据损失）</span></span><br><span class=\"line\">    <span class=\"comment\"># save_total_limit=3, # 如果磁盘空间有限，则可以限制只保存3个模型权重</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    model=model,</span><br><span class=\"line\">    args=training_args,</span><br><span class=\"line\">    data_collator=data_collator,</span><br><span class=\"line\">    train_dataset=train_dataset,</span><br><span class=\"line\">    eval_dataset=test_dataset,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#检查点</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids dtype:&quot;</span>, train_dataset[<span class=\"number\">0</span>][<span class=\"string\">&quot;input_ids&quot;</span>].dtype)  <span class=\"comment\"># 应该是 torch.int64 (long)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;attention_mask dtype:&quot;</span>, train_dataset[<span class=\"number\">0</span>][<span class=\"string\">&quot;attention_mask&quot;</span>].dtype)  <span class=\"comment\"># 应该是 torch.int64 (long)</span></span><br><span class=\"line\">batch = data_collator([train_dataset[<span class=\"number\">0</span>], train_dataset[<span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids shape:&quot;</span>, batch[<span class=\"string\">&quot;input_ids&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;batch[&quot;input_ids&quot;]:<span class=\"subst\">&#123;batch[<span class=\"string\">&quot;input_ids&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;labels shape:&quot;</span>, batch[<span class=\"string\">&quot;labels&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"comment\"># print(&quot;labels min/max:&quot;, torch.min(batch[&quot;labels&quot;]), torch.max(batch[&quot;labels&quot;]))</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids min/max:&quot;</span>, torch.<span class=\"built_in\">min</span>(batch[<span class=\"string\">&quot;input_ids&quot;</span>]), torch.<span class=\"built_in\">max</span>(batch[<span class=\"string\">&quot;input_ids&quot;</span>]))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.__version__)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.version.cuda)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.is_available()) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练模型</span></span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n",
            "tags": [
                "Bert",
                "GPT"
            ]
        },
        {
            "id": "http://example.com/2025/05/13/Transformer/",
            "url": "http://example.com/2025/05/13/Transformer/",
            "title": "Transformer",
            "date_published": "2025-05-13T07:07:29.000Z",
            "content_html": "<h2 id=\"Transformer四层结构\"><a href=\"#Transformer四层结构\" class=\"headerlink\" title=\"Transformer四层结构\"></a>Transformer四层结构</h2><p>Transformer结构：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ol>\n<li><p>嵌入表示层</p>\n<p>Transformer的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：</p>\n<p><img src=\"/image2.png\" alt=\"image1\"></p>\n<p>根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#transformer位置编码</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model,max_seq_len = <span class=\"number\">80</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 根据pos和i创建一个常量PE矩阵</span></span><br><span class=\"line\">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> pos <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>):</span><br><span class=\"line\">                pe[pos, i] = math.sin(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">                pe[pos, i + <span class=\"number\">1</span>] = math.cos(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">        pe = pe.unsqueeze(<span class=\"number\">0</span>) <span class=\"comment\">#形状 (1, seq_len, d_model)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">&#x27;pe&#x27;</span>, pe)   <span class=\"comment\">#将 pe 保存为模型的一部分（不参与梯度更新，但会随模型保存/加载）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"comment\">#x:(batch_size, seq_len, d_model)</span></span><br><span class=\"line\">        seq_len = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = x + <span class=\"variable language_\">self</span>.pe[:,:seq_len].cuda()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>使用正余弦的原因是，函数的范围是[-1，1]与词向量相加不会太偏离原始语义，同时第pos+k个位置的编码是第pos个位置编码的线性组合（根据三角函数和角公式决定），这就蕴含了单词之间的距离信息：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n</li>\n<li><p>自注意力层</p>\n<p>自注意力机制，即自己作为QKV进行计算，但是解码器有两个注意力模块，一个是掩码多头，一个是交叉多头注意力，但是原理其实和下面代码差不多，直接用代码展示比较能说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#transformer多头自注意力机制</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, heads, d_model,dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.q_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.k_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.v_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out = nn.Linear(d_model,d_model)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">q, k, v, d_k, mask = <span class=\"literal\">None</span>, dropout = <span class=\"literal\">None</span> </span>):</span><br><span class=\"line\">        <span class=\"comment\"># 转置k相乘 ​​除以 math.sqrt(d_k)​​ 的操作是缩放点积注意力，防止点积数值过大​</span></span><br><span class=\"line\">        scores = torch.matmul(q,k.transpose(-<span class=\"number\">2</span>,-<span class=\"number\">1</span>)) / math.sqrt(d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">            scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>) <span class=\"comment\">#掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        scores = F.sofmax(scores,dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            scores = dropout(scores)</span><br><span class=\"line\"></span><br><span class=\"line\">        output = torch.matmul(scores,v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        batch_size = q.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 利用线性计算划分成h个头</span></span><br><span class=\"line\">        q = <span class=\"variable language_\">self</span>.q_linear(q).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        k = <span class=\"variable language_\">self</span>.k_linear(k).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        v = <span class=\"variable language_\">self</span>.v_linear(v).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#转置头和seq_len位置</span></span><br><span class=\"line\">        k = k.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        q = q.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        v = v.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        scores = <span class=\"variable language_\">self</span>.attention(q, k, v, <span class=\"variable language_\">self</span>.d_k, mask, <span class=\"variable language_\">self</span>.dropout)</span><br><span class=\"line\">        <span class=\"comment\"># 拼接多头输出并线性变换</span></span><br><span class=\"line\">        concat = scores.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(batch_size, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        output = <span class=\"variable language_\">self</span>.out(concat) </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>前馈层</p>\n<p>接收注意力层的输出，通过带有ReLU的2层全连接网络，第一层会映射到高纬度，因为隐藏层维度的增大有利于提高质量（实验证明）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#前馈层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FeedForward</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff = <span class=\"number\">2038</span>, dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear2 = nn.Linear(d_ff,d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.linear1(x)))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.linear2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>残差连接和归一化</p>\n<p>​    由Transformer结构组成的网络结构通常都非常庞大。编码器和解码器均由很多层基本的Transformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究人员在 Transformer 块中进一步引入了残差连接与层归一化技术，以进一步提升训练的稳定性。具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。</p>\n</li>\n</ol>\n<h2 id=\"解码器与编码器\"><a href=\"#解码器与编码器\" class=\"headerlink\" title=\"解码器与编码器\"></a>解码器与编码器</h2><p>​\t编码器端较容易实现。相比于编码器端，解码器端更复杂。具体来说，解码器的每个 Transformer 块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力部分。这主要是因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息。解码器端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的，因此这一额外增加的掩码是用来掩盖后续的文本信息的，以防模型在训练阶段直接看到后续的文本序列，进而无法得到有效的训练。此外，解码器端额外增加了一个多头交叉注意力模块，使用交叉注意力方法，同时接收来自编码器端的输出和当前 Transformer 块的前一个掩码注意力层的输出。查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。</p>\n<p>​\t解码器端以自回归的方式生成目标语言文本，即在每个时间步 <em>t</em>，根据编码器端输出的源语言文本表示，以及前t <em>−</em> 1 个时刻生成的目标语言文本，生成当前时刻的目标语言单词（以我的理解来说，训练阶段是没有显示时间步概念的，通过<strong>一次性输入完整序列 + 掩码矩阵</strong>，在单次前向传播中并行计算出所有位置的输出，同时利用掩码强制模型行为与自回归一致，而推理时必须显式按时间步生成，因为未来词未知（无法并行））。</p>\n<h2 id=\"以推理生成中文翻译-我爱你-为例：\"><a href=\"#以推理生成中文翻译-我爱你-为例：\" class=\"headerlink\" title=\"以推理生成中文翻译 &quot;我爱你&quot; 为例：\"></a>以推理生成中文翻译 <code>&quot;我爱你&quot;</code> 为例：</h2><table>\n<thead>\n<tr>\n<th align=\"center\">时间步</th>\n<th align=\"center\">图1中对应的模块流程</th>\n<th align=\"center\">具体操作</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><code>t=1</code></td>\n<td align=\"center\">词元嵌入 → 位置编码 → 掩码多头注意力 → 编码器-解码器注意力 → 前馈网络 → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt;</code>，输出 <code>&quot;我&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=2</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我</code>) → 位置编码 → 掩码多头注意力 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我</code>，输出 <code>&quot;爱&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=3</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱</code>) → 位置编码 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱</code>，输出 <code>&quot;你&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=4</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱你</code>) → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱你</code>，输出 <code>&lt;end&gt;</code> 的概率分布，终止生成。</td>\n</tr>\n</tbody></table>\n<p>可参考文章：<a href=\"https://blog.csdn.net/m0_64148253/article/details/140422497\">https://blog.csdn.net/m0_64148253/article/details/140422497</a></p>\n",
            "tags": [
                "LLM",
                "transformer"
            ]
        },
        {
            "id": "http://example.com/2025/05/13/DatalinkX/",
            "url": "http://example.com/2025/05/13/DatalinkX/",
            "title": "DatalinkX",
            "date_published": "2025-05-12T16:35:28.000Z",
            "content_html": "<p>DatalinkX是一个基于Flink的异构数据源流转服务，用来作为数据源之间的数据同步工具，通过抽象异构数据源驱动加借助FlinkX开源组件屏蔽数据源间不同的通信协议，通过页面化配置数据同步任务的方式简化数据库同步流程，并通过Xxl-Job组件实现定时调度。</p>\n<h2 id=\"模块设置\"><a href=\"#模块设置\" class=\"headerlink\" title=\"模块设置\"></a>模块设置</h2><ul>\n<li>业务数据管理模块:负责管理数据源和同步任务的增删改查。包含数据源管理、任务管理等。</li>\n<li>同步任务管理模块:借助xx1-job实现任务调度管理，确保异构数据源的定时同步和实时更新需求，同时支持批量任务管理，提高任务调度的效率。</li>\n<li>RPC通信模块:利用Retrofit2实现分布式服务间的RPC调用，提供高效的数据请求和响应机制支持各模块间的数据传输和服务调用。</li>\n<li>数据源驱动模块:负责异构数据源的驱动配置，兼容多类型数据源的连接、读取和写入操作，为业务数据管理和同步提供支持。</li>\n<li>消息通道模块:采用Redis Stream实现分布式消息队列，保障各模块间的消息通信效率和数据传5.<br>输的可靠性。</li>\n<li>实时刷新前端模块:通过SSE技术实现数据的实时推送，配合Ant Design在前端即时刷新数据提供用户友好的动态体验。</li>\n<li>公共模块:包含系统基础功能、工具方法和通用接口，实现各模块间的共享，提升系统的可维护性和复用性。</li>\n<li>FlinkX模块:负责人规模数据的实时同步、数据清洗和格式转换，支持流式和批量数据的整合处理，为系统提供数据支持。<br>数据清洗与转换模块:基于Flinkx的清洗能力，对不同数据源的数据进行标准化、过滤和转换以确保进入系统的数据一致性。</li>\n</ul>\n<h2 id=\"重要模块\"><a href=\"#重要模块\" class=\"headerlink\" title=\"重要模块\"></a>重要模块</h2><ul>\n<li><h3 id=\"消息队列-SSE实现实时推送\"><a href=\"#消息队列-SSE实现实时推送\" class=\"headerlink\" title=\"消息队列+SSE实现实时推送\"></a>消息队列+SSE实现实时推送</h3><ol>\n<li>首先基于自定义注解@messageHub标注消费者方法，在程序运行初始化完Bean之后，实现对每个Bean进行循环检查，寻找带该注解的方法，找到后将其注册成消费者。</li>\n<li>基于Redis的stream数据结构实现轻量级的消息队列，出于两点原因选择该方式，首先是本项目只需要使用消息队列实现任务状态和任务进度的推送，不需要严格意义上的高可用、低延迟，其次是RabbitMQ或者是Kafka服务的复杂性，一个健壮的项目并不是引入越多的中间件会越好，而是Bug越少越好，或者说出Bug概率越低越好。而Redis的stream借助了很多Kafka的设计思路，而且更轻量级。</li>\n<li>消费者的注册，其实是非常简单的一件事，只需要带有@messageHub的，就创建一个线程监听注解中配置的消息地址，监听到消息后回调给该消费者，并返回ACK标记消息已被消费，如果在消费过程中出现故障，则会继续在pending队列中消费，一般会重试4次，如果消费不了，只能是放弃，因为这并不是一个十分重要的操作，丢失部分进度消息是用户能够接收的。</li>\n<li>基于SSE实例实现消息实时推送，SSE是使用流信息向浏览器进行信息推送，也就是信息不是一次性的，而是连续不断的，类似于视频的播放。SSE是一个单向通信，只能服务器向浏览器发送，这里不选择websocket的原因和2一样，基于业务层面去考虑。</li>\n</ol>\n</li>\n<li><h3 id=\"抽象数据源Driver设计\"><a href=\"#抽象数据源Driver设计\" class=\"headerlink\" title=\"抽象数据源Driver设计\"></a>抽象数据源Driver设计</h3><p>​       该项目是支持异构数据源同步的，所以会有很多种数据源，进行每种数据源连接是确实可以使用if else去定义，但是这并不好维护，而且可拓展性十分差，想象一下每次新增一个数据源就写多一个if else，代码的可读性也会很差，所以这里会基于模板模式和工厂模式对数据源驱动的获取进行抽象。</p>\n<p>​    我们把数据源实例抽象为几个步骤，读写分离，然后分别定义为接口，将支持JDBC的数据源做一种实现，不支持JDBC的数据源也做一种实现。每种实现又有一个基础类去实现基础功能。</p>\n<p>如下图所示：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ol>\n<li><p>DsDriverFactory遵循工厂设计原则，每个数据源必要信息通过压缩算法返回一个唯一字符串connectld，DsDriverFactory向外暴露getDsReader、getDsWriter方法，通过解析connectld得到数据源type，根据数据源type获取对应Driver.class并通过反射生成对应的驱动对象。</p>\n</li>\n<li><p>要新增数据源，比如新增一个Mysql的数据源，那么只需要继承JDBC这个类，JDBC这个类已经实现了部分基本的功能，并制定了相应的执行流程，Mysql继承之后，只需要把获取数据源JdbcUrl这种每个数据源都不同的抽象方法给实现了就行。</p>\n</li>\n<li><p>总体流程就是，用户提交了注册新数据源的信息之后，注册信息会存入数据库中，等到创建流转任务时，会获取数据源下的数据表，这时就调用DsDriverFactory获取数据源实例，并调用实例中的方法获取对应的数据&#x2F;写入数据。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">PACKAGE_PREFIX</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;com.datalinkx.driver.dsdriver.&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String <span class=\"title function_\">getDriverClass</span><span class=\"params\">(String driverName)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> PACKAGE_PREFIX + driverName.toLowerCase() + <span class=\"string\">&quot;driver&quot;</span> + <span class=\"string\">&quot;.&quot;</span> + ConnectIdUtils.toPascalCase(driverName) + <span class=\"string\">&quot;Driver&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//getDriver 方法依赖于 ConnectIdUtils 和 getDriverClass 方法来确定应该加载和实例化哪个驱动类。</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> IDsDriver <span class=\"title function_\">getDriver</span><span class=\"params\">(String connectId)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">dsType</span> <span class=\"operator\">=</span> ConnectIdUtils.getDsType(connectId);</span><br><span class=\"line\">        <span class=\"comment\">//拼接地址，也就是包的地址，来获取对应的类名，并返回给DsServiceImpl类，让他知道用的是哪个实现类，这就是用来替代无数个if else的关键地方</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">driverClassName</span> <span class=\"operator\">=</span> getDriverClass(dsType);</span><br><span class=\"line\">        Class&lt;?&gt; driverClass = Class.forName(driverClassName);</span><br><span class=\"line\">        Constructor&lt;?&gt; constructor = driverClass.getDeclaredConstructor(String.class);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (IDsDriver) constructor.newInstance(connectId);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> IStreamDriver <span class=\"title function_\">getStreamDriver</span><span class=\"params\">(String connectId)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">dsType</span> <span class=\"operator\">=</span> ConnectIdUtils.getDsType(connectId);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">driverClassName</span> <span class=\"operator\">=</span> getDriverClass(dsType);</span><br><span class=\"line\">        Class&lt;?&gt; driverClass = Class.forName(driverClassName);</span><br><span class=\"line\">        Constructor&lt;?&gt; constructor = driverClass.getDeclaredConstructor(String.class);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (IStreamDriver) constructor.newInstance(connectId);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> IDsReader <span class=\"title function_\">getDsReader</span><span class=\"params\">(String connectId)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * getDriver(connectId) 返回的是一个 IDsDriver 类型的对象，然后将其强制转换为 IDsReader 类型。这种转换能够成功执行的前提是：</span></span><br><span class=\"line\"><span class=\"comment\">         * IDsReader 接口继承自 IDsDriver 接口：如果 IDsReader 接口是 IDsDriver 接口的子接口，那么所有的 IDsReader 实现也必然是 IDsDriver 的实现。</span></span><br><span class=\"line\"><span class=\"comment\">         * 这种情况下，转换是合法的，因为 IDsReader 是 IDsDriver 的子类型。</span></span><br><span class=\"line\"><span class=\"comment\">         * 实现类的兼容性：调用 getDriver(connectId) 方法返回的具体实现类必须同时实现了 IDsDriver 和 IDsReader 接口。这是类型转换能够成功的关键。</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> (IDsReader) getDriver(connectId);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (InstantiationException | IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class=\"line\">                log.error(<span class=\"string\">&quot;driver load error&quot;</span>, e);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (NoSuchMethodException e) &#123;</span><br><span class=\"line\">            log.error(<span class=\"string\">&quot;driver load error&quot;</span>, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Exception</span>(<span class=\"string\">&quot;can not initialize driver&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> IDsWriter <span class=\"title function_\">getDsWriter</span><span class=\"params\">(String connectId)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> (IDsWriter) getDriver(connectId);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (InstantiationException | IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class=\"line\">                log.error(<span class=\"string\">&quot;driver load error&quot;</span>, e);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (NoSuchMethodException e) &#123;</span><br><span class=\"line\">            log.error(<span class=\"string\">&quot;driver load error&quot;</span>, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Exception</span>(<span class=\"string\">&quot;can not initialize driver&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></li>\n</ol>\n</li>\n<li><h3 id=\"XXL-Job平台\"><a href=\"#XXL-Job平台\" class=\"headerlink\" title=\"XXL-Job平台\"></a>XXL-Job平台</h3><p>​    xxl-job是一个分布式任务调度平台，核心思想是将调度任务抽象成为调度器和执行器，平台本身不承担业务逻辑，只负责发起调度请求后，由执行器接收调度请求并执行任务，这里的任务被抽象为分散的JobHandler。JobHandler分散到各个业务代码中，依netty保持与调度器的交互，通过这种方式即可实现调度与任务相互解耦，从而提高系统整体的稳定性和拓展性。如下图：</p>\n<p><img src=\"/image2.png\" alt=\"image2\"></p>\n<ol>\n<li><p>调度器只需要借助maven打包xxl-job的admin即可，然后java -jar把jar包运行起来即可，或者是用docker直接部署，而执行器则放在项目中的Job模块下，使用@XxlJob(“XXX”)在方法上定义执行器即可</p>\n</li>\n<li><p>为了解决xxl-job登录的问题，这里会使用一个拦截器，拦截器逻辑非常朴实无华，在发请求之前，判断cookieValue是否存在，如果不存在就通过datalinkx-client的能力发起一次登录请求。登陆后保存cookie加到原请求中。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> Response <span class=\"title function_\">intercept</span><span class=\"params\">(Chain chain)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">Request</span> <span class=\"variable\">request</span> <span class=\"operator\">=</span> chain.request();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tResponse response;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (StringUtils.isEmpty(cookieValue)) &#123;</span><br><span class=\"line\">\t\t\t\tretrofit2.<span class=\"type\">Response</span> <span class=\"variable\">loginResp</span> <span class=\"operator\">=</span> xxlLoginClient.login(username, passwd, <span class=\"string\">&quot;on&quot;</span>).execute();</span><br><span class=\"line\">\t\t\t\tcookieValue = loginResp.headers().get(SET_COOKIE_HEADER);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (StringUtils.isNotEmpty(cookieValue)) &#123;<span class=\"comment\">//检查是否成功获取了 Cookie 值</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//如果获取了 Cookie，则创建一个新的请求，将 Cookie 添加到请求头中</span></span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Request</span> <span class=\"variable\">newRequest</span> <span class=\"operator\">=</span> chain.request().newBuilder()</span><br><span class=\"line\">\t\t\t\t\t\t.addHeader(HEADER_COOKIE, cookieValue)</span><br><span class=\"line\">\t\t\t\t\t\t.build();</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//使用新的请求继续执行链式调用，获取响应</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//当前拦截器已经完成了对请求的处理，将请求传递给拦截器链中的下一个拦截器继续处理</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 最后一个拦截器处理请求后，不再调用 proceed，而是直接将请求发送到服务器。服务器的响应返回，按相反顺序通过拦截器链。</span></span><br><span class=\"line\">\t\t\t\tresponse = chain.proceed(newRequest);</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\tlog.error(<span class=\"string\">&quot;xxl-job login error&quot;</span>);</span><br><span class=\"line\">\t\t\t\tresponse = chain.proceed(chain.request());</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> e;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (response.code() == <span class=\"number\">200</span>) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"type\">ResponseBody</span> <span class=\"variable\">responseBody</span> <span class=\"operator\">=</span> response.body();</span><br><span class=\"line\">\t\t\t<span class=\"type\">MediaType</span> <span class=\"variable\">contentType</span> <span class=\"operator\">=</span> responseBody != <span class=\"literal\">null</span> ? responseBody.contentType() : <span class=\"literal\">null</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (contentType != <span class=\"literal\">null</span> &amp;&amp; contentType.subtype().equals(<span class=\"string\">&quot;json&quot;</span>)) &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"type\">String</span> <span class=\"variable\">bodyString</span> <span class=\"operator\">=</span> getBody(response);</span><br><span class=\"line\">\t\t\t\tbodyString = bodyString.replaceFirst(<span class=\"string\">&quot;\\&quot;result\\&quot;:\\\\s*\\&quot;\\\\s*\\&quot;&quot;</span>, <span class=\"string\">&quot;\\&quot;result\\&quot;: null&quot;</span>);</span><br><span class=\"line\">\t\t\t\t<span class=\"type\">ResponseBody</span> <span class=\"variable\">body</span> <span class=\"operator\">=</span> ResponseBody.create(contentType, bodyString);</span><br><span class=\"line\">\t\t\t\tresponse = response.newBuilder().body(body).build();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> response;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure></li>\n</ol>\n</li>\n<li><p>流转任务的生命钩子函数设计</p>\n<p>​    通常的模板方法模式中会设计一个abstract的抽象方法，交给它的子类实现，这个方法称为模板方法。而钩子方法，是对于抽象方法或者接口中定义的方法的一个空实现，也是模板方法模式的一种实现方式。设计钩子方法的主要目的是干预执行流程，使得控制行为流程更加灵活，更符合实际业务的需求。在该项目流转任务的生命周期都是基于钩子方法+模板实现的，任务由xxl-job调度中心通过netty回调到DataTransHandler执行器中，执行器中注入任务触发类DataTransferAction，由doAction开始一次任务的执行，而FlinkAction是继承的AbstractDataTransferAction实现各种模板和钩子方法:</p>\n<p>​    六个钩子方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> <span class=\"keyword\">void</span> <span class=\"title function_\">begin</span><span class=\"params\">(D info)</span>;</span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> <span class=\"keyword\">void</span> <span class=\"title function_\">end</span><span class=\"params\">(D info, <span class=\"type\">int</span> status, String errmsg)</span>;</span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> <span class=\"keyword\">void</span> <span class=\"title function_\">beforeExec</span><span class=\"params\">(U unit)</span> <span class=\"keyword\">throws</span> Exception;</span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> <span class=\"keyword\">void</span> <span class=\"title function_\">execute</span><span class=\"params\">(U unit)</span> <span class=\"keyword\">throws</span> Exception;</span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> <span class=\"type\">boolean</span> <span class=\"title function_\">checkResult</span><span class=\"params\">(U unit)</span>;</span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> <span class=\"keyword\">void</span> <span class=\"title function_\">afterExec</span><span class=\"params\">(U unit, <span class=\"type\">boolean</span> success, String errorMsg)</span>;</span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> U <span class=\"title function_\">convertExecUnit</span><span class=\"params\">(D info)</span>;</span><br></pre></td></tr></table></figure>\n\n<p>​    doAction串联生命周期：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">doAction</span><span class=\"params\">(T actionInfo)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 获取job详情</span></span><br><span class=\"line\">    <span class=\"type\">D</span> <span class=\"variable\">detail</span> <span class=\"operator\">=</span> getJobDetail(actionInfo);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">StringBuffer</span> <span class=\"variable\">error</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">StringBuffer</span>();</span><br><span class=\"line\">        <span class=\"comment\">// 准备执行job</span></span><br><span class=\"line\">        begin(detail);</span><br><span class=\"line\">        <span class=\"comment\">// ....</span></span><br><span class=\"line\">        <span class=\"comment\">// 遍历执行启动flink任务</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 每个单元执行前的准备</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (isStop()) &#123;</span><br><span class=\"line\">                log.error(<span class=\"string\">&quot;logkill trigger&quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">InterruptedException</span>();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            beforeExec(unit)</span><br><span class=\"line\">            <span class=\"comment\">// 启动任务</span></span><br><span class=\"line\">            execute(unit);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">            log.error(<span class=\"string\">&quot;user stop&quot;</span>, e);</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> e;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Throwable e) &#123;</span><br><span class=\"line\">            log.error(<span class=\"string\">&quot;execute flink task error.&quot;</span>, e);</span><br><span class=\"line\">            unitCount.decrementAndGet();</span><br><span class=\"line\">            afterExec(unit, <span class=\"literal\">false</span>, e.getMessage());</span><br><span class=\"line\">            error.append(e.getMessage()).append(<span class=\"string\">&quot;\\r\\n&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 整个Job结束后的处理</span></span><br><span class=\"line\">        end(detail, error.length() == <span class=\"number\">0</span> ? JOB_STATUS_SUCCESS : JOB_STATUS_ERROR, error.length() == <span class=\"number\">0</span> ? <span class=\"string\">&quot;success&quot;</span> : error.toString());</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">        log.error(<span class=\"string\">&quot;Stop task by user.&quot;</span>);</span><br><span class=\"line\">        JobUtils.cntx().setCanceled(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        end(detail, JOB_STATUS_STOP, <span class=\"string\">&quot;cancel the job&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> e;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (Throwable e) &#123;</span><br><span class=\"line\">        log.error(<span class=\"string\">&quot;sync failed&quot;</span>, e);</span><br><span class=\"line\">        end(detail, JOB_STATUS_ERROR, e.getMessage());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>​    这个方法可谓是整个datalinkx的灵魂，核心中的核心，最核心的代码，我们通过这个方法将一个流转任务的生命周期串联起来。<br>这样真正的实现类DataTransferAction.java只需要继承AbstractDataTransferAction实现各个具体的生命周期方法即可，Xx1-Job回调来的方法直接调用FlinkAction继承的doAction方法即可将任务串联执行。</p>\n</li>\n</ul>\n",
            "tags": [
                "datalinkx"
            ]
        },
        {
            "id": "http://example.com/2025/05/12/LLM-concept/",
            "url": "http://example.com/2025/05/12/LLM-concept/",
            "title": "LLM-concept",
            "date_published": "2025-05-12T07:54:00.000Z",
            "content_html": "<h1 id=\"大模型基本概念\"><a href=\"#大模型基本概念\" class=\"headerlink\" title=\"大模型基本概念\"></a>大模型基本概念</h1><h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ul>\n<li><p>语言模型就是对自然语言的概率分布进行建模，即P(w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p>\n</li>\n<li><p>计算下一个词的概率 P(wn | w1 w2 w3… wn-1) </p>\n<p><img src=\"/image1.png\" alt=\"image\"></p>\n</li>\n</ul>\n<h2 id=\"发展历程\"><a href=\"#发展历程\" class=\"headerlink\" title=\"发展历程\"></a>发展历程</h2><p>从n-gram:</p>\n<p><img src=\"/image2.png\" alt=\"image\"></p>\n<p>到neural language model: 每个词都映射成一个低维向量</p>\n<p><img src=\"/image3.png\" alt=\"image\"></p>\n<p>再到后面的transformer出现，transformer的出现，NLP进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是PLM。</p>\n<p>随着OpenAI发布的1750亿个参数（GPT-3），开启LLM时代</p>\n<h2 id=\"问题发现\"><a href=\"#问题发现\" class=\"headerlink\" title=\"问题发现\"></a>问题发现</h2><p> • 大模型（如GPT-3）参数量极大（1750亿+），传统“预训练+微调”范式成本过高（需为每个任务调整海量参数）。</p>\n<ol>\n<li><p>解决方案：<br>• 开发新范式（ICL&#x2F;Prompt），通过输入指令或示例直接引导模型，避免微调。</p>\n<p>• 但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p>\n</li>\n<li><p>模型构建的关键：<br>• 预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握ICL&#x2F;Prompt所需的能力（如任务模式识别、指令遵循）。</p>\n<p>• 后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p>\n</li>\n<li><p>结论：<br>• 新范式（ICL&#x2F;Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p>\n</li>\n</ol>\n<h2 id=\"LLM的构建流程\"><a href=\"#LLM的构建流程\" class=\"headerlink\" title=\"LLM的构建流程\"></a>LLM的构建流程</h2><ul>\n<li>预训练： 利用海量训练数据构建多样化内容，构建基础模型——&gt;对长文本建模，使模型具有语言生成能力</li>\n<li>有监督微调SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li>\n<li>奖励建模RM：训练一个能够判断文本质量的裁判，对同个提示词，比较SFT生成的多个输出的质量</li>\n<li>强化学习RLHF(human feedback)：基于RM，优化SFT模型</li>\n</ul>\n<p>SFT相当于学生学会答题，RM是评分老师，判断answer好坏，RLHF是学生根据老师评分改进答题策略</p>\n<h2 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h2><p><strong>N-gram 模型详解</strong><br> N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p>\n<p> N-gram的定义：</p>\n<p>• 指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p>\n<p>• 例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure>\n\n<p>• 核心假设：</p>\n<p>• 马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p><strong>如何计算概率？</strong><br> N-gram 通过统计语料库中词序列的频率来估计概率：</p>\n<p>计算 <code>P(sat | the cat)</code>：</p>\n<p>P(sat∣the cat)&#x3D;Count(“the cat”)Count(“the cat sat”)</p>\n<p>若语料中 “the cat” 出现 100 次，”the cat sat” 出现 30 次，则 <code>P(sat | the cat) = 0.3</code>。</p>\n<p><strong>N-gram 的优缺点</strong></p>\n<table>\n<thead>\n<tr>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>简单高效，计算速度快。</td>\n<td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td>\n</tr>\n<tr>\n<td>小规模数据即可训练。</td>\n<td>数据稀疏性（罕见 n-gram 概率不准确）。</td>\n</tr>\n<tr>\n<td>曾广泛用于机器翻译、拼写检查等任务。</td>\n<td>无法理解语义（仅统计共现频率）。</td>\n</tr>\n</tbody></table>\n",
            "tags": [
                "LLM",
                "concept"
            ]
        },
        {
            "id": "http://example.com/2025/05/11/resume/",
            "url": "http://example.com/2025/05/11/resume/",
            "title": "resume",
            "date_published": "2025-05-11T07:54:37.000Z",
            "content_html": "<h1 id=\"Resume\"><a href=\"#Resume\" class=\"headerlink\" title=\"Resume\"></a>Resume</h1><h2 id=\"教育经历\"><a href=\"#教育经历\" class=\"headerlink\" title=\"教育经历\"></a>教育经历</h2><ul>\n<li>深圳大学（硕士） 2024-2027 计算机技术 硕士一等奖学金</li>\n<li>广东财经大学（本科） 2020-2024 计算机科学与技术 学业奖学金  CET-6 蓝桥杯省赛二等奖</li>\n</ul>\n<h2 id=\"实习经历\"><a href=\"#实习经历\" class=\"headerlink\" title=\"实习经历\"></a>实习经历</h2><ul>\n<li><p>近期：深圳迅策科技股份有限公司 \t    \t后端研发实习生 技术支持中心—政府项目组</p>\n<p>后端研发实习生 技术支持中心—政府项目组</p>\n<ul>\n<li>工作描述：负责智慧交通平台道路信息服务的后端研发</li>\n<li>参与问题定位开发：使用AOP并结合自定义注解获取全局请求与处理信息，增强API的可追踪性和调试效率，通过此配置团队成员可以快速定位问题和分析系统行为，提高开发效率</li>\n<li>数据表设计与查询优化：独立完成交通流量、违法信息、道路信息等5个模块的数据设计，包含数据表设计、字段抽象与设计，同时在百万级数据的交通流量表设计上建立索引，将平均回表次数从160w次优化为200次</li>\n<li>道路模块开发：负责相关需求开发，并基于内部CI&#x2F;CD平台搭建自动化测试流水线，保证在提交测试前各接口单元测试覆盖率达到70%以上，核心链路全覆盖</li>\n</ul>\n</li>\n<li><p>其他：望海康信、CVTE</p>\n</li>\n</ul>\n<h2 id=\"项目经历\"><a href=\"#项目经历\" class=\"headerlink\" title=\"项目经历\"></a>项目经历</h2><ul>\n<li>生物大语言模型集成平台\t2024.10 - 2024.12\t论文转化成果网站（后端开发成员）<ul>\n<li>项目描述：生物语言模型集成平台是一个面向生物医学研究领域的工具类网站，旨在整合实验室研发的多种大语言模型，为研究人员提供便捷的模型调用和数据分析服务，并且还提供实验室研究成果展示等功能。本人负责部分模块后端开发，同时负责工作分配以及把控进度。</li>\n<li>项目技术栈：SpringCloud + SpringBoot + Mybatis</li>\n<li>服务拆分：基于跨语言与团队成员擅长技术的需求，将平台拆分为网关、模型处理、数据管理服务三个独立模块，并结合Nacos注册中心、Feign远程调用技术</li>\n<li>登录与用户管理：利用Spring Cloud Gateway接口，解析和验证JWT令牌，并传递用户信息至下游服务，实现服务间用户信息共享。对用户密码采取BCrypt密码加密方式，有效保障用户账号安全</li>\n<li>代码重构：分析所负责项目中多个相似的查询请求，通过自动化Mapper接口与实体类的映射，同时结合动态构建查询条件，实现了通用查询框架，提高了代码的复用性，减少了至少7个Mapper接口编码工作</li>\n<li>异步执行与存储优化：将模型调用的同步执行操作，结合Mq技术，转化为异步操作，实现平均响应时间从2000ms+降低至300ms内，并基于MinIO部署专用文件存储服务器，实现文件转存，减少本地服务器压力</li>\n</ul>\n</li>\n<li>异构数据源同步平台      2023.10 - 2023.12     后端开发</li>\n<li>项目描述：该系统是一个基于Flink的异构数据源流转服务，用来作为数据源之间的数据同步工具，通过抽象异构数据源驱动，同时屏蔽数据源间不同的通信协议，通过页面化配置数据同步任务的方式简化数据库同步流程，并实现定时调度。</li>\n<li>项目技术栈：Flink + Chunjun + Xxl-Job + Redis + Retrofit2</li>\n<li>数据源Driver设计：使用工厂设计模式、模板设计模式抽象化数据源的交互逻辑，同时支持灵活扩展数据源</li>\n<li>流转进度推送：基于Redis stream搭建轻量级消息队列，同时结合SSE实例实现任务流转进度即时刷新</li>\n<li>任务调度：借助Xxl-Job实现任务调度管理，确保异构数据源的定时同步和实时更新需求，支持批量任务管理</li>\n<li>数据同步：使用钩子方法覆盖流转任务完整生命周期，并定义模板方法以高度可扩展的方式串联任务生命周期</li>\n</ul>\n<h2 id=\"专业技能\"><a href=\"#专业技能\" class=\"headerlink\" title=\"专业技能\"></a>专业技能</h2><ul>\n<li>熟悉Java基础编程，具有良好的面向对象编程思想、熟悉多线程、集合等知识</li>\n<li>熟悉SpringBoot、SpringCloud、Mybatis等开发框架，了解微服务架构以及Nacos、Gateway等组件的使用</li>\n<li>熟悉Mysql数据库的使用，对索引、锁机制、事务、日志、数据库范式有一定理解</li>\n<li>熟悉Redis的使用，熟悉五种常见数据类型，对缓存持久化、缓存穿透、缓存击穿有一定理解</li>\n<li>熟悉Linux系统以及常用命令的使用，对Docker、Kubernetes有一定的理解</li>\n<li>熟悉计算机网络原理，对OSI七层模型、TCP&#x2F;UDP、HTTP&#x2F;HTTPS协议有一定理解</li>\n<li>熟悉操作系统基础知识，对进程调度、内存管理、虚拟内存等有一定理解</li>\n</ul>\n",
            "tags": [
                "resume"
            ]
        }
    ]
}