<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Phoenix • Posts by &#34;paper&#34; category</title>
        <link>http://example.com</link>
        <description>Every day is a chance to learn something new</description>
        <language>en</language>
        <pubDate>Tue, 11 Nov 2025 19:21:54 +0800</pubDate>
        <lastBuildDate>Tue, 11 Nov 2025 19:21:54 +0800</lastBuildDate>
        <category>datalinkx</category>
        <category>混合专家模型</category>
        <category>LLaMA</category>
        <category>LLM</category>
        <category>concept</category>
        <category>Bert</category>
        <category>GPT</category>
        <category>transformer</category>
        <category>resume</category>
        <category>paper</category>
        <item>
            <guid isPermalink="true">http://example.com/2025/11/11/BIBM-paper-CFPLM/</guid>
            <title>BIBM paper: CFPLM</title>
            <link>http://example.com/2025/11/11/BIBM-paper-CFPLM/</link>
            <category>paper</category>
            <pubDate>Tue, 11 Nov 2025 19:21:54 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models&#34;&gt;#&lt;/a&gt; CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models&lt;/h1&gt;
&lt;h2 id=&#34;abstract&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#abstract&#34;&gt;#&lt;/a&gt; Abstract&lt;/h2&gt;
&lt;p&gt;Protein-RNA interactions (PRIs) play pivotal roles in biological processes such as gene regulation, making their prediction essential for therapeutic and mechanistic studies. While traditional wet-lab methods are time-consuming and challenging, computational approaches offer efficient alternatives. Graph-based methods show promise by capturing both direct interactive domains (protein-RNA interaction) and indirect collaborative domains (functional similarity among proteins/RNAs). However, integrating these domains and learning meaningful node representations remain critical challenges. To address this, we propose ​​CFPLM​​, a collaborative framework fusing large language models, graph convolutional networks, and cross-attention mechanisms to improve PRI prediction. Experiment results demonstrate that CFPLM achieves robust, state-of-the-art performance across three benchmark datasets. It’s anticipated to have applicability to similar other interaction prediction tasks.&lt;/p&gt;
&lt;h2 id=&#34;iintroduction&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#iintroduction&#34;&gt;#&lt;/a&gt; Ⅰ.Introduction&lt;/h2&gt;
&lt;p&gt;As two essential biological macromolecules, the interaction between proteins and RNA plays a crucial role in cellular activities, such as gene expression regulation, translation control, and viral replication [1].Traditional biological experiments can identify direct interactions between RNA and protein with confidence. However, they are complicated and challenging, making them expensive to verify each potential interaction individually [2].&lt;br&gt;
In response to these challenges, researchers are encouraged to develop more efficient and economical computational methods to aid in screening and predicting potential interactions, thereby enhancing research efficiency and productivity. Among the various calculation methods currently available, the machine learning-based method is the most widely used in this field. These methods can be divided into two groups: one involves feature learning and classification, and the other is graph-based association prediction. The former extracts features based on protein and RNA sequences or structures, and constructs a classification model based on these features to determine whether a given protein can interact with an RNA. For example, EnANNDeep combines an adaptive K-nearest neighbor classifier, a deep neural network, and a deep forest model, and integrates the prediction results of the three models using a weighted average to improve classification performance [3]. LPIDF utilizes self-encoder technology to extract sequence features of lncRNA and protein, and combines an ensemble learning method to predict possible interactions [4]. In MHAM, a deep learning model based on a multi-head attention mechanism and a residual link is used to predict the interaction between RNA and protein [5]. LGFC-CNN combines the original sequence, artificial, and structural features to predict PRI through deep learning [6]. LPI-CNNCP uses convolutional neural networks and replication filling techniques to predict the interaction between lncRNA and protein [7]. AptaNet [8] and AptaTrans [9] employ deep neural networks and transformer-based encoders, respectively, to capture the complex interactions between proteins and nucleic acid aptamers. The advantage of this type of method is that it can predict the potential interaction between any given protein and RNA. However, the interaction patterns between proteins and RNA are complex, and currently known data on protein-RNA interactions is limited, making it difficult to train a reliable end-to-end prediction model. Therefore, existing methods are difficult to generalize excellent predictive performance to different datasets.&lt;br&gt;
Graph-based methods have emerged as particularly promising approaches. Because it can better utilize existing knowledge through molecular similarity networks and interaction networks. For example, LPICGAE is a combination method based on a graph self-encoder, which is used to predict PRI [10]. BiHo-GNN is the first method to integrate isomorphic and heterogeneous networks through bipartite graph embedding to predict PRI [11]. NPI-GNN was proposed for predicting noncoding RNA–protein interactions using graph neural networks[12]. CCGNN proposed a new method to predict PRI by incorporating information from the interaction domain and cooperation domain [13]. From the above research, it is evident that collaborative domain construction and node representation learning are key factors in determining this method. However, when constructing collaborative domains and node representation, most existing graph-based methods fail to fully capture the complex information in RNA and protein sequences, which results in limited performance of the finally trained models.&lt;br&gt;
In recent years, the breakthrough progress of large language models (LLMs) has transformed various aspects of life, including scientific research paradigms. Currently, numerous large language models have been trained on RNA and protein sequences, such as EVO [14], BIRNA-BERT [15], RNA-FM [16], ESM [17], and ProteinBERT [18], which have been widely utilized in biological tasks, including interaction prediction, and have achieved notable success. For example, PAIR used an RNA-FM language model to get the embedding vector of nucleic acid aptamer, and performed well in protein-nucleic acid aptamer interaction task [19]; In DTI-LM [20], ChemBERTa [21] and ESM language models are used to generate embedding vectors for drug molecules and protein, and show good performance in their prediction tasks . These studies demonstrate that LLMs have the potential to offer a more powerful approach to constructing collaborative domains and characterizing the nodes. However, there is currently a lack of research to explore this point. Moreover, most graph-based methods operate on information from the collaborative domain and interaction information within RNA or protein separately, thereby ignoring some useful cross-information and dependence between RNA and protein. How to better combine LLMs with other technologies to address the challenges in PRI prediction remains to be explored.&lt;br&gt;
To address these gaps, we propose a novel framework, named CFPLM, for identifying protein-RNA interactions that utilizes a cross-attention fusion neural network in conjunction with large language models. The contributions of this study are as follows:&lt;br&gt;
(1) We introduce a new strategy for constructing collaborative domains based on both protein and RNA language models, which provides a more reliable method for knowledge construction in graph-based approaches.&lt;br&gt;
(2) We design a new learning model for protein-RNA interactions by combining a graph convolutional neural network (GCN) with a cross-attention mechanism, thereby overcoming the limitations of existing graph learning models in this field.&lt;br&gt;
The experimental results demonstrate the proposed framework is more efficient and accurate for predicting protein-RNA interactions, offering valuable insights into the interaction mechanisms between RNA and protein, as well as related biological processes.&lt;/p&gt;
&lt;h2 id=&#34;iimaterial-and-method&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#iimaterial-and-method&#34;&gt;#&lt;/a&gt; II.Material and Method&lt;/h2&gt;
&lt;p&gt;A.Datasets&lt;br&gt;
The benchmark dataset used in this study was constructed based on the protein-RNA complex released from the PDB (Protein Data Bank) database between 1989 and 2023. We labeled a protein chain interacts with an RNA according to the spatial distance between in the complex. According to the previous research [22], the distance threshold is set to 3.5 angstroms (protein-RNA pairs with spatial distance less than 3.5 angstroms are interacted). To reduce data redundancy and ensure objective experimental results, we used BLAST to cluster proteins and RNAs, respectively, based on sequence similarity (with threshold S=90). We then randomly select a sequence from each cluster to build a benchmark database. To avoid noise caused by incomplete protein fragments, we filter out protein sequences with lengths less than 50. Finally, 2,465 RNA sequences, 5,897 protein sequences, and 8,579 pairs of interaction information were obtained. According to previous work [23], we adopt the following protocol for processing the dataset: randomly sampling the same number of protein-RNA pairs that are not interactions as negative samples.&lt;br&gt;
For a comprehensive evaluation, we also employed another two datasets: miRTarBase (version 8.0) [24] and a protein-lncRNA dataset [10]. There are 3,924 miRNAs, 20,992 target genes, and 186,416 verified interactions in miRTarBase. While 3,046 lncRNAs, 136 proteins, and 8,112 interactions in the protein-lncRNA dataset.&lt;br&gt;
B.Framework of CFPLM&lt;br&gt;
CFPLM includes three modules (Fig. 1): 1) Feature extraction, which uses large language models to extract initial feature vectors of RNA and protein; 2) Graph structure, which uses cosine similarity to calculate the similarity of RNA-RNA and protein-protein, and then obtains the similarity graph, and at the same time transforms the RNA-protein interaction pair into an interaction graph; 3) Feature optimization, which uses graph convolution network to capture the complex features of RNA and protein, and uses cross attention to fuse the feature vectors of similarity domain and interaction domain. The first and second points are shown in Fig. 1 (a), and the third point is shown in Fig. 1 (b-c).&lt;br&gt;
(1)Feature Extraction&lt;br&gt;
a)RNA Encoder: BIRNA-BERT is an RNA language model based on Transformer architecture, which is characterized by adopting a double labeling scheme, combining nucleotide-level labeling and byte-pair coding labeling, and can dynamically adjust the labeling strategy according to the sequence length and computing resources, so that the model can handle long-sequence tasks while retaining the ability to handle short-sequence tasks using nucleotide-level information. RNA-FM, the basic RNA model based on self-supervised learning, has been proven effective in multiple tasks for the first time. Another example is the latest Evo, which demonstrates the powerful ability of genome language models in function-oriented design. Regarding the choice of language model, we will compare the above language models in the results section. Ultimately, we chose to use the BIRNA-BERT model based on its ease of use, as it can be well adapted to the task and saves computing resources and experimental time.&lt;br&gt;
b)Protein Encoder: ESM-2 is a large language model for protein sequences. The model can learn the evolution pattern in protein sequences by training on the modeling target of masked language and transform it into a deep understanding of protein structure. ProteinBERT is also a language model specially designed for protein sequences. ProteinBERT enhances the efficiency and performance of protein sequence analysis by integrating a novel pre-training task that combines language modeling and gene ontology annotation prediction. The model has performed well in several protein task benchmarks, approaching or exceeding the performance of existing large-scale models [18]. We compared the above language models and finally adopt the ESM-2 model. In this study, we employ a protein language model based on the ESM-2 architecture with 150M parameters, and train it on the UniRef50 dataset [25].&lt;br&gt;
(2)Graph Construction&lt;br&gt;
In DTI-LM, the feature expression is improved by calculating the similarity protein-protein and drug. The similarity function is used in CCGNN to construct the similarity map between protein and RNA. FMSRT constructs multi-source similarity map according to similarity [26] . The success of these studies shows that similarity information can provide rich information for the study of biomolecular interaction. In this study, we will use the cosine similarity calculation method to calculate the similarity of RNA feature vectors and protein feature vectors respectively, and convert them into RNA-RNA similarity graph and Protein-Protein similarity graph. At the same time, NPI-GNN model uses the bipartite graph of ncRNA-protein to infer the interaction [12], and GANLDA model uses the bipartite graph of lncRNA and diseases to predict the interaction [27]. These methods combine the topological information of PRI networks, and effectively improve the performance of the model in link prediction tasks. In this study, we construct an RNA-protein interaction bipartite graph for feature optimization in graph neural networks.&lt;br&gt;
(3)Feature Optimization&lt;br&gt;
a)Graph convolution network: GCN is a spectral-based graph neural network that processes graph-structured data by aggregating neighborhood information to learn low-dimensional node representations. It efficiently preserves local structure while supporting tasks like node classification and link prediction.  The RNA feature matrix and protein feature matrix will enter the graph convolution network with three graphs, and finally get the updated node feature vector.&lt;br&gt;
b)Cross Attention: To better capture the interaction and similarity information between RNA and proteins, a cross-attention mechanism is introduced. Its core lies in dynamically learning the interactions between the two through the attention mechanism, so a cross-attention module is designed in the model to handle such information. This module contains 4 attention heads, each with a feature dimension of 64. Fig. 1 © illustrates the calculation process of the cross-attention module, where, for the RNA and protein feature vector matrices obtained from the interaction graph, RNA serves as the Query and protein acts as the Key and Value, and vice versa. Meanwhile, the RNA and protein matrices derived from the similarity graph are also optimized and updated using this method.&lt;br&gt;
c)Loss Function: In this study, we use binary cross entropy as the loss function, calculating the average of all samples when calculating the loss.&lt;/p&gt;
&lt;p&gt;FrameWork:&lt;br&gt;
&lt;img src=&#34;image1.png&#34; alt=&#34;image1&#34;&gt;&lt;/p&gt;
&lt;p&gt;References&lt;br&gt;
[1]E. Jankowsky, and M. E. J. N. r. M. c. b. Harris, “Specificity and nonspecificity in RNA–protein interactions,” Nature Reviews Molecular Cell Biology, vol. 16, no. 9, pp. 533-544, 2015.&lt;br&gt;
[2]M. Philip, T. Chen, and S. Tyagi, “A Survey of Current Resources to Study lncRNA-Protein Interactions,” Noncoding RNA, vol. 7, no. 2, pp. 33, Jun 8, 2021.&lt;br&gt;
[3]L. Peng, J. Tan, X. Tian, and L. Zhou, “EnANNDeep: An Ensemble-based lncRNA-protein Interaction Prediction Framework with Adaptive k-Nearest Neighbor Classifier and Deep Models,” Interdisciplinary Sciences: Computational Life Sciences, vol. 14, no. 1, pp. 209-232, Mar, 2022.&lt;br&gt;
[4]X. Tian, L. Shen, Z. Wang, L. Zhou, and L. Peng, “A novel lncRNA-protein interaction prediction method based on deep forest with cascade forest structure,” Scientific Reports, vol. 11, no. 1, pp. 18881, Sep 23, 2021.&lt;br&gt;
[5]Z. Zhou, Z. Du, J. Wei, L. Zhuo, S. Pan, X. Fu, and X. Lian, “MHAM-NPI: Predicting ncRNA-protein interactions based on multi-head attention mechanism,” Computers in Biology and Medicine, vol. 163, pp. 107143, Sep, 2023.&lt;br&gt;
[6]L. Huang, S. Q. Jiao, S. Yang, S. Q. Zhang, X. P. Zhu, R. Guo, and Y. Wang, “LGFC-CNN: Prediction of lncRNA-Protein Interactions by Using Multiple Types of Features through Deep Learning,” Genes, vol. 12, no. 11, pp. 1689, Nov, 2021.&lt;br&gt;
[7]S. W. Zhang, X. X. Zhang, X. N. Fan, and W. N. Li, “LPI-CNNCP: Prediction of lncRNA-protein interactions by using convolutional neural network with the copy-padding trick,” Analytical Biochemistry, vol. 601, pp. 113767, Jul 15, 2020.&lt;br&gt;
[8]N. Emami, and R. Ferdousi, “AptaNet as a deep learning approach for aptamer–protein interaction prediction,” Scientific reports, vol. 11, no. 1, pp. 6074, 2021.&lt;br&gt;
[9]I. Shin, K. Kang, J. Kim, S. Sel, J. Choi, J.-W. Lee, H. Y. Kang, and G. Song, “AptaTrans: a deep neural network for predicting aptamer-protein interaction using pretrained encoders,” BMC Bioinformatics, vol. 24, no. 1, pp. 447, 2023/11/27, 2023.&lt;br&gt;
[10]J. Zhao, J. Sun, S. C. Shuai, Q. Zhao, and J. Shuai, “Predicting potential interactions between lncRNAs and proteins via combined graph auto-encoder methods,” Briefings in Bioinformatics, vol. 24, no. 1, Jan 19, 2023.&lt;br&gt;
[11]Y. Ma, H. Zhang, C. Jin, and C. Kang, “Predicting lncRNA-protein interactions with bipartite graph embedding and deep graph neural networks,” Frontiers in Genetics, vol. 14, pp. 1136672, 2023.&lt;br&gt;
[12]Z.-A. Shen, T. Luo, Y.-K. Zhou, H. Yu, and P.-F. J. B. i. b. Du, “NPI-GNN: Predicting ncRNA–protein interactions with deep graph neural networks,” Briefings in Bioinformatics, vol. 22, no. 5, pp. bbab051, 2021.&lt;br&gt;
[13]H. Li, B. Wu, M. Sun, Z. Zhu, K. Chen, and H. Ge, “Cross-domain contrastive graph neural network for lncRNA–protein interaction prediction,” Knowledge-Based Systems, vol. 296, 2024.&lt;br&gt;
[14]E. Nguyen, M. Poli, M. G. Durrant, B. Kang, D. Katrekar, D. B. Li, L. J. Bartie, A. W. Thomas, S. H. King, G. Brixi, J. Sullivan, M. Y. Ng, A. Lewis, A. Lou, S. Ermon, S. A. Baccus, T. Hernandez-Boussard, C. Ré, P. D. Hsu, and B. L. Hie, “Sequence modeling and design from molecular to genome scale with Evo,” Science, vol. 386, no. 6723, pp. eado9336, 2024.&lt;br&gt;
[15]M. T. Tahmid, H. S. Shahgir, S. Mahbub, Y. Dong, and M. S. J. b. Bayzid, “BiRNA-BERT allows efficient RNA language modeling with adaptive tokenization,” bioRxiv:2024.07.02.601703, 2024.&lt;br&gt;
[16]J. Chen, Z. Hu, S. Sun, Q. Tan, Y. Wang, Q. Yu, L. Zong, L. Hong, J. Xiao, and T. J. a. p. a. Shen, “Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions,” arXiv preprint  arXiv:2204.00300, 2022.&lt;br&gt;
[17]Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. Dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, “Evolutionary-scale prediction of atomic-level protein structure with a language model,” Science, vol. 379, no. 6637, pp. 1123-1130, Mar 17, 2023.&lt;br&gt;
[18]N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial, “ProteinBERT: a universal deep-learning model of protein sequence and function,” Bioinformatics, vol. 38, no. 8, pp. 2102-2110, Apr 12, 2022.&lt;br&gt;
[19]J. Zhang, Z. Yan, H. Zeng, and Z. Zhu, “PAIR: protein-aptamer interaction prediction based on language models and contrastive learning framework.” 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), Lisbon, Portugal, 2024, pp. 5426-5432&lt;br&gt;
[20]K. T. Ahmed, M. I. Ansari, and W. J. B. Zhang, “DTI-LM: language model powered drug–target interaction prediction,” Bioinformatics, vol. 40, no. 9, pp. btae533, 2024.&lt;br&gt;
[21]S. Chithrananda, G. Grand, and B. J. a. p. a. Ramsundar, “ChemBERTa: large-scale self-supervised pretraining for molecular property prediction,” arXiv preprint, arXiv:2010.09885, 2020.&lt;br&gt;
[22]J. Yan, and L. Kurgan, “DRNApred, fast sequence-based method that accurately predicts and discriminates DNA- and RNA-binding residues,” Nucleic Acids Research, vol. 45, no. 10, pp. e84-e84, Jun 2, 2017.&lt;br&gt;
[23]Y. Li, H. Sun, S. Feng, Q. Zhang, S. Han, and W. Du, “Capsule-LPI: a LncRNA-protein interaction predicting tool based on a capsule network,” BMC Bioinformatics, vol. 22, no. 1, pp. 246, May 13, 2021.&lt;br&gt;
[24]H.-Y. Huang, Y.-C.-D. Lin, J. Li, K.-Y. Huang, S. Shrestha, H.-C. Hong, Y. Tang, Y.-G. Chen, C.-N. Jin, Y. Yu, J.-T. Xu, Y.-M. Li, X.-X. Cai, Z.-Y. Zhou, X.-H. Chen, Y.-Y. Pei, L. Hu, J.-J. Su, S.-D. Cui, F. Wang, Y.-Y. Xie, S.-Y. Ding, M.-F. Luo, C.-H. Chou, N.-W. Chang, K.-W. Chen, Y.-H. Cheng, X.-H. Wan, W.-L. Hsu, T.-Y. Lee, F.-X. Wei, and H.-D. Huang, “miRTarBase 2020: updates to the experimentally validated microRNA–target interaction database,” Nucleic Acids Research, vol. 48, no. D1, pp. D148-D154, 2019.&lt;br&gt;
[25]B. E. Suzek, H. Huang, P. McGarvey, R. Mazumder, and C. H. Wu, “UniRef: comprehensive and non-redundant UniProt reference clusters,” Bioinformatics, vol. 23, no. 10, pp. 1282-8, May 15, 2007.&lt;br&gt;
[26]X. Zhang, M. Liu, Z. Li, L. Zhuo, X. Fu, and Q. Zou, “Fusion of multi-source relationships and topology to infer lncRNA-protein interactions,” Molecular Therapy Nucleic Acids, vol. 35, no. 2, pp. 102187, Jun 11, 2024.&lt;br&gt;
[27]W. Lan, X. M. Wu, Q. F. Chen, W. Peng, J. X. Wang, and Y. P. Chen, “GANLDA: Graph attention network for lncRNA-disease associations prediction,” Neurocomputing, vol. 469, pp. 384-393, Jan 16, 2022.&lt;br&gt;
[28]Q. Le, and T. Mikolov, “Distributed representations of sentences and documents,” 2014 International Conference on Machine Learning, Beijing,China, 2014,pp. 1188-1196.&lt;br&gt;
[29]S. Yang, Y. Wang, Y. Lin, D. Shao, K. He, and L. J. M. Huang, “LncMirNet: predicting LncRNA–miRNA interaction based on deep learning of ribonucleic acid sequences,” Molecules, vol. 25, no. 19, pp. 4372, 2020.&lt;br&gt;
[30]Z. Wang, S. Liang, S. Liu, Z. Meng, J. Wang, and S. Liang, “Sequence pre-training-based graph neural network for predicting lncRNA-miRNA associations,” Briefings in Bioinformatics, vol. 24, no. 5, pp. bbad317, Sep 20, 2023.&lt;br&gt;
[31]Y. Han, and S. W. Zhang, “ncRPI-LGAT: Prediction of ncRNA-protein interactions with line graph attention network framework,” Computational and Structural Biotechnology Journal, vol. 21, pp. 2286-2295, 2023.&lt;br&gt;
[32]H. Ji, X. Wang, C. Shi, B. Wang, P. S. J. I. T. o. K. Yu, and D. Engineering, “Heterogeneous graph propagation network,” IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 521-532, 2021.&lt;br&gt;
[33]L. v. d. Maaten, and G. J. J. o. m. l. r. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579-2605, 2008.&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
