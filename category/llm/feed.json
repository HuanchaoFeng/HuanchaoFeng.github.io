{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"llm\" category",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "url": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "title": "LLaMA&混合专家模型",
            "date_published": "2025-05-19T10:50:11.000Z",
            "content_html": "<h2 id=\"LLaMA-model-structure\"><a href=\"#LLaMA-model-structure\" class=\"headerlink\" title=\"LLaMA  model  structure\"></a>LLaMA  model  structure</h2><p>​\tLLaMA是基于transformer的decoder部分构建的，采用前置层归一化、使用RMSNorm规划函数，激活函数更改为SwiGLU，使用旋转位置嵌入更改的decoder模型。更改的位置如下所示：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ul>\n<li><p>RMSNorm函数：</p>\n<p><img src=\"/image2.png\" alt=\"image2\"></p>\n<p>原始层归一化函数：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><strong>对比</strong></th>\n<th align=\"center\"><strong>LayerNorm</strong></th>\n<th align=\"center\"><strong>RMSNorm</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>归一化目标</strong></td>\n<td align=\"center\">均值中心化 + 方差缩放</td>\n<td align=\"center\">仅均方根（RMS）缩放</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>计算复杂度</strong></td>\n<td align=\"center\">较高（需计算均值和方差）</td>\n<td align=\"center\">较低（仅需均方值）</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>参数数量</strong></td>\n<td align=\"center\"><em>γ</em>+<em>β</em>（2d 参数）</td>\n<td align=\"center\">仅 <em>γ</em>（d 参数）</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>SwiGLU</p>\n<p>SwiGLU是门控线性单元（GLU）的变体，公式如下：<br><img src=\"/image4.png\" alt=\"image2\"></p>\n<p><img src=\"/image5.png\" alt=\"image2\"></p>\n<p>第二个公式的激活函数是sigmoid，sigmoid函数特点：</p>\n</li>\n</ul>\n<p><img src=\"/image6.png\" alt=\"image2\"></p>\n<p>当β趋向于0时，相当于y&#x3D;x&#x2F;2，线性函数，当β趋向于无穷时（x&gt;0,x&lt;0,x&#x3D;0)，相当于ReLU函数，当β&#x3D;1，swish光滑且非单调。</p>\n<p>Swish(xW)为门控权重（相当于选择遗忘比例），用权重对xV逐元素加权，用W2映射回原维度。</p>\n<ul>\n<li><p>RoPE（待）</p>\n<p>传统PE，model需要学习隐式位置关系，而RoPE通过旋转矩阵直接编码位置，即将位置信息通过旋转矩阵融合key\\query向量中，直接建模相对位置依赖关系，value是不需要旋转的:</p>\n<p><img src=\"/image7.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<p>以上三个改变与原decoder结合实现了LLaMA</p>\n<h2 id=\"注意力机制优化\"><a href=\"#注意力机制优化\" class=\"headerlink\" title=\"注意力机制优化\"></a>注意力机制优化</h2><p>在 Transformer 结构中，自注意力机制的时间和存储复杂度与序列的长度呈平方的关系，因此占用了大量的计算设备内存并消耗了大量的计算资源，比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>: Q K </span><br><span class=\"line\">Q = [](seq_len*vector_len)</span><br><span class=\"line\">K = [](seq_len*vector_len)</span><br><span class=\"line\">Q*K(转置) = seq_len * seq_len</span><br></pre></td></tr></table></figure>\n\n<p>那么就有这种情况：</p>\n<ul>\n<li><strong>显存占用</strong>：</li>\n<li><strong>计算时间</strong>：<br>每次注意力计算需 seq_len方 * d 次操作，<em>seq_len&#x3D;32</em>k*, <em>d</em>&#x3D;1024 时约为 10的12 次方次操作。</li>\n</ul>\n<p>所以需要方法去优化这一问题，以下举例两种方法：</p>\n<ul>\n<li><p>稀疏注意力</p>\n<ul>\n<li>全局注意力：在稀疏注意力中保留少量全局节点（如 [CLS] token 或特定位置），这些节点可以与序列中<strong>所有其他位置</strong>交互。</li>\n<li>带状注意力：每个 Query 只与<strong>固定宽度邻域内</strong>的 Key 交互（类似对角带状矩阵）</li>\n<li>膨胀注意力：以<strong>固定间隔跳跃采样</strong> Key</li>\n<li>随机注意力：每个 Query 随机选择 <em>r</em> 个位置进行交互</li>\n<li>局部块注意力：多个不重叠块交互</li>\n</ul>\n<p>一般利用上述的复合模式s</p>\n</li>\n<li><p>低秩注意力:</p>\n<p><img src=\"/image8.jpg\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"混合专家模型\"><a href=\"#混合专家模型\" class=\"headerlink\" title=\"混合专家模型\"></a>混合专家模型</h2><p>​\t混合专家模型 (MixedExpert Models，MoEs) 日益受到关注。依据大模型缩放法则，模型规模是提升性能的关键，然而规模扩大必然使计算资源大幅增加。因此，在有限计算资源预算下，如何用更少训练步数训练更大模型成为关键问题。为解决该问题，混合专家模型基于一个简洁的思想：模型不同部分（即“专家”）专注不同任务或数据层面。</p>\n<p>其实就是把模型内部的一组专用子网络，每个子网络负责处理数据中特定类型的任务，如：</p>\n<ul>\n<li>输入句子是数学问题 → 激活“数学专家”</li>\n<li>输入是诗歌 → 激活“文学专家”</li>\n</ul>\n<p>优势：若model有100个专家，每次输入仅用2个，计算量少，而且每个专家通过训练集中学习特定模式，比通用模块更高效。</p>\n<p>混合专家模型按照门控网络（Gate）类型，可以从广义上讲可以分为三个大类：稀疏混合专家模型（Sparse MoE）、稠密混合专家模型（Dense MoE）、软混合专家模型（Soft MoE）：</p>\n<ul>\n<li><p>稀疏混合专家模型：input之后，门控网络仅激活少数专家</p>\n</li>\n<li><p>稠密混合专家模型：所有专家激活，甲醛组合输出，这个是要计算每个wi,bi与x的结果</p>\n</li>\n<li><p>软混合专家模型：门控网络分配的权重直接融合不同专家的参数，得到w(融合)，b(融合)，融合与x计算&#x3D;w*x+b</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><strong>类型</strong></th>\n<th align=\"center\"><strong>专家激活方式</strong></th>\n<th align=\"center\"><strong>计算量</strong></th>\n<th align=\"center\"><strong>参数量扩展性</strong></th>\n<th align=\"center\"><strong>典型场景</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>稀疏MoE</strong></td>\n<td align=\"center\">硬性Top-k选择（如k&#x3D;2）</td>\n<td align=\"center\"><em>O</em>(<em>k</em>⋅FFN)</td>\n<td align=\"center\">极高（万亿级）</td>\n<td align=\"center\">大规模预训练（Mixtral, GPT-4）</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>稠密MoE</strong></td>\n<td align=\"center\">所有专家加权求和</td>\n<td align=\"center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td align=\"center\">低（十亿级）</td>\n<td align=\"center\">小规模多任务学习</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>软MoE</strong></td>\n<td align=\"center\">软性稀疏权重</td>\n<td align=\"center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td align=\"center\">中（百亿级）</td>\n<td align=\"center\">平衡效率与稳定性需求</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n",
            "tags": [
                "混合专家模型",
                "LLaMA"
            ]
        },
        {
            "id": "http://example.com/2025/05/17/GPT-BertBuild/",
            "url": "http://example.com/2025/05/17/GPT-BertBuild/",
            "title": "GPT&BertBuild",
            "date_published": "2025-05-16T16:37:02.000Z",
            "content_html": "<h2 id=\"生成式预训练语言模型GPT\"><a href=\"#生成式预训练语言模型GPT\" class=\"headerlink\" title=\"生成式预训练语言模型GPT\"></a>生成式预训练语言模型GPT</h2><p>GPT的模型结构，由多个transformer的decoder组成，结构如下：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<p>该模型利用的是transformer的解码器部分，训练和推理的过程是类似的，12层transformer模块都在做类似的事情，到最后一层后输出预测的分数（映射到词分类中，得到预测的分数&#x2F;置信度）</p>\n<ul>\n<li><p>有监督下游任务微调：</p>\n<p>在进行下游任务微调时，仅使用GPT的最后一层的最后一个词的隐藏状态，同个全连接层映射到标签空间。因为每个词的隐藏状态都聚合了左侧所有历史词的信息，因此最后一个词的隐藏状态天然编码了整个序列的全局语义。</p>\n<p>同时再进行微调的时候，可能会出现模型遗忘预训练阶段学习的通用只是表示，损失通用与泛化能力，出现灾难性遗忘的问题，所以通常采用混合预训练任务损失和下游微调损失缓解这一问题，Loss Function如下:<br><img src=\"/image2.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"Bert模型构建\"><a href=\"#Bert模型构建\" class=\"headerlink\" title=\"Bert模型构建\"></a>Bert模型构建</h2><p>Bert模型一般是基于Transformer的编码器部分构建的，是一个相当于完形填空(mask)的模型，是考虑双向的模型（同时利用两侧信息）</p>\n<p>例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>:the cat site on the mat</span><br><span class=\"line\">tokenizer(分词):（假设不加入CLS标记和SEP标记）</span><br><span class=\"line\">    code:[<span class=\"number\">1996</span>,<span class=\"number\">4248</span>,<span class=\"number\">2825</span>,<span class=\"number\">2007</span>,<span class=\"number\">1996</span>,<span class=\"number\">3829</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>] <span class=\"comment\"># 0为pad的编码，max假设为8</span></span><br><span class=\"line\">    attention_mask:[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>] <span class=\"comment\"># 0表示填充的位置</span></span><br><span class=\"line\">random mask:选取到了site作为掩码，替换为mask的编码-&gt;<span class=\"number\">103</span></span><br><span class=\"line\">\tcode:[<span class=\"number\">1996</span>,<span class=\"number\">4248</span>,<span class=\"number\">103</span>,<span class=\"number\">2007</span>,<span class=\"number\">1996</span>,<span class=\"number\">3829</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    attention_mask:[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    label:[-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,<span class=\"number\">2825</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>] <span class=\"comment\">#这个是用于计算掩码的真实标签，-100表示不需要计算，只保留被掩码的位置</span></span><br><span class=\"line\">train:</span><br><span class=\"line\">    将上述数据送入Bert模型中</span><br><span class=\"line\">output:</span><br><span class=\"line\">    [</span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        [..,..,..,..,], <span class=\"comment\">#取第二个位置进行计算交叉熵</span></span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        ........</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>训练过程：加载dataset,切分traindataset和testdataset –&gt; 用traindataset训练词元分析器，形成vocab词表 –&gt; 加载预训练词元分析器 –&gt; 分词序列 –&gt; 加载随即权重的BERT模型，设置掩码比例，进行训练，下面是代码训练过程：</p>\n<p>dataset获取与镜像设置：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#下载数据集：</span></span><br><span class=\"line\">pip install huggingface_hub</span><br><span class=\"line\">huggingface-cli download --repo-<span class=\"built_in\">type</span> dataset legacy-datasets/wikipedia  --local-<span class=\"built_in\">dir</span> wikipedia</span><br><span class=\"line\"><span class=\"comment\"># mirror</span></span><br><span class=\"line\">export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>\n\n<p>environment(有部分可能不需要）:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">dependencies:</span><br><span class=\"line\">  - _libgcc_mutex=<span class=\"number\">0.1</span>=main</span><br><span class=\"line\">  - _openmp_mutex=<span class=\"number\">5.1</span>=1_gnu</span><br><span class=\"line\">  - bzip2=<span class=\"number\">1.0</span><span class=\"number\">.8</span>=h5eee18b_6</span><br><span class=\"line\">  - ca-certificates=<span class=\"number\">2025.2</span><span class=\"number\">.25</span>=h06a4308_0</span><br><span class=\"line\">  - expat=<span class=\"number\">2.7</span><span class=\"number\">.1</span>=h6a678d5_0</span><br><span class=\"line\">  - ld_impl_linux-<span class=\"number\">64</span>=<span class=\"number\">2.40</span>=h12ee557_0</span><br><span class=\"line\">  - libffi=<span class=\"number\">3.4</span><span class=\"number\">.4</span>=h6a678d5_1</span><br><span class=\"line\">  - libgcc-ng=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libgomp=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libstdcxx-ng=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libuuid=<span class=\"number\">1.41</span><span class=\"number\">.5</span>=h5eee18b_0</span><br><span class=\"line\">  - ncurses=<span class=\"number\">6.4</span>=h6a678d5_0</span><br><span class=\"line\">  - openssl=<span class=\"number\">3.0</span><span class=\"number\">.16</span>=h5eee18b_0</span><br><span class=\"line\">  - pip=<span class=\"number\">25.1</span>=pyhc872135_2</span><br><span class=\"line\">  - python=<span class=\"number\">3.12</span><span class=\"number\">.9</span>=h5148396_0</span><br><span class=\"line\">  - readline=<span class=\"number\">8.2</span>=h5eee18b_0</span><br><span class=\"line\">  - setuptools=<span class=\"number\">78.1</span><span class=\"number\">.1</span>=py312h06a4308_0</span><br><span class=\"line\">  - sqlite=<span class=\"number\">3.45</span><span class=\"number\">.3</span>=h5eee18b_0</span><br><span class=\"line\">  - tk=<span class=\"number\">8.6</span><span class=\"number\">.14</span>=h39e8969_0</span><br><span class=\"line\">  - wheel=<span class=\"number\">0.45</span><span class=\"number\">.1</span>=py312h06a4308_0</span><br><span class=\"line\">  - xz=<span class=\"number\">5.6</span><span class=\"number\">.4</span>=h5eee18b_1</span><br><span class=\"line\">  - zlib=<span class=\"number\">1.2</span><span class=\"number\">.13</span>=h5eee18b_1</span><br><span class=\"line\">  - pip:</span><br><span class=\"line\">      - accelerate==<span class=\"number\">0.26</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - aiohappyeyeballs==<span class=\"number\">2.6</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - aiohttp==<span class=\"number\">3.11</span><span class=\"number\">.18</span></span><br><span class=\"line\">      - aiosignal==<span class=\"number\">1.3</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - attrs==<span class=\"number\">25.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - certifi==<span class=\"number\">2025.4</span><span class=\"number\">.26</span></span><br><span class=\"line\">      - charset-normalizer==<span class=\"number\">3.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - datasets==<span class=\"number\">3.6</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - dill==<span class=\"number\">0.3</span><span class=\"number\">.8</span></span><br><span class=\"line\">      - filelock==<span class=\"number\">3.18</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - frozenlist==<span class=\"number\">1.6</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - fsspec==<span class=\"number\">2025.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - huggingface-hub==<span class=\"number\">0.31</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - idna==<span class=\"number\">3.10</span></span><br><span class=\"line\">      - jinja2==<span class=\"number\">3.1</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - markupsafe==<span class=\"number\">3.0</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - mpmath==<span class=\"number\">1.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - multidict==<span class=\"number\">6.4</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - multiprocess==<span class=\"number\">0.70</span><span class=\"number\">.16</span></span><br><span class=\"line\">      - mwparserfromhell==<span class=\"number\">0.6</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - networkx==<span class=\"number\">3.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - numpy==<span class=\"number\">2.2</span><span class=\"number\">.5</span></span><br><span class=\"line\">      - nvidia-cublas-cu12==<span class=\"number\">12.6</span><span class=\"number\">.4</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - nvidia-cuda-cupti-cu12==<span class=\"number\">12.6</span><span class=\"number\">.80</span></span><br><span class=\"line\">      - nvidia-cuda-nvrtc-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cuda-runtime-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cudnn-cu12==<span class=\"number\">9.5</span><span class=\"number\">.1</span><span class=\"number\">.17</span></span><br><span class=\"line\">      - nvidia-cufft-cu12==<span class=\"number\">11.3</span><span class=\"number\">.0</span><span class=\"number\">.4</span></span><br><span class=\"line\">      - nvidia-cufile-cu12==<span class=\"number\">1.11</span><span class=\"number\">.1</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - nvidia-curand-cu12==<span class=\"number\">10.3</span><span class=\"number\">.7</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cusolver-cu12==<span class=\"number\">11.7</span><span class=\"number\">.1</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-cusparse-cu12==<span class=\"number\">12.5</span><span class=\"number\">.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-cusparselt-cu12==<span class=\"number\">0.6</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - nvidia-nccl-cu12==<span class=\"number\">2.26</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-nvjitlink-cu12==<span class=\"number\">12.6</span><span class=\"number\">.85</span></span><br><span class=\"line\">      - nvidia-nvtx-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - packaging==<span class=\"number\">25.0</span></span><br><span class=\"line\">      - pandas==<span class=\"number\">2.2</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - propcache==<span class=\"number\">0.3</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - psutil==<span class=\"number\">7.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - pyarrow==<span class=\"number\">20.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - python-dateutil==<span class=\"number\">2.9</span><span class=\"number\">.0</span>.post0</span><br><span class=\"line\">      - pytz==<span class=\"number\">2025.2</span></span><br><span class=\"line\">      - pyyaml==<span class=\"number\">6.0</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - regex==<span class=\"number\">2024.11</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - requests==<span class=\"number\">2.32</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - safetensors==<span class=\"number\">0.5</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - six==<span class=\"number\">1.17</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - sympy==<span class=\"number\">1.14</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - tokenizers==<span class=\"number\">0.21</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - torch==<span class=\"number\">2.7</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - tqdm==<span class=\"number\">4.67</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - transformers==<span class=\"number\">4.51</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - triton==<span class=\"number\">3.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - typing-extensions==<span class=\"number\">4.13</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - tzdata==<span class=\"number\">2025.2</span></span><br><span class=\"line\">      - urllib3==<span class=\"number\">2.4</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - xxhash==<span class=\"number\">3.5</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - yarl==<span class=\"number\">1.20</span><span class=\"number\">.0</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> BertTokenizerFast,BertConfig,BertForMaskedLM,DataCollatorForLanguageModeling,TrainingArguments,Trainer</span><br><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> tokenizers <span class=\"keyword\">import</span> BertWordPieceTokenizer</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> chain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class=\"string\">&#x27;2&#x27;</span> <span class=\"comment\"># 这个得加，多gpu跑的话会有问题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载本地数据</span></span><br><span class=\"line\">wiki = load_dataset(<span class=\"string\">&quot;/data/hcfeng/learnLLM/wikipedia&quot;</span>, <span class=\"string\">&quot;20220301.en&quot;</span>, split=<span class=\"string\">&quot;train&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#仅保留text列</span></span><br><span class=\"line\">wiki = wiki.remove_columns([col <span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> wiki.column_names <span class=\"keyword\">if</span> col!=<span class=\"string\">&quot;text&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\">#切割数据</span></span><br><span class=\"line\">dataset = wiki</span><br><span class=\"line\">d = dataset.train_test_split(train_size=<span class=\"number\">0.9</span>,test_size=<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dataset_to_text</span>(<span class=\"params\">dataset,filename</span>):</span><br><span class=\"line\">    outpur_path=<span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/&quot;</span>+filename <span class=\"comment\">#拼接地址</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(outpur_path,<span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> dataset[<span class=\"string\">&#x27;text&#x27;</span>]:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(t,file=f)</span><br><span class=\"line\">dataset_to_text(d[<span class=\"string\">&#x27;train&#x27;</span>],<span class=\"string\">&#x27;train.txt&#x27;</span>)</span><br><span class=\"line\">dataset_to_text(d[<span class=\"string\">&#x27;test&#x27;</span>],<span class=\"string\">&#x27;test.txt&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">special_tokens = [</span><br><span class=\"line\"><span class=\"string\">&quot;[PAD]&quot;</span>, <span class=\"string\">&quot;[UNK]&quot;</span>, <span class=\"string\">&quot;[CLS]&quot;</span>, <span class=\"string\">&quot;[SEP]&quot;</span>, <span class=\"string\">&quot;[MASK]&quot;</span>, <span class=\"string\">&quot;&lt;S&gt;&quot;</span>, <span class=\"string\">&quot;&lt;T&gt;&quot;</span></span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果根据训练和测试两个集合训练词元分析器，则需要修改files</span></span><br><span class=\"line\"><span class=\"comment\"># files = [&quot;train.txt&quot;, &quot;test.txt&quot;]</span></span><br><span class=\"line\"><span class=\"comment\"># 仅根据训练集合训练词元分析器</span></span><br><span class=\"line\">files = [<span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/train.txt&quot;</span>]</span><br><span class=\"line\"><span class=\"comment\"># BERT中采用的默认词表大小为30522，可以随意修改</span></span><br><span class=\"line\">vocab_size = <span class=\"number\">30522</span></span><br><span class=\"line\"><span class=\"comment\"># 最大序列长度，该值越小，训练速度越快</span></span><br><span class=\"line\">max_length = <span class=\"number\">512</span></span><br><span class=\"line\"><span class=\"comment\"># 是否将长样本截断</span></span><br><span class=\"line\">truncate_longer_samples = <span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"comment\"># 初始化WordPiece词元分析器</span></span><br><span class=\"line\">tokenizer = BertWordPieceTokenizer()</span><br><span class=\"line\"><span class=\"comment\"># 训练词元分析器，设定的 vocab_size 是最大允许值，但实际生成的词表大小可能更小</span></span><br><span class=\"line\">tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens) </span><br><span class=\"line\"><span class=\"comment\"># 允许截断达到最大512个词元</span></span><br><span class=\"line\">tokenizer.enable_truncation(max_length=max_length)</span><br><span class=\"line\">model_path = <span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果文件夹不存在，则先创建文件夹</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.isdir(model_path):</span><br><span class=\"line\">    os.mkdir(model_path)</span><br><span class=\"line\"><span class=\"comment\"># 保存词元分析器模型</span></span><br><span class=\"line\">tokenizer.save_model(model_path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将一些词元分析器中的配置保存到配置文件，包括特殊词元、转换为小写、最大序列长度等</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(os.path.join(model_path, <span class=\"string\">&quot;config.json&quot;</span>), <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    tokenizer_cfg = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;do_lower_case&quot;</span>: <span class=\"literal\">True</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;unk_token&quot;</span>: <span class=\"string\">&quot;[UNK]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;sep_token&quot;</span>: <span class=\"string\">&quot;[SEP]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;pad_token&quot;</span>: <span class=\"string\">&quot;[PAD]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;cls_token&quot;</span>: <span class=\"string\">&quot;[CLS]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;mask_token&quot;</span>: <span class=\"string\">&quot;[MASK]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;model_max_length&quot;</span>: max_length,</span><br><span class=\"line\">    <span class=\"string\">&quot;max_len&quot;</span>: max_length,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    json.dump(tokenizer_cfg, f)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tokenizer处理序列会自动添加CLS和SEP，但是tokenizer.tokenize()只会进行基础分词，不会添加特殊分词</span></span><br><span class=\"line\">tokenizer = BertTokenizerFast.from_pretrained(<span class=\"string\">&#x27;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;tokenier:<span class=\"subst\">&#123;tokenizer&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">encode_with_truncation</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; 使用词元分析对句子进行处理并截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], truncation=<span class=\"literal\">True</span>, padding=<span class=\"string\">&quot;max_length&quot;</span>,max_length=max_length, return_special_tokens_mask=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">encode_without_truncation</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; 使用词元分析对句子进行处理且不截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], return_special_tokens_mask=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 编码函数将依赖于truncate_longer_samples变量</span></span><br><span class=\"line\">encode = encode_with_truncation <span class=\"keyword\">if</span> truncate_longer_samples <span class=\"keyword\">else</span> encode_without_truncation</span><br><span class=\"line\"><span class=\"comment\"># 对训练数据集进行分词处理</span></span><br><span class=\"line\">train_dataset = d[<span class=\"string\">&quot;train&quot;</span>].<span class=\"built_in\">map</span>(encode, batched=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># 对测试数据集进行分词处理</span></span><br><span class=\"line\">test_dataset = d[<span class=\"string\">&quot;test&quot;</span>].<span class=\"built_in\">map</span>(encode, batched=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> truncate_longer_samples:</span><br><span class=\"line\">    <span class=\"comment\"># 移除其他列，将input_ids和attention_mask设置为PyTorch张量</span></span><br><span class=\"line\">    train_dataset.set_format(<span class=\"built_in\">type</span>=<span class=\"string\">&quot;torch&quot;</span>, columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>])</span><br><span class=\"line\">    test_dataset.set_format(<span class=\"built_in\">type</span>=<span class=\"string\">&quot;torch&quot;</span>, columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>])</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 移除其他列，将它们保留为Python列表</span></span><br><span class=\"line\">    test_dataset.set_format(columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>, <span class=\"string\">&quot;special_tokens_mask&quot;</span>])</span><br><span class=\"line\">    train_dataset.set_format(columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>, <span class=\"string\">&quot;special_tokens_mask&quot;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用于不截断的情况</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">group_texts</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 拼接所有文本</span></span><br><span class=\"line\">    concatenated_examples = &#123;k: <span class=\"built_in\">list</span>(chain(*examples[k])) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> examples.keys()&#125;</span><br><span class=\"line\">    total_length = <span class=\"built_in\">len</span>(concatenated_examples[<span class=\"built_in\">list</span>(examples.keys())[<span class=\"number\">0</span>]])</span><br><span class=\"line\">    <span class=\"comment\"># 舍弃了剩余部分，如果模型支持填充而不是舍弃，则可以根据需要自定义这部分</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> total_length &gt;= max_length:</span><br><span class=\"line\">        total_length = (total_length // max_length) * max_length</span><br><span class=\"line\">    <span class=\"comment\"># 按照最大长度分割成块</span></span><br><span class=\"line\">    result = &#123;</span><br><span class=\"line\">    k: [t[i : i + max_length] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, total_length, max_length)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k, t <span class=\"keyword\">in</span> concatenated_examples.items()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> truncate_longer_samples:</span><br><span class=\"line\">    train_dataset = train_dataset.<span class=\"built_in\">map</span>(group_texts, batched=<span class=\"literal\">True</span>,desc=<span class=\"string\">f&quot;Grouping texts in chunks of <span class=\"subst\">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class=\"line\">    test_dataset = test_dataset.<span class=\"built_in\">map</span>(group_texts, batched=<span class=\"literal\">True</span>,desc=<span class=\"string\">f&quot;Grouping texts in chunks of <span class=\"subst\">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 将它们从列表转换为PyTorch张量</span></span><br><span class=\"line\">    train_dataset.set_format(<span class=\"string\">&quot;torch&quot;</span>)</span><br><span class=\"line\">    test_dataset.set_format(<span class=\"string\">&quot;torch&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;tokenizer.vocab_size:<span class=\"subst\">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#这里vocab_size一般设为tokenizer.vocab_size，其实只要大于这个数字都是可以的，只是会占用了显存空间（training)，但是不能设小，会出现索引越界的问题</span></span><br><span class=\"line\">model_config = BertConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=max_length) </span><br><span class=\"line\">model = BertForMaskedLM(config=model_config)</span><br><span class=\"line\"></span><br><span class=\"line\">data_collator = DataCollatorForLanguageModeling(</span><br><span class=\"line\">    tokenizer=tokenizer, mlm=<span class=\"literal\">True</span>, mlm_probability=<span class=\"number\">0.2</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">training_args = TrainingArguments(</span><br><span class=\"line\">    output_dir=model_path, <span class=\"comment\"># 输出目录，用于保存模型检查点</span></span><br><span class=\"line\">    eval_strategy=<span class=\"string\">&quot;steps&quot;</span>, <span class=\"comment\"># 每隔`logging_steps`步进行一次评估</span></span><br><span class=\"line\">    overwrite_output_dir=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    num_train_epochs=<span class=\"number\">100</span>, <span class=\"comment\"># 训练时的轮数，可以根据需要进行调整</span></span><br><span class=\"line\">    per_device_train_batch_size=<span class=\"number\">10</span>, <span class=\"comment\"># 训练批量大小，可以根据GPU内存容量将其设置得尽可能大</span></span><br><span class=\"line\">    gradient_accumulation_steps=<span class=\"number\">8</span>, <span class=\"comment\"># 在更新权重之前累积梯度</span></span><br><span class=\"line\">    per_device_eval_batch_size=<span class=\"number\">64</span>, <span class=\"comment\"># 评估批量大小</span></span><br><span class=\"line\">    logging_steps=<span class=\"number\">1000</span>, <span class=\"comment\"># 每隔1000步进行一次评估，记录并保存模型检查点</span></span><br><span class=\"line\">    save_steps=<span class=\"number\">1000</span>,</span><br><span class=\"line\">    <span class=\"comment\"># load_best_model_at_end=True, # 是否在训练结束时加载最佳模型（根据损失）</span></span><br><span class=\"line\">    <span class=\"comment\"># save_total_limit=3, # 如果磁盘空间有限，则可以限制只保存3个模型权重</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    model=model,</span><br><span class=\"line\">    args=training_args,</span><br><span class=\"line\">    data_collator=data_collator,</span><br><span class=\"line\">    train_dataset=train_dataset,</span><br><span class=\"line\">    eval_dataset=test_dataset,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#检查点</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids dtype:&quot;</span>, train_dataset[<span class=\"number\">0</span>][<span class=\"string\">&quot;input_ids&quot;</span>].dtype)  <span class=\"comment\"># 应该是 torch.int64 (long)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;attention_mask dtype:&quot;</span>, train_dataset[<span class=\"number\">0</span>][<span class=\"string\">&quot;attention_mask&quot;</span>].dtype)  <span class=\"comment\"># 应该是 torch.int64 (long)</span></span><br><span class=\"line\">batch = data_collator([train_dataset[<span class=\"number\">0</span>], train_dataset[<span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids shape:&quot;</span>, batch[<span class=\"string\">&quot;input_ids&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;batch[&quot;input_ids&quot;]:<span class=\"subst\">&#123;batch[<span class=\"string\">&quot;input_ids&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;labels shape:&quot;</span>, batch[<span class=\"string\">&quot;labels&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"comment\"># print(&quot;labels min/max:&quot;, torch.min(batch[&quot;labels&quot;]), torch.max(batch[&quot;labels&quot;]))</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids min/max:&quot;</span>, torch.<span class=\"built_in\">min</span>(batch[<span class=\"string\">&quot;input_ids&quot;</span>]), torch.<span class=\"built_in\">max</span>(batch[<span class=\"string\">&quot;input_ids&quot;</span>]))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.__version__)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.version.cuda)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.is_available()) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练模型</span></span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n",
            "tags": [
                "Bert",
                "GPT"
            ]
        },
        {
            "id": "http://example.com/2025/05/13/Transformer/",
            "url": "http://example.com/2025/05/13/Transformer/",
            "title": "Transformer",
            "date_published": "2025-05-13T07:07:29.000Z",
            "content_html": "<h2 id=\"Transformer四层结构\"><a href=\"#Transformer四层结构\" class=\"headerlink\" title=\"Transformer四层结构\"></a>Transformer四层结构</h2><p>Transformer结构：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ol>\n<li><p>嵌入表示层</p>\n<p>Transformer的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：</p>\n<p><img src=\"/image2.png\" alt=\"image1\"></p>\n<p>根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#transformer位置编码</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model,max_seq_len = <span class=\"number\">80</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 根据pos和i创建一个常量PE矩阵</span></span><br><span class=\"line\">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> pos <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>):</span><br><span class=\"line\">                pe[pos, i] = math.sin(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">                pe[pos, i + <span class=\"number\">1</span>] = math.cos(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">        pe = pe.unsqueeze(<span class=\"number\">0</span>) <span class=\"comment\">#形状 (1, seq_len, d_model)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">&#x27;pe&#x27;</span>, pe)   <span class=\"comment\">#将 pe 保存为模型的一部分（不参与梯度更新，但会随模型保存/加载）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"comment\">#x:(batch_size, seq_len, d_model)</span></span><br><span class=\"line\">        seq_len = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = x + <span class=\"variable language_\">self</span>.pe[:,:seq_len].cuda()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>使用正余弦的原因是，函数的范围是[-1，1]与词向量相加不会太偏离原始语义，同时第pos+k个位置的编码是第pos个位置编码的线性组合（根据三角函数和角公式决定），这就蕴含了单词之间的距离信息：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n</li>\n<li><p>自注意力层</p>\n<p>自注意力机制，即自己作为QKV进行计算，但是解码器有两个注意力模块，一个是掩码多头，一个是交叉多头注意力，但是原理其实和下面代码差不多，直接用代码展示比较能说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#transformer多头自注意力机制</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, heads, d_model,dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.q_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.k_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.v_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out = nn.Linear(d_model,d_model)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">q, k, v, d_k, mask = <span class=\"literal\">None</span>, dropout = <span class=\"literal\">None</span> </span>):</span><br><span class=\"line\">        <span class=\"comment\"># 转置k相乘 ​​除以 math.sqrt(d_k)​​ 的操作是缩放点积注意力，防止点积数值过大​</span></span><br><span class=\"line\">        scores = torch.matmul(q,k.transpose(-<span class=\"number\">2</span>,-<span class=\"number\">1</span>)) / math.sqrt(d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">            scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>) <span class=\"comment\">#掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        scores = F.sofmax(scores,dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            scores = dropout(scores)</span><br><span class=\"line\"></span><br><span class=\"line\">        output = torch.matmul(scores,v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        batch_size = q.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 利用线性计算划分成h个头</span></span><br><span class=\"line\">        q = <span class=\"variable language_\">self</span>.q_linear(q).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        k = <span class=\"variable language_\">self</span>.k_linear(k).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        v = <span class=\"variable language_\">self</span>.v_linear(v).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#转置头和seq_len位置</span></span><br><span class=\"line\">        k = k.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        q = q.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        v = v.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        scores = <span class=\"variable language_\">self</span>.attention(q, k, v, <span class=\"variable language_\">self</span>.d_k, mask, <span class=\"variable language_\">self</span>.dropout)</span><br><span class=\"line\">        <span class=\"comment\"># 拼接多头输出并线性变换</span></span><br><span class=\"line\">        concat = scores.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(batch_size, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        output = <span class=\"variable language_\">self</span>.out(concat) </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>前馈层</p>\n<p>接收注意力层的输出，通过带有ReLU的2层全连接网络，第一层会映射到高纬度，因为隐藏层维度的增大有利于提高质量（实验证明）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#前馈层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FeedForward</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff = <span class=\"number\">2038</span>, dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear2 = nn.Linear(d_ff,d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.linear1(x)))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.linear2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>残差连接和归一化</p>\n<p>​    由Transformer结构组成的网络结构通常都非常庞大。编码器和解码器均由很多层基本的Transformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究人员在 Transformer 块中进一步引入了残差连接与层归一化技术，以进一步提升训练的稳定性。具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。</p>\n</li>\n</ol>\n<h2 id=\"解码器与编码器\"><a href=\"#解码器与编码器\" class=\"headerlink\" title=\"解码器与编码器\"></a>解码器与编码器</h2><p>​\t编码器端较容易实现。相比于编码器端，解码器端更复杂。具体来说，解码器的每个 Transformer 块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力部分。这主要是因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息。解码器端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的，因此这一额外增加的掩码是用来掩盖后续的文本信息的，以防模型在训练阶段直接看到后续的文本序列，进而无法得到有效的训练。此外，解码器端额外增加了一个多头交叉注意力模块，使用交叉注意力方法，同时接收来自编码器端的输出和当前 Transformer 块的前一个掩码注意力层的输出。查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。</p>\n<p>​\t解码器端以自回归的方式生成目标语言文本，即在每个时间步 <em>t</em>，根据编码器端输出的源语言文本表示，以及前t <em>−</em> 1 个时刻生成的目标语言文本，生成当前时刻的目标语言单词（以我的理解来说，训练阶段是没有显示时间步概念的，通过<strong>一次性输入完整序列 + 掩码矩阵</strong>，在单次前向传播中并行计算出所有位置的输出，同时利用掩码强制模型行为与自回归一致，而推理时必须显式按时间步生成，因为未来词未知（无法并行））。</p>\n<h2 id=\"以推理生成中文翻译-我爱你-为例：\"><a href=\"#以推理生成中文翻译-我爱你-为例：\" class=\"headerlink\" title=\"以推理生成中文翻译 &quot;我爱你&quot; 为例：\"></a>以推理生成中文翻译 <code>&quot;我爱你&quot;</code> 为例：</h2><table>\n<thead>\n<tr>\n<th align=\"center\">时间步</th>\n<th align=\"center\">图1中对应的模块流程</th>\n<th align=\"center\">具体操作</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><code>t=1</code></td>\n<td align=\"center\">词元嵌入 → 位置编码 → 掩码多头注意力 → 编码器-解码器注意力 → 前馈网络 → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt;</code>，输出 <code>&quot;我&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=2</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我</code>) → 位置编码 → 掩码多头注意力 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我</code>，输出 <code>&quot;爱&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=3</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱</code>) → 位置编码 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱</code>，输出 <code>&quot;你&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=4</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱你</code>) → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱你</code>，输出 <code>&lt;end&gt;</code> 的概率分布，终止生成。</td>\n</tr>\n</tbody></table>\n<p>可参考文章：<a href=\"https://blog.csdn.net/m0_64148253/article/details/140422497\">https://blog.csdn.net/m0_64148253/article/details/140422497</a></p>\n",
            "tags": [
                "LLM",
                "transformer"
            ]
        },
        {
            "id": "http://example.com/2025/05/12/LLM-concept/",
            "url": "http://example.com/2025/05/12/LLM-concept/",
            "title": "LLM-concept",
            "date_published": "2025-05-12T07:54:00.000Z",
            "content_html": "<h1 id=\"大模型基本概念\"><a href=\"#大模型基本概念\" class=\"headerlink\" title=\"大模型基本概念\"></a>大模型基本概念</h1><h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ul>\n<li><p>语言模型就是对自然语言的概率分布进行建模，即P(w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p>\n</li>\n<li><p>计算下一个词的概率 P(wn | w1 w2 w3… wn-1) </p>\n<p><img src=\"/image1.png\" alt=\"image\"></p>\n</li>\n</ul>\n<h2 id=\"发展历程\"><a href=\"#发展历程\" class=\"headerlink\" title=\"发展历程\"></a>发展历程</h2><p>从n-gram:</p>\n<p><img src=\"/image2.png\" alt=\"image\"></p>\n<p>到neural language model: 每个词都映射成一个低维向量</p>\n<p><img src=\"/image3.png\" alt=\"image\"></p>\n<p>再到后面的transformer出现，transformer的出现，NLP进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是PLM。</p>\n<p>随着OpenAI发布的1750亿个参数（GPT-3），开启LLM时代</p>\n<h2 id=\"问题发现\"><a href=\"#问题发现\" class=\"headerlink\" title=\"问题发现\"></a>问题发现</h2><p> • 大模型（如GPT-3）参数量极大（1750亿+），传统“预训练+微调”范式成本过高（需为每个任务调整海量参数）。</p>\n<ol>\n<li><p>解决方案：<br>• 开发新范式（ICL&#x2F;Prompt），通过输入指令或示例直接引导模型，避免微调。</p>\n<p>• 但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p>\n</li>\n<li><p>模型构建的关键：<br>• 预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握ICL&#x2F;Prompt所需的能力（如任务模式识别、指令遵循）。</p>\n<p>• 后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p>\n</li>\n<li><p>结论：<br>• 新范式（ICL&#x2F;Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p>\n</li>\n</ol>\n<h2 id=\"LLM的构建流程\"><a href=\"#LLM的构建流程\" class=\"headerlink\" title=\"LLM的构建流程\"></a>LLM的构建流程</h2><ul>\n<li>预训练： 利用海量训练数据构建多样化内容，构建基础模型——&gt;对长文本建模，使模型具有语言生成能力</li>\n<li>有监督微调SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li>\n<li>奖励建模RM：训练一个能够判断文本质量的裁判，对同个提示词，比较SFT生成的多个输出的质量</li>\n<li>强化学习RLHF(human feedback)：基于RM，优化SFT模型</li>\n</ul>\n<p>SFT相当于学生学会答题，RM是评分老师，判断answer好坏，RLHF是学生根据老师评分改进答题策略</p>\n<h2 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h2><p><strong>N-gram 模型详解</strong><br> N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p>\n<p> N-gram的定义：</p>\n<p>• 指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p>\n<p>• 例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure>\n\n<p>• 核心假设：</p>\n<p>• 马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p><strong>如何计算概率？</strong><br> N-gram 通过统计语料库中词序列的频率来估计概率：</p>\n<p>计算 <code>P(sat | the cat)</code>：</p>\n<p>P(sat∣the cat)&#x3D;Count(“the cat”)Count(“the cat sat”)</p>\n<p>若语料中 “the cat” 出现 100 次，”the cat sat” 出现 30 次，则 <code>P(sat | the cat) = 0.3</code>。</p>\n<p><strong>N-gram 的优缺点</strong></p>\n<table>\n<thead>\n<tr>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>简单高效，计算速度快。</td>\n<td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td>\n</tr>\n<tr>\n<td>小规模数据即可训练。</td>\n<td>数据稀疏性（罕见 n-gram 概率不准确）。</td>\n</tr>\n<tr>\n<td>曾广泛用于机器翻译、拼写检查等任务。</td>\n<td>无法理解语义（仅统计共现频率）。</td>\n</tr>\n</tbody></table>\n",
            "tags": [
                "LLM",
                "concept"
            ]
        }
    ]
}