{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"llm\" category",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/12/LLM-concept/",
            "url": "http://example.com/2025/05/12/LLM-concept/",
            "title": "LLM-concept",
            "date_published": "2025-05-12T07:44:46.000Z",
            "content_html": "<h1 id=\"大模型基本概念\"><a href=\"#大模型基本概念\" class=\"headerlink\" title=\"大模型基本概念\"></a>大模型基本概念</h1><h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ul>\n<li><p>语言模型就是对自然语言的概率分布进行建模，即P(w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p>\n</li>\n<li><p>计算下一个词的概率 P(wn | w1 w2 w3… wn-1) </p>\n<p><img src=\"C:\\Users\\34174\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250512152219575.png\" alt=\"image-20250512152219575\"></p>\n</li>\n</ul>\n<h2 id=\"发展历程\"><a href=\"#发展历程\" class=\"headerlink\" title=\"发展历程\"></a>发展历程</h2><p>从n-gram:</p>\n<p><img src=\"C:\\Users\\34174\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250512152520160.png\" alt=\"image-20250512152520160\"></p>\n<p>到neural language model: 每个词都映射成一个低维向量</p>\n<p><img src=\"C:\\Users\\34174\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250512152628359.png\" alt=\"image-20250512152628359\"></p>\n<p>再到后面的transformer出现，transformer的出现，NLP进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是PLM。</p>\n<p>随着OpenAI发布的1750亿个参数（GPT-3），开启LLM时代</p>\n<h2 id=\"问题发现\"><a href=\"#问题发现\" class=\"headerlink\" title=\"问题发现\"></a>问题发现</h2><p> • 大模型（如GPT-3）参数量极大（1750亿+），传统“预训练+微调”范式成本过高（需为每个任务调整海量参数）。</p>\n<ol>\n<li><p>解决方案：<br>• 开发新范式（ICL&#x2F;Prompt），通过输入指令或示例直接引导模型，避免微调。</p>\n<p>• 但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p>\n</li>\n<li><p>模型构建的关键：<br>• 预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握ICL&#x2F;Prompt所需的能力（如任务模式识别、指令遵循）。</p>\n<p>• 后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p>\n</li>\n<li><p>结论：<br>• 新范式（ICL&#x2F;Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p>\n</li>\n</ol>\n<h2 id=\"LLM的构建流程\"><a href=\"#LLM的构建流程\" class=\"headerlink\" title=\"LLM的构建流程\"></a>LLM的构建流程</h2><ul>\n<li>预训练： 利用海量训练数据构建多样化内容，构建基础模型——&gt;对长文本建模，使模型具有语言生成能力</li>\n<li>有监督微调SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li>\n<li>奖励建模RM：训练一个能够判断文本质量的裁判，对同个提示词，比较SFT生成的多个输出的质量</li>\n<li>强化学习RLHF(human feedback)：基于RM，优化SFT模型</li>\n</ul>\n<p>SFT相当于学生学会答题，RM是评分老师，判断answer好坏，RLHF是学生根据老师评分改进答题策略</p>\n<h2 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h2><p><strong>N-gram 模型详解</strong><br> N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p>\n<p> N-gram的定义：</p>\n<p>• 指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p>\n<p>• 例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure>\n\n<p>• 核心假设：</p>\n<p>• 马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p><strong>如何计算概率？</strong><br> N-gram 通过统计语料库中词序列的频率来估计概率：</p>\n<p>计算 <code>P(sat | the cat)</code>：</p>\n<p>P(sat∣the cat)&#x3D;Count(“the cat”)Count(“the cat sat”)</p>\n<p>若语料中 “the cat” 出现 100 次，”the cat sat” 出现 30 次，则 <code>P(sat | the cat) = 0.3</code>。</p>\n<p><strong>N-gram 的优缺点</strong></p>\n<table>\n<thead>\n<tr>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>简单高效，计算速度快。</td>\n<td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td>\n</tr>\n<tr>\n<td>小规模数据即可训练。</td>\n<td>数据稀疏性（罕见 n-gram 概率不准确）。</td>\n</tr>\n<tr>\n<td>曾广泛用于机器翻译、拼写检查等任务。</td>\n<td>无法理解语义（仅统计共现频率）。</td>\n</tr>\n</tbody></table>\n",
            "tags": [
                "LLM",
                "concept"
            ]
        }
    ]
}