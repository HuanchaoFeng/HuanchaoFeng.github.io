<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>BIBM paper: CFPLM | Phoenix</title><meta name="author" content="Phoenix"><meta name="copyright" content="Phoenix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="# CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models # Abstract Protein-RNA interactions (PRIs) play pivotal roles in biological processes such">
<meta property="og:type" content="article">
<meta property="og:title" content="BIBM paper: CFPLM">
<meta property="og:url" content="http://example.com/2025/11/11/BIBM-paper-CFPLM/index.html">
<meta property="og:site_name" content="Phoenix">
<meta property="og:description" content="# CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models # Abstract Protein-RNA interactions (PRIs) play pivotal roles in biological processes such">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-11-11T11:21:54.000Z">
<meta property="article:modified_time" content="2025-11-11T11:58:01.298Z">
<meta property="article:author" content="Phoenix">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "BIBM paper: CFPLM",
  "url": "http://example.com/2025/11/11/BIBM-paper-CFPLM/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2025-11-11T11:21:54.000Z",
  "dateModified": "2025-11-11T11:58:01.298Z",
  "author": [
    {
      "@type": "Person",
      "name": "Phoenix",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/11/11/BIBM-paper-CFPLM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BIBM paper: CFPLM',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-folder"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default_top_img.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Phoenix</span></a><a class="nav-page-title" href="/"><span class="site-name">BIBM paper: CFPLM</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-folder"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">BIBM paper: CFPLM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-11-11T11:21:54.000Z" title="Created 2025-11-11 19:21:54">2025-11-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-11T11:58:01.298Z" title="Updated 2025-11-11 19:58:01">2025-11-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>21mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models"><a class="markdownIt-Anchor" href="#cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models">#</a> CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models</h1>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract">#</a> Abstract</h2>
<p>Protein-RNA interactions (PRIs) play pivotal roles in biological processes such as gene regulation, making their prediction essential for therapeutic and mechanistic studies. While traditional wet-lab methods are time-consuming and challenging, computational approaches offer efficient alternatives. Graph-based methods show promise by capturing both direct interactive domains (protein-RNA interaction) and indirect collaborative domains (functional similarity among proteins/RNAs). However, integrating these domains and learning meaningful node representations remain critical challenges. To address this, we propose ​​CFPLM​​, a collaborative framework fusing large language models, graph convolutional networks, and cross-attention mechanisms to improve PRI prediction. Experiment results demonstrate that CFPLM achieves robust, state-of-the-art performance across three benchmark datasets. It’s anticipated to have applicability to similar other interaction prediction tasks.</p>
<h2 id="iintroduction"><a class="markdownIt-Anchor" href="#iintroduction">#</a> Ⅰ.Introduction</h2>
<p>As two essential biological macromolecules, the interaction between proteins and RNA plays a crucial role in cellular activities, such as gene expression regulation, translation control, and viral replication [1].Traditional biological experiments can identify direct interactions between RNA and protein with confidence. However, they are complicated and challenging, making them expensive to verify each potential interaction individually [2].<br>
In response to these challenges, researchers are encouraged to develop more efficient and economical computational methods to aid in screening and predicting potential interactions, thereby enhancing research efficiency and productivity. Among the various calculation methods currently available, the machine learning-based method is the most widely used in this field. These methods can be divided into two groups: one involves feature learning and classification, and the other is graph-based association prediction. The former extracts features based on protein and RNA sequences or structures, and constructs a classification model based on these features to determine whether a given protein can interact with an RNA. For example, EnANNDeep combines an adaptive K-nearest neighbor classifier, a deep neural network, and a deep forest model, and integrates the prediction results of the three models using a weighted average to improve classification performance [3]. LPIDF utilizes self-encoder technology to extract sequence features of lncRNA and protein, and combines an ensemble learning method to predict possible interactions [4]. In MHAM, a deep learning model based on a multi-head attention mechanism and a residual link is used to predict the interaction between RNA and protein [5]. LGFC-CNN combines the original sequence, artificial, and structural features to predict PRI through deep learning [6]. LPI-CNNCP uses convolutional neural networks and replication filling techniques to predict the interaction between lncRNA and protein [7]. AptaNet [8] and AptaTrans [9] employ deep neural networks and transformer-based encoders, respectively, to capture the complex interactions between proteins and nucleic acid aptamers. The advantage of this type of method is that it can predict the potential interaction between any given protein and RNA. However, the interaction patterns between proteins and RNA are complex, and currently known data on protein-RNA interactions is limited, making it difficult to train a reliable end-to-end prediction model. Therefore, existing methods are difficult to generalize excellent predictive performance to different datasets.<br>
Graph-based methods have emerged as particularly promising approaches. Because it can better utilize existing knowledge through molecular similarity networks and interaction networks. For example, LPICGAE is a combination method based on a graph self-encoder, which is used to predict PRI [10]. BiHo-GNN is the first method to integrate isomorphic and heterogeneous networks through bipartite graph embedding to predict PRI [11]. NPI-GNN was proposed for predicting noncoding RNA–protein interactions using graph neural networks[12]. CCGNN proposed a new method to predict PRI by incorporating information from the interaction domain and cooperation domain [13]. From the above research, it is evident that collaborative domain construction and node representation learning are key factors in determining this method. However, when constructing collaborative domains and node representation, most existing graph-based methods fail to fully capture the complex information in RNA and protein sequences, which results in limited performance of the finally trained models.<br>
In recent years, the breakthrough progress of large language models (LLMs) has transformed various aspects of life, including scientific research paradigms. Currently, numerous large language models have been trained on RNA and protein sequences, such as EVO [14], BIRNA-BERT [15], RNA-FM [16], ESM [17], and ProteinBERT [18], which have been widely utilized in biological tasks, including interaction prediction, and have achieved notable success. For example, PAIR used an RNA-FM language model to get the embedding vector of nucleic acid aptamer, and performed well in protein-nucleic acid aptamer interaction task [19]; In DTI-LM [20], ChemBERTa [21] and ESM language models are used to generate embedding vectors for drug molecules and protein, and show good performance in their prediction tasks . These studies demonstrate that LLMs have the potential to offer a more powerful approach to constructing collaborative domains and characterizing the nodes. However, there is currently a lack of research to explore this point. Moreover, most graph-based methods operate on information from the collaborative domain and interaction information within RNA or protein separately, thereby ignoring some useful cross-information and dependence between RNA and protein. How to better combine LLMs with other technologies to address the challenges in PRI prediction remains to be explored.<br>
To address these gaps, we propose a novel framework, named CFPLM, for identifying protein-RNA interactions that utilizes a cross-attention fusion neural network in conjunction with large language models. The contributions of this study are as follows:</p>
<ul>
<li>(1) We introduce a new strategy for constructing collaborative domains based on both protein and RNA language models, which provides a more reliable method for knowledge construction in graph-based approaches.</li>
<li>(2) We design a new learning model for protein-RNA interactions by combining a graph convolutional neural network (GCN) with a cross-attention mechanism, thereby overcoming the limitations of existing graph learning models in this field.<br>
The experimental results demonstrate the proposed framework is more efficient and accurate for predicting protein-RNA interactions, offering valuable insights into the interaction mechanisms between RNA and protein, as well as related biological processes.</li>
</ul>
<h2 id="iimaterial-and-method"><a class="markdownIt-Anchor" href="#iimaterial-and-method">#</a> II.Material and Method</h2>
<ul>
<li>A.Datasets<br>
The benchmark dataset used in this study was constructed based on the protein-RNA complex released from the PDB (Protein Data Bank) database between 1989 and 2023. We labeled a protein chain interacts with an RNA according to the spatial distance between in the complex. According to the previous research [22], the distance threshold is set to 3.5 angstroms (protein-RNA pairs with spatial distance less than 3.5 angstroms are interacted). To reduce data redundancy and ensure objective experimental results, we used BLAST to cluster proteins and RNAs, respectively, based on sequence similarity (with threshold S=90). We then randomly select a sequence from each cluster to build a benchmark database. To avoid noise caused by incomplete protein fragments, we filter out protein sequences with lengths less than 50. Finally, 2,465 RNA sequences, 5,897 protein sequences, and 8,579 pairs of interaction information were obtained. According to previous work [23], we adopt the following protocol for processing the dataset: randomly sampling the same number of protein-RNA pairs that are not interactions as negative samples.<br>
For a comprehensive evaluation, we also employed another two datasets: miRTarBase (version 8.0) [24] and a protein-lncRNA dataset [10]. There are 3,924 miRNAs, 20,992 target genes, and 186,416 verified interactions in miRTarBase. While 3,046 lncRNAs, 136 proteins, and 8,112 interactions in the protein-lncRNA dataset.</li>
<li>B.Framework of CFPLM<br>
CFPLM includes three modules (Fig. 1): 1) Feature extraction, which uses large language models to extract initial feature vectors of RNA and protein; 2) Graph structure, which uses cosine similarity to calculate the similarity of RNA-RNA and protein-protein, and then obtains the similarity graph, and at the same time transforms the RNA-protein interaction pair into an interaction graph; 3) Feature optimization, which uses graph convolution network to capture the complex features of RNA and protein, and uses cross attention to fuse the feature vectors of similarity domain and interaction domain. The first and second points are shown in Fig. 1 (a), and the third point is shown in Fig. 1 (b-c).</li>
<li>(1)Feature Extraction
<ul>
<li>a)RNA Encoder: BIRNA-BERT is an RNA language model based on Transformer architecture, which is characterized by adopting a double labeling scheme, combining nucleotide-level labeling and byte-pair coding labeling, and can dynamically adjust the labeling strategy according to the sequence length and computing resources, so that the model can handle long-sequence tasks while retaining the ability to handle short-sequence tasks using nucleotide-level information. RNA-FM, the basic RNA model based on self-supervised learning, has been proven effective in multiple tasks for the first time. Another example is the latest Evo, which demonstrates the powerful ability of genome language models in function-oriented design. Regarding the choice of language model, we will compare the above language models in the results section. Ultimately, we chose to use the BIRNA-BERT model based on its ease of use, as it can be well adapted to the task and saves computing resources and experimental time.</li>
<li>b)Protein Encoder: ESM-2 is a large language model for protein sequences. The model can learn the evolution pattern in protein sequences by training on the modeling target of masked language and transform it into a deep understanding of protein structure. ProteinBERT is also a language model specially designed for protein sequences. ProteinBERT enhances the efficiency and performance of protein sequence analysis by integrating a novel pre-training task that combines language modeling and gene ontology annotation prediction. The model has performed well in several protein task benchmarks, approaching or exceeding the performance of existing large-scale models [18]. We compared the above language models and finally adopt the ESM-2 model. In this study, we employ a protein language model based on the ESM-2 architecture with 150M parameters, and train it on the UniRef50 dataset [25].</li>
</ul>
</li>
<li>(2)Graph Construction<br>
In DTI-LM, the feature expression is improved by calculating the similarity protein-protein and drug. The similarity function is used in CCGNN to construct the similarity map between protein and RNA. FMSRT constructs multi-source similarity map according to similarity [26] . The success of these studies shows that similarity information can provide rich information for the study of biomolecular interaction. In this study, we will use the cosine similarity calculation method to calculate the similarity of RNA feature vectors and protein feature vectors respectively, and convert them into RNA-RNA similarity graph and Protein-Protein similarity graph. At the same time, NPI-GNN model uses the bipartite graph of ncRNA-protein to infer the interaction [12], and GANLDA model uses the bipartite graph of lncRNA and diseases to predict the interaction [27]. These methods combine the topological information of PRI networks, and effectively improve the performance of the model in link prediction tasks. In this study, we construct an RNA-protein interaction bipartite graph for feature optimization in graph neural networks.</li>
<li>(3)Feature Optimization
<ul>
<li>a)Graph convolution network: GCN is a spectral-based graph neural network that processes graph-structured data by aggregating neighborhood information to learn low-dimensional node representations. It efficiently preserves local structure while supporting tasks like node classification and link prediction.  The RNA feature matrix and protein feature matrix will enter the graph convolution network with three graphs, and finally get the updated node feature vector.</li>
<li>b)Cross Attention: To better capture the interaction and similarity information between RNA and proteins, a cross-attention mechanism is introduced. Its core lies in dynamically learning the interactions between the two through the attention mechanism, so a cross-attention module is designed in the model to handle such information. This module contains 4 attention heads, each with a feature dimension of 64. Fig. 1 © illustrates the calculation process of the cross-attention module, where, for the RNA and protein feature vector matrices obtained from the interaction graph, RNA serves as the Query and protein acts as the Key and Value, and vice versa. Meanwhile, the RNA and protein matrices derived from the similarity graph are also optimized and updated using this method.</li>
<li>c)Loss Function: In this study, we use binary cross entropy as the loss function, calculating the average of all samples when calculating the loss.</li>
</ul>
</li>
</ul>
<p>FrameWork:<br>
<img src="image4.png" alt="image1"></p>
<h2 id="references"><a class="markdownIt-Anchor" href="#references">#</a> References</h2>
<p>[1]E. Jankowsky, and M. E. J. N. r. M. c. b. Harris, “Specificity and nonspecificity in RNA–protein interactions,” Nature Reviews Molecular Cell Biology, vol. 16, no. 9, pp. 533-544, 2015.<br>
[2]M. Philip, T. Chen, and S. Tyagi, “A Survey of Current Resources to Study lncRNA-Protein Interactions,” Noncoding RNA, vol. 7, no. 2, pp. 33, Jun 8, 2021.<br>
[3]L. Peng, J. Tan, X. Tian, and L. Zhou, “EnANNDeep: An Ensemble-based lncRNA-protein Interaction Prediction Framework with Adaptive k-Nearest Neighbor Classifier and Deep Models,” Interdisciplinary Sciences: Computational Life Sciences, vol. 14, no. 1, pp. 209-232, Mar, 2022.<br>
[4]X. Tian, L. Shen, Z. Wang, L. Zhou, and L. Peng, “A novel lncRNA-protein interaction prediction method based on deep forest with cascade forest structure,” Scientific Reports, vol. 11, no. 1, pp. 18881, Sep 23, 2021.<br>
[5]Z. Zhou, Z. Du, J. Wei, L. Zhuo, S. Pan, X. Fu, and X. Lian, “MHAM-NPI: Predicting ncRNA-protein interactions based on multi-head attention mechanism,” Computers in Biology and Medicine, vol. 163, pp. 107143, Sep, 2023.<br>
[6]L. Huang, S. Q. Jiao, S. Yang, S. Q. Zhang, X. P. Zhu, R. Guo, and Y. Wang, “LGFC-CNN: Prediction of lncRNA-Protein Interactions by Using Multiple Types of Features through Deep Learning,” Genes, vol. 12, no. 11, pp. 1689, Nov, 2021.<br>
[7]S. W. Zhang, X. X. Zhang, X. N. Fan, and W. N. Li, “LPI-CNNCP: Prediction of lncRNA-protein interactions by using convolutional neural network with the copy-padding trick,” Analytical Biochemistry, vol. 601, pp. 113767, Jul 15, 2020.<br>
[8]N. Emami, and R. Ferdousi, “AptaNet as a deep learning approach for aptamer–protein interaction prediction,” Scientific reports, vol. 11, no. 1, pp. 6074, 2021.<br>
[9]I. Shin, K. Kang, J. Kim, S. Sel, J. Choi, J.-W. Lee, H. Y. Kang, and G. Song, “AptaTrans: a deep neural network for predicting aptamer-protein interaction using pretrained encoders,” BMC Bioinformatics, vol. 24, no. 1, pp. 447, 2023/11/27, 2023.<br>
[10]J. Zhao, J. Sun, S. C. Shuai, Q. Zhao, and J. Shuai, “Predicting potential interactions between lncRNAs and proteins via combined graph auto-encoder methods,” Briefings in Bioinformatics, vol. 24, no. 1, Jan 19, 2023.<br>
[11]Y. Ma, H. Zhang, C. Jin, and C. Kang, “Predicting lncRNA-protein interactions with bipartite graph embedding and deep graph neural networks,” Frontiers in Genetics, vol. 14, pp. 1136672, 2023.<br>
[12]Z.-A. Shen, T. Luo, Y.-K. Zhou, H. Yu, and P.-F. J. B. i. b. Du, “NPI-GNN: Predicting ncRNA–protein interactions with deep graph neural networks,” Briefings in Bioinformatics, vol. 22, no. 5, pp. bbab051, 2021.<br>
[13]H. Li, B. Wu, M. Sun, Z. Zhu, K. Chen, and H. Ge, “Cross-domain contrastive graph neural network for lncRNA–protein interaction prediction,” Knowledge-Based Systems, vol. 296, 2024.<br>
[14]E. Nguyen, M. Poli, M. G. Durrant, B. Kang, D. Katrekar, D. B. Li, L. J. Bartie, A. W. Thomas, S. H. King, G. Brixi, J. Sullivan, M. Y. Ng, A. Lewis, A. Lou, S. Ermon, S. A. Baccus, T. Hernandez-Boussard, C. Ré, P. D. Hsu, and B. L. Hie, “Sequence modeling and design from molecular to genome scale with Evo,” Science, vol. 386, no. 6723, pp. eado9336, 2024.<br>
[15]M. T. Tahmid, H. S. Shahgir, S. Mahbub, Y. Dong, and M. S. J. b. Bayzid, “BiRNA-BERT allows efficient RNA language modeling with adaptive tokenization,” bioRxiv:2024.07.02.601703, 2024.<br>
[16]J. Chen, Z. Hu, S. Sun, Q. Tan, Y. Wang, Q. Yu, L. Zong, L. Hong, J. Xiao, and T. J. a. p. a. Shen, “Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions,” arXiv preprint  arXiv:2204.00300, 2022.<br>
[17]Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. Dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, “Evolutionary-scale prediction of atomic-level protein structure with a language model,” Science, vol. 379, no. 6637, pp. 1123-1130, Mar 17, 2023.<br>
[18]N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial, “ProteinBERT: a universal deep-learning model of protein sequence and function,” Bioinformatics, vol. 38, no. 8, pp. 2102-2110, Apr 12, 2022.<br>
[19]J. Zhang, Z. Yan, H. Zeng, and Z. Zhu, “PAIR: protein-aptamer interaction prediction based on language models and contrastive learning framework.” 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), Lisbon, Portugal, 2024, pp. 5426-5432<br>
[20]K. T. Ahmed, M. I. Ansari, and W. J. B. Zhang, “DTI-LM: language model powered drug–target interaction prediction,” Bioinformatics, vol. 40, no. 9, pp. btae533, 2024.<br>
[21]S. Chithrananda, G. Grand, and B. J. a. p. a. Ramsundar, “ChemBERTa: large-scale self-supervised pretraining for molecular property prediction,” arXiv preprint, arXiv:2010.09885, 2020.<br>
[22]J. Yan, and L. Kurgan, “DRNApred, fast sequence-based method that accurately predicts and discriminates DNA- and RNA-binding residues,” Nucleic Acids Research, vol. 45, no. 10, pp. e84-e84, Jun 2, 2017.<br>
[23]Y. Li, H. Sun, S. Feng, Q. Zhang, S. Han, and W. Du, “Capsule-LPI: a LncRNA-protein interaction predicting tool based on a capsule network,” BMC Bioinformatics, vol. 22, no. 1, pp. 246, May 13, 2021.<br>
[24]H.-Y. Huang, Y.-C.-D. Lin, J. Li, K.-Y. Huang, S. Shrestha, H.-C. Hong, Y. Tang, Y.-G. Chen, C.-N. Jin, Y. Yu, J.-T. Xu, Y.-M. Li, X.-X. Cai, Z.-Y. Zhou, X.-H. Chen, Y.-Y. Pei, L. Hu, J.-J. Su, S.-D. Cui, F. Wang, Y.-Y. Xie, S.-Y. Ding, M.-F. Luo, C.-H. Chou, N.-W. Chang, K.-W. Chen, Y.-H. Cheng, X.-H. Wan, W.-L. Hsu, T.-Y. Lee, F.-X. Wei, and H.-D. Huang, “miRTarBase 2020: updates to the experimentally validated microRNA–target interaction database,” Nucleic Acids Research, vol. 48, no. D1, pp. D148-D154, 2019.<br>
[25]B. E. Suzek, H. Huang, P. McGarvey, R. Mazumder, and C. H. Wu, “UniRef: comprehensive and non-redundant UniProt reference clusters,” Bioinformatics, vol. 23, no. 10, pp. 1282-8, May 15, 2007.<br>
[26]X. Zhang, M. Liu, Z. Li, L. Zhuo, X. Fu, and Q. Zou, “Fusion of multi-source relationships and topology to infer lncRNA-protein interactions,” Molecular Therapy Nucleic Acids, vol. 35, no. 2, pp. 102187, Jun 11, 2024.<br>
[27]W. Lan, X. M. Wu, Q. F. Chen, W. Peng, J. X. Wang, and Y. P. Chen, “GANLDA: Graph attention network for lncRNA-disease associations prediction,” Neurocomputing, vol. 469, pp. 384-393, Jan 16, 2022.<br>
[28]Q. Le, and T. Mikolov, “Distributed representations of sentences and documents,” 2014 International Conference on Machine Learning, Beijing,China, 2014,pp. 1188-1196.<br>
[29]S. Yang, Y. Wang, Y. Lin, D. Shao, K. He, and L. J. M. Huang, “LncMirNet: predicting LncRNA–miRNA interaction based on deep learning of ribonucleic acid sequences,” Molecules, vol. 25, no. 19, pp. 4372, 2020.<br>
[30]Z. Wang, S. Liang, S. Liu, Z. Meng, J. Wang, and S. Liang, “Sequence pre-training-based graph neural network for predicting lncRNA-miRNA associations,” Briefings in Bioinformatics, vol. 24, no. 5, pp. bbad317, Sep 20, 2023.<br>
[31]Y. Han, and S. W. Zhang, “ncRPI-LGAT: Prediction of ncRNA-protein interactions with line graph attention network framework,” Computational and Structural Biotechnology Journal, vol. 21, pp. 2286-2295, 2023.<br>
[32]H. Ji, X. Wang, C. Shi, B. Wang, P. S. J. I. T. o. K. Yu, and D. Engineering, “Heterogeneous graph propagation network,” IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 521-532, 2021.<br>
[33]L. v. d. Maaten, and G. J. J. o. m. l. r. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579-2605, 2008.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Phoenix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/11/11/BIBM-paper-CFPLM/">http://example.com/2025/11/11/BIBM-paper-CFPLM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/25/Datalinkx%E6%A8%A1%E5%9D%97%E8%AE%B2%E8%A7%A31/" title="Datalinkx模块讲解1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Datalinkx模块讲解1</div></div><div class="info-2"><div class="info-item-1"># Datalinkx-messageHub 与 Datalinkx-SSE  用于 datalinkx-job 和 datalinkx-server 之间的异步通信，这个模块只有在推送流转进度中才使用，job 是生成者，检测流转任务的进度数据后，调用生产者 api 把数据发送到 redis 中，datlainkx-sse 是消费者，注册一个常驻代理线程，实时监控 redis 中的数据，监听到后，回调到标记了 @MessageHub 注解的方法中  # Redis stream 结构作为消息队列  一条消息只能由一个消费者组中的一个消费者消费。  # TopicReloadTask 类  是一个定时任务，用来周期性从数据库中读取所有 topic，并把他们更新到 redis 的白名单集合中，更新时采用删除原来的名单，再写入新的名单（lua 脚本控制原子性，避免删除后失败，未写入新名单） 用于几个地方：发送端检查是否有该 topic、消费端是否订阅了存在的 topic。实际上本项目只需要用 1 个 topic 即可  # TopicDaemonWarden 类  用于注册定时执行器 ...</div></div></div></a><a class="pagination-related" href="/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/" title="LLaMA&amp;混合专家模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">LLaMA&混合专家模型</div></div><div class="info-2"><div class="info-item-1"># LLaMA  model  structure ​	LLaMA 是基于 transformer 的 decoder 部分构建的，采用前置层归一化、使用 RMSNorm 规划函数，激活函数更改为 SwiGLU，使用旋转位置嵌入更改的 decoder 模型。更改的位置如下所示：    RMSNorm 函数：  原始层归一化函数：     对比 LayerNorm RMSNorm     归一化目标 均值中心化 + 方差缩放 仅均方根（RMS）缩放   计算复杂度 较高（需计算均值和方差） 较低（仅需均方值）   参数数量 γ+β（2d 参数） 仅 γ（d 参数）      SwiGLU SwiGLU 是门控线性单元（GLU）的变体，公式如下：   第二个公式的激活函数是 sigmoid，sigmoid 函数特点：    当 β 趋向于 0 时，相当于 y=x/2，线性函数，当 β 趋向于无穷时（x&gt;0,x&lt;0,x=0)，相当于 ReLU 函数，当 β=1，swish 光滑且非单调。 Swish (xW) 为门控权重（相当于选择遗忘比例），用权重对 xV 逐元素加权，用...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/05/12/LLM-concept/" title="LLM-concept"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-12</div><div class="info-item-2">LLM-concept</div></div><div class="info-2"><div class="info-item-1"># 大模型基本概念 # 目标   语言模型就是对自然语言的概率分布进行建模，即 P (w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率   计算下一个词的概率 P (wn | w1 w2 w3… wn-1)    # 发展历程 从 n-gram:  到 neural language model: 每个词都映射成一个低维向量  再到后面的 transformer 出现，transformer 的出现，NLP 进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是 PLM。 随着 OpenAI 发布的 1750 亿个参数（GPT-3），开启 LLM 时代 # 问题发现 ・大模型（如 GPT-3）参数量极大（1750 亿 +），传统 “预训练 + 微调” 范式成本过高（需为每个任务调整海量参数）。   解决方案： ・开发新范式（ICL/Prompt），通过输入指令或示例直接引导模型，避免微调。 ・但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。   ...</div></div></div></a><a class="pagination-related" href="/2025/05/13/Transformer/" title="Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-13</div><div class="info-item-2">Transformer</div></div><div class="info-2"><div class="info-item-1"># Transformer 四层结构 Transformer 结构：    嵌入表示层 Transformer 的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：  根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示： 1234567891011121314151617181920#transformer位置编码class PositionalEncoder(nn.Module):    def __init__(self, d_model,max_seq_len = 80):        super().__init__()        self.d_model = d_model     # 根据pos和i创建一个常量PE矩阵        pe = torch.zeros(max_seq_len, d_model)        for pos in range(max_seq_len...</div></div></div></a><a class="pagination-related" href="/2025/12/11/RAG-simple-use/" title="RAG-simple-use"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-11</div><div class="info-item-2">RAG-simple-use</div></div><div class="info-2"><div class="info-item-1">外挂知识库RAG为什么引入：  LLM 的规模越大，大模型训练的成本越高，周期也就越长。那么具有时效性的数据也就无法参与训练，所以也就无法直接回答时效性相关的问题 通用的 LLM 没有企业内部数据和用户数据，那么企业想要在保证安全的前提下使用LLM，最好的方式就是把数据全部放在本地，企业数据的业务计算全部在本地完成。  文本切分方式固定规则切分 固定长度切分：按照固定token长度切分（每n个token切一块） 固定长度 + 重叠： 减少语义断裂 按字符分割： 按照标点符号、换行、空格分割（CharacterTextSplitter） 递归切分：按照段落、行、空格、字符的顺序进行切分（先按照段落,若&gt;chunk则按照行，以此类推）  结构化切分 标题切分 段落切分  语义切分 局部语义切分：句子级别embedding + 相邻句子similarity，相似度低则表示语义跳跃，即为切分点 句子切分：将文本切成句子，然后按chunk_size拼成一组句子 主题切分：做embedding聚类&#x2F;对每个句子进行相似度匹配，把相似句子归为1类  窗口切分 滑动窗口 动态窗口（...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Phoenix</div><div class="author-info-description">Every day is a chance to learn something new</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/HuanchaoFeng" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/2301_78214059?type=blog" target="_blank" title=""><i class="fa fa-book-open"></i></a><a class="social-icon" href="/2410673017@mails.szu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models"><span class="toc-number">1.</span> <span class="toc-text"> CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-number">1.1.</span> <span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#iintroduction"><span class="toc-number">1.2.</span> <span class="toc-text"> Ⅰ.Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#iimaterial-and-method"><span class="toc-number">1.3.</span> <span class="toc-text"> II.Material and Method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">1.4.</span> <span class="toc-text"> References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/11/RAG-simple-use/" title="RAG-simple-use">RAG-simple-use</a><time datetime="2025-12-11T07:37:07.000Z" title="Created 2025-12-11 15:37:07">2025-12-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/26/Datalinkx%E6%A8%A1%E5%9D%97%E8%AE%B2%E8%A7%A32/" title="Datalinkx模块讲解2">Datalinkx模块讲解2</a><time datetime="2025-11-26T04:14:04.000Z" title="Created 2025-11-26 12:14:04">2025-11-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/25/Datalinkx%E6%A8%A1%E5%9D%97%E8%AE%B2%E8%A7%A31/" title="Datalinkx模块讲解1">Datalinkx模块讲解1</a><time datetime="2025-11-25T13:40:43.000Z" title="Created 2025-11-25 21:40:43">2025-11-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/11/BIBM-paper-CFPLM/" title="BIBM paper: CFPLM">BIBM paper: CFPLM</a><time datetime="2025-11-11T11:21:54.000Z" title="Created 2025-11-11 19:21:54">2025-11-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/" title="LLaMA&amp;混合专家模型">LLaMA&amp;混合专家模型</a><time datetime="2025-05-19T10:50:11.000Z" title="Created 2025-05-19 18:50:11">2025-05-19</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By Phoenix</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.0-b2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>