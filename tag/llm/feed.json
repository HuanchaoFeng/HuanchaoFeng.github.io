{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"llm\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/12/LLM-concept/",
            "url": "http://example.com/2025/05/12/LLM-concept/",
            "title": "LLM-concept",
            "date_published": "2025-05-12T07:54:00.000Z",
            "content_html": "<h1 id=\"大模型基本概念\"><a class=\"markdownIt-Anchor\" href=\"#大模型基本概念\">#</a> 大模型基本概念</h1>\n<h2 id=\"目标\"><a class=\"markdownIt-Anchor\" href=\"#目标\">#</a> 目标</h2>\n<ul>\n<li>\n<p>语言模型就是对自然语言的概率分布进行建模，即 P (w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p>\n</li>\n<li>\n<p>计算下一个词的概率 P (wn | w1 w2 w3… wn-1)</p>\n<p><img src=\"image1.png\" alt=\"image\"></p>\n</li>\n</ul>\n<h2 id=\"发展历程\"><a class=\"markdownIt-Anchor\" href=\"#发展历程\">#</a> 发展历程</h2>\n<p>从 n-gram:</p>\n<p><img src=\"image2.png\" alt=\"image\"></p>\n<p>到 neural language model: 每个词都映射成一个低维向量</p>\n<p><img src=\"image3.png\" alt=\"image\"></p>\n<p>再到后面的 transformer 出现，transformer 的出现，NLP 进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是 PLM。</p>\n<p>随着 OpenAI 发布的 1750 亿个参数（GPT-3），开启 LLM 时代</p>\n<h2 id=\"问题发现\"><a class=\"markdownIt-Anchor\" href=\"#问题发现\">#</a> 问题发现</h2>\n<p>・大模型（如 GPT-3）参数量极大（1750 亿 +），传统 “预训练 + 微调” 范式成本过高（需为每个任务调整海量参数）。</p>\n<ol>\n<li>\n<p>解决方案：<br>\n・开发新范式（ICL/Prompt），通过输入指令或示例直接引导模型，避免微调。</p>\n<p>・但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p>\n</li>\n<li>\n<p>模型构建的关键：<br>\n・预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握 ICL/Prompt 所需的能力（如任务模式识别、指令遵循）。</p>\n<p>・后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p>\n</li>\n<li>\n<p>结论：<br>\n・新范式（ICL/Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p>\n</li>\n</ol>\n<h2 id=\"llm的构建流程\"><a class=\"markdownIt-Anchor\" href=\"#llm的构建流程\">#</a> LLM 的构建流程</h2>\n<ul>\n<li>预训练： 利用海量训练数据构建多样化内容，构建基础模型 ——&gt; 对长文本建模，使模型具有语言生成能力</li>\n<li>有监督微调 SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li>\n<li>奖励建模 RM：训练一个能够判断文本质量的裁判，对同个提示词，比较 SFT 生成的多个输出的质量</li>\n<li>强化学习 RLHF (human feedback)：基于 RM，优化 SFT 模型</li>\n</ul>\n<p>SFT 相当于学生学会答题，RM 是评分老师，判断 answer 好坏，RLHF 是学生根据老师评分改进答题策略</p>\n<h2 id=\"补充\"><a class=\"markdownIt-Anchor\" href=\"#补充\">#</a> 补充</h2>\n<p><strong>N-gram 模型详解</strong><br>\n N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1 个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p>\n<p>N-gram 的定义：</p>\n<p>・指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p>\n<p>・例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure>\n<p>・核心假设：</p>\n<p>・马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>如何计算概率？</strong><br>\nN-gram 通过统计语料库中词序列的频率来估计概率：</p>\n<p>计算  <code>P(sat | the cat)</code> ：</p>\n<p>P(sat∣the cat)=Count(“the cat”)Count(“the cat sat”)</p>\n<p>若语料中 “the cat” 出现 100 次，“the cat sat” 出现 30 次，则  <code>P(sat | the cat) = 0.3</code> 。</p>\n<p><strong>N-gram 的优缺点</strong></p>\n<table>\n<thead>\n<tr>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>简单高效，计算速度快。</td>\n<td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td>\n</tr>\n<tr>\n<td>小规模数据即可训练。</td>\n<td>数据稀疏性（罕见 n-gram 概率不准确）。</td>\n</tr>\n<tr>\n<td>曾广泛用于机器翻译、拼写检查等任务。</td>\n<td>无法理解语义（仅统计共现频率）。</td>\n</tr>\n</tbody>\n</table>\n",
            "tags": [
                "LLM",
                "concept"
            ]
        }
    ]
}