{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"llm\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/13/Transformer/",
            "url": "http://example.com/2025/05/13/Transformer/",
            "title": "Transformer",
            "date_published": "2025-05-13T07:07:29.000Z",
            "content_html": "<h2 id=\"Transformer四层结构\"><a href=\"#Transformer四层结构\" class=\"headerlink\" title=\"Transformer四层结构\"></a>Transformer四层结构</h2><p>Transformer结构：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ol>\n<li><p>嵌入表示层</p>\n<p>Transformer的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：</p>\n<p><img src=\"/image2.png\" alt=\"image1\"></p>\n<p>根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#transformer位置编码</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model,max_seq_len = <span class=\"number\">80</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 根据pos和i创建一个常量PE矩阵</span></span><br><span class=\"line\">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> pos <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>):</span><br><span class=\"line\">                pe[pos, i] = math.sin(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">                pe[pos, i + <span class=\"number\">1</span>] = math.cos(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">        pe = pe.unsqueeze(<span class=\"number\">0</span>) <span class=\"comment\">#形状 (1, seq_len, d_model)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">&#x27;pe&#x27;</span>, pe)   <span class=\"comment\">#将 pe 保存为模型的一部分（不参与梯度更新，但会随模型保存/加载）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"comment\">#x:(batch_size, seq_len, d_model)</span></span><br><span class=\"line\">        seq_len = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = x + <span class=\"variable language_\">self</span>.pe[:,:seq_len].cuda()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>使用正余弦的原因是，函数的范围是[-1，1]与词向量相加不会太偏离原始语义，同时第pos+k个位置的编码是第pos个位置编码的线性组合（根据三角函数和角公式决定），这就蕴含了单词之间的距离信息：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n</li>\n<li><p>自注意力层</p>\n<p>自注意力机制，即自己作为QKV进行计算，但是解码器有两个注意力模块，一个是掩码多头，一个是交叉多头注意力，但是原理其实和下面代码差不多，直接用代码展示比较能说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#transformer多头自注意力机制</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, heads, d_model,dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.q_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.k_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.v_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out = nn.Linear(d_model,d_model)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">q, k, v, d_k, mask = <span class=\"literal\">None</span>, dropout = <span class=\"literal\">None</span> </span>):</span><br><span class=\"line\">        <span class=\"comment\"># 转置k相乘 ​​除以 math.sqrt(d_k)​​ 的操作是缩放点积注意力，防止点积数值过大​</span></span><br><span class=\"line\">        scores = torch.matmul(q,k.transpose(-<span class=\"number\">2</span>,-<span class=\"number\">1</span>)) / math.sqrt(d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">            scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>) <span class=\"comment\">#掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        scores = F.sofmax(scores,dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            scores = dropout(scores)</span><br><span class=\"line\"></span><br><span class=\"line\">        output = torch.matmul(scores,v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        batch_size = q.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 利用线性计算划分成h个头</span></span><br><span class=\"line\">        q = <span class=\"variable language_\">self</span>.q_linear(q).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        k = <span class=\"variable language_\">self</span>.k_linear(k).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        v = <span class=\"variable language_\">self</span>.v_linear(v).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#转置头和seq_len位置</span></span><br><span class=\"line\">        k = k.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        q = q.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        v = v.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        scores = <span class=\"variable language_\">self</span>.attention(q, k, v, <span class=\"variable language_\">self</span>.d_k, mask, <span class=\"variable language_\">self</span>.dropout)</span><br><span class=\"line\">        <span class=\"comment\"># 拼接多头输出并线性变换</span></span><br><span class=\"line\">        concat = scores.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(batch_size, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        output = <span class=\"variable language_\">self</span>.out(concat) </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>前馈层</p>\n<p>接收注意力层的输出，通过带有ReLU的2层全连接网络，第一层会映射到高纬度，因为隐藏层维度的增大有利于提高质量（实验证明）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#前馈层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FeedForward</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff = <span class=\"number\">2038</span>, dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear2 = nn.Linear(d_ff,d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.linear1(x)))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.linear2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>残差连接和归一化</p>\n<p>​    由Transformer结构组成的网络结构通常都非常庞大。编码器和解码器均由很多层基本的Transformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究人员在 Transformer 块中进一步引入了残差连接与层归一化技术，以进一步提升训练的稳定性。具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。</p>\n</li>\n</ol>\n<h2 id=\"解码器与编码器\"><a href=\"#解码器与编码器\" class=\"headerlink\" title=\"解码器与编码器\"></a>解码器与编码器</h2><p>​\t编码器端较容易实现。相比于编码器端，解码器端更复杂。具体来说，解码器的每个 Transformer 块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力部分。这主要是因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息。解码器端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的，因此这一额外增加的掩码是用来掩盖后续的文本信息的，以防模型在训练阶段直接看到后续的文本序列，进而无法得到有效的训练。此外，解码器端额外增加了一个多头交叉注意力模块，使用交叉注意力方法，同时接收来自编码器端的输出和当前 Transformer 块的前一个掩码注意力层的输出。查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。</p>\n<p>​\t解码器端以自回归的方式生成目标语言文本，即在每个时间步 <em>t</em>，根据编码器端输出的源语言文本表示，以及前t <em>−</em> 1 个时刻生成的目标语言文本，生成当前时刻的目标语言单词（以我的理解来说，训练阶段是没有显示时间步概念的，通过<strong>一次性输入完整序列 + 掩码矩阵</strong>，在单次前向传播中并行计算出所有位置的输出，同时利用掩码强制模型行为与自回归一致，而推理时必须显式按时间步生成，因为未来词未知（无法并行））。</p>\n<h2 id=\"以推理生成中文翻译-我爱你-为例：\"><a href=\"#以推理生成中文翻译-我爱你-为例：\" class=\"headerlink\" title=\"以推理生成中文翻译 &quot;我爱你&quot; 为例：\"></a>以推理生成中文翻译 <code>&quot;我爱你&quot;</code> 为例：</h2><table>\n<thead>\n<tr>\n<th align=\"center\">时间步</th>\n<th align=\"center\">图1中对应的模块流程</th>\n<th align=\"center\">具体操作</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><code>t=1</code></td>\n<td align=\"center\">词元嵌入 → 位置编码 → 掩码多头注意力 → 编码器-解码器注意力 → 前馈网络 → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt;</code>，输出 <code>&quot;我&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=2</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我</code>) → 位置编码 → 掩码多头注意力 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我</code>，输出 <code>&quot;爱&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=3</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱</code>) → 位置编码 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱</code>，输出 <code>&quot;你&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=4</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱你</code>) → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱你</code>，输出 <code>&lt;end&gt;</code> 的概率分布，终止生成。</td>\n</tr>\n</tbody></table>\n",
            "tags": [
                "LLM",
                "transformer"
            ]
        },
        {
            "id": "http://example.com/2025/05/12/LLM-concept/",
            "url": "http://example.com/2025/05/12/LLM-concept/",
            "title": "LLM-concept",
            "date_published": "2025-05-12T07:54:00.000Z",
            "content_html": "<h1 id=\"大模型基本概念\"><a class=\"markdownIt-Anchor\" href=\"#大模型基本概念\">#</a> 大模型基本概念</h1>\n<h2 id=\"目标\"><a class=\"markdownIt-Anchor\" href=\"#目标\">#</a> 目标</h2>\n<ul>\n<li>\n<p>语言模型就是对自然语言的概率分布进行建模，即 P (w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p>\n</li>\n<li>\n<p>计算下一个词的概率 P (wn | w1 w2 w3… wn-1)</p>\n<p><img src=\"image1.png\" alt=\"image\"></p>\n</li>\n</ul>\n<h2 id=\"发展历程\"><a class=\"markdownIt-Anchor\" href=\"#发展历程\">#</a> 发展历程</h2>\n<p>从 n-gram:</p>\n<p><img src=\"image2.png\" alt=\"image\"></p>\n<p>到 neural language model: 每个词都映射成一个低维向量</p>\n<p><img src=\"image3.png\" alt=\"image\"></p>\n<p>再到后面的 transformer 出现，transformer 的出现，NLP 进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是 PLM。</p>\n<p>随着 OpenAI 发布的 1750 亿个参数（GPT-3），开启 LLM 时代</p>\n<h2 id=\"问题发现\"><a class=\"markdownIt-Anchor\" href=\"#问题发现\">#</a> 问题发现</h2>\n<p>・大模型（如 GPT-3）参数量极大（1750 亿 +），传统 “预训练 + 微调” 范式成本过高（需为每个任务调整海量参数）。</p>\n<ol>\n<li>\n<p>解决方案：<br>\n・开发新范式（ICL/Prompt），通过输入指令或示例直接引导模型，避免微调。</p>\n<p>・但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p>\n</li>\n<li>\n<p>模型构建的关键：<br>\n・预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握 ICL/Prompt 所需的能力（如任务模式识别、指令遵循）。</p>\n<p>・后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p>\n</li>\n<li>\n<p>结论：<br>\n・新范式（ICL/Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p>\n</li>\n</ol>\n<h2 id=\"llm的构建流程\"><a class=\"markdownIt-Anchor\" href=\"#llm的构建流程\">#</a> LLM 的构建流程</h2>\n<ul>\n<li>预训练： 利用海量训练数据构建多样化内容，构建基础模型 ——&gt; 对长文本建模，使模型具有语言生成能力</li>\n<li>有监督微调 SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li>\n<li>奖励建模 RM：训练一个能够判断文本质量的裁判，对同个提示词，比较 SFT 生成的多个输出的质量</li>\n<li>强化学习 RLHF (human feedback)：基于 RM，优化 SFT 模型</li>\n</ul>\n<p>SFT 相当于学生学会答题，RM 是评分老师，判断 answer 好坏，RLHF 是学生根据老师评分改进答题策略</p>\n<h2 id=\"补充\"><a class=\"markdownIt-Anchor\" href=\"#补充\">#</a> 补充</h2>\n<p><strong>N-gram 模型详解</strong><br>\n N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1 个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p>\n<p>N-gram 的定义：</p>\n<p>・指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p>\n<p>・例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure>\n<p>・核心假设：</p>\n<p>・马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>如何计算概率？</strong><br>\nN-gram 通过统计语料库中词序列的频率来估计概率：</p>\n<p>计算  <code>P(sat | the cat)</code> ：</p>\n<p>P(sat∣the cat)=Count(“the cat”)Count(“the cat sat”)</p>\n<p>若语料中 “the cat” 出现 100 次，“the cat sat” 出现 30 次，则  <code>P(sat | the cat) = 0.3</code> 。</p>\n<p><strong>N-gram 的优缺点</strong></p>\n<table>\n<thead>\n<tr>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>简单高效，计算速度快。</td>\n<td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td>\n</tr>\n<tr>\n<td>小规模数据即可训练。</td>\n<td>数据稀疏性（罕见 n-gram 概率不准确）。</td>\n</tr>\n<tr>\n<td>曾广泛用于机器翻译、拼写检查等任务。</td>\n<td>无法理解语义（仅统计共现频率）。</td>\n</tr>\n</tbody>\n</table>\n",
            "tags": [
                "LLM",
                "concept"
            ]
        }
    ]
}