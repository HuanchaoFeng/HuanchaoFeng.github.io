{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"llm\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/11/11/BIBM-paper-CFPLM/",
            "url": "http://example.com/2025/11/11/BIBM-paper-CFPLM/",
            "title": "BIBM paper: CFPLM",
            "date_published": "2025-11-11T11:21:54.000Z",
            "content_html": "<h1 id=\"cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models\"><a class=\"markdownIt-Anchor\" href=\"#cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models\">#</a> CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models</h1>\n<h2 id=\"abstract\"><a class=\"markdownIt-Anchor\" href=\"#abstract\">#</a> Abstract</h2>\n<p>Protein-RNA interactions (PRIs) play pivotal roles in biological processes such as gene regulation, making their prediction essential for therapeutic and mechanistic studies. While traditional wet-lab methods are time-consuming and challenging, computational approaches offer efficient alternatives. Graph-based methods show promise by capturing both direct interactive domains (protein-RNA interaction) and indirect collaborative domains (functional similarity among proteins/RNAs). However, integrating these domains and learning meaningful node representations remain critical challenges. To address this, we propose ​​CFPLM​​, a collaborative framework fusing large language models, graph convolutional networks, and cross-attention mechanisms to improve PRI prediction. Experiment results demonstrate that CFPLM achieves robust, state-of-the-art performance across three benchmark datasets. It’s anticipated to have applicability to similar other interaction prediction tasks.</p>\n<h2 id=\"iintroduction\"><a class=\"markdownIt-Anchor\" href=\"#iintroduction\">#</a> Ⅰ.Introduction</h2>\n<p>As two essential biological macromolecules, the interaction between proteins and RNA plays a crucial role in cellular activities, such as gene expression regulation, translation control, and viral replication [1].Traditional biological experiments can identify direct interactions between RNA and protein with confidence. However, they are complicated and challenging, making them expensive to verify each potential interaction individually [2].<br>\nIn response to these challenges, researchers are encouraged to develop more efficient and economical computational methods to aid in screening and predicting potential interactions, thereby enhancing research efficiency and productivity. Among the various calculation methods currently available, the machine learning-based method is the most widely used in this field. These methods can be divided into two groups: one involves feature learning and classification, and the other is graph-based association prediction. The former extracts features based on protein and RNA sequences or structures, and constructs a classification model based on these features to determine whether a given protein can interact with an RNA. For example, EnANNDeep combines an adaptive K-nearest neighbor classifier, a deep neural network, and a deep forest model, and integrates the prediction results of the three models using a weighted average to improve classification performance [3]. LPIDF utilizes self-encoder technology to extract sequence features of lncRNA and protein, and combines an ensemble learning method to predict possible interactions [4]. In MHAM, a deep learning model based on a multi-head attention mechanism and a residual link is used to predict the interaction between RNA and protein [5]. LGFC-CNN combines the original sequence, artificial, and structural features to predict PRI through deep learning [6]. LPI-CNNCP uses convolutional neural networks and replication filling techniques to predict the interaction between lncRNA and protein [7]. AptaNet [8] and AptaTrans [9] employ deep neural networks and transformer-based encoders, respectively, to capture the complex interactions between proteins and nucleic acid aptamers. The advantage of this type of method is that it can predict the potential interaction between any given protein and RNA. However, the interaction patterns between proteins and RNA are complex, and currently known data on protein-RNA interactions is limited, making it difficult to train a reliable end-to-end prediction model. Therefore, existing methods are difficult to generalize excellent predictive performance to different datasets.<br>\nGraph-based methods have emerged as particularly promising approaches. Because it can better utilize existing knowledge through molecular similarity networks and interaction networks. For example, LPICGAE is a combination method based on a graph self-encoder, which is used to predict PRI [10]. BiHo-GNN is the first method to integrate isomorphic and heterogeneous networks through bipartite graph embedding to predict PRI [11]. NPI-GNN was proposed for predicting noncoding RNA–protein interactions using graph neural networks[12]. CCGNN proposed a new method to predict PRI by incorporating information from the interaction domain and cooperation domain [13]. From the above research, it is evident that collaborative domain construction and node representation learning are key factors in determining this method. However, when constructing collaborative domains and node representation, most existing graph-based methods fail to fully capture the complex information in RNA and protein sequences, which results in limited performance of the finally trained models.<br>\nIn recent years, the breakthrough progress of large language models (LLMs) has transformed various aspects of life, including scientific research paradigms. Currently, numerous large language models have been trained on RNA and protein sequences, such as EVO [14], BIRNA-BERT [15], RNA-FM [16], ESM [17], and ProteinBERT [18], which have been widely utilized in biological tasks, including interaction prediction, and have achieved notable success. For example, PAIR used an RNA-FM language model to get the embedding vector of nucleic acid aptamer, and performed well in protein-nucleic acid aptamer interaction task [19]; In DTI-LM [20], ChemBERTa [21] and ESM language models are used to generate embedding vectors for drug molecules and protein, and show good performance in their prediction tasks . These studies demonstrate that LLMs have the potential to offer a more powerful approach to constructing collaborative domains and characterizing the nodes. However, there is currently a lack of research to explore this point. Moreover, most graph-based methods operate on information from the collaborative domain and interaction information within RNA or protein separately, thereby ignoring some useful cross-information and dependence between RNA and protein. How to better combine LLMs with other technologies to address the challenges in PRI prediction remains to be explored.<br>\nTo address these gaps, we propose a novel framework, named CFPLM, for identifying protein-RNA interactions that utilizes a cross-attention fusion neural network in conjunction with large language models. The contributions of this study are as follows:</p>\n<ul>\n<li>(1) We introduce a new strategy for constructing collaborative domains based on both protein and RNA language models, which provides a more reliable method for knowledge construction in graph-based approaches.</li>\n<li>(2) We design a new learning model for protein-RNA interactions by combining a graph convolutional neural network (GCN) with a cross-attention mechanism, thereby overcoming the limitations of existing graph learning models in this field.<br>\nThe experimental results demonstrate the proposed framework is more efficient and accurate for predicting protein-RNA interactions, offering valuable insights into the interaction mechanisms between RNA and protein, as well as related biological processes.</li>\n</ul>\n<h2 id=\"iimaterial-and-method\"><a class=\"markdownIt-Anchor\" href=\"#iimaterial-and-method\">#</a> II.Material and Method</h2>\n<ul>\n<li>A.Datasets<br>\nThe benchmark dataset used in this study was constructed based on the protein-RNA complex released from the PDB (Protein Data Bank) database between 1989 and 2023. We labeled a protein chain interacts with an RNA according to the spatial distance between in the complex. According to the previous research [22], the distance threshold is set to 3.5 angstroms (protein-RNA pairs with spatial distance less than 3.5 angstroms are interacted). To reduce data redundancy and ensure objective experimental results, we used BLAST to cluster proteins and RNAs, respectively, based on sequence similarity (with threshold S=90). We then randomly select a sequence from each cluster to build a benchmark database. To avoid noise caused by incomplete protein fragments, we filter out protein sequences with lengths less than 50. Finally, 2,465 RNA sequences, 5,897 protein sequences, and 8,579 pairs of interaction information were obtained. According to previous work [23], we adopt the following protocol for processing the dataset: randomly sampling the same number of protein-RNA pairs that are not interactions as negative samples.<br>\nFor a comprehensive evaluation, we also employed another two datasets: miRTarBase (version 8.0) [24] and a protein-lncRNA dataset [10]. There are 3,924 miRNAs, 20,992 target genes, and 186,416 verified interactions in miRTarBase. While 3,046 lncRNAs, 136 proteins, and 8,112 interactions in the protein-lncRNA dataset.</li>\n<li>B.Framework of CFPLM<br>\nCFPLM includes three modules (Fig. 1): 1) Feature extraction, which uses large language models to extract initial feature vectors of RNA and protein; 2) Graph structure, which uses cosine similarity to calculate the similarity of RNA-RNA and protein-protein, and then obtains the similarity graph, and at the same time transforms the RNA-protein interaction pair into an interaction graph; 3) Feature optimization, which uses graph convolution network to capture the complex features of RNA and protein, and uses cross attention to fuse the feature vectors of similarity domain and interaction domain. The first and second points are shown in Fig. 1 (a), and the third point is shown in Fig. 1 (b-c).</li>\n<li>(1)Feature Extraction\n<ul>\n<li>a)RNA Encoder: BIRNA-BERT is an RNA language model based on Transformer architecture, which is characterized by adopting a double labeling scheme, combining nucleotide-level labeling and byte-pair coding labeling, and can dynamically adjust the labeling strategy according to the sequence length and computing resources, so that the model can handle long-sequence tasks while retaining the ability to handle short-sequence tasks using nucleotide-level information. RNA-FM, the basic RNA model based on self-supervised learning, has been proven effective in multiple tasks for the first time. Another example is the latest Evo, which demonstrates the powerful ability of genome language models in function-oriented design. Regarding the choice of language model, we will compare the above language models in the results section. Ultimately, we chose to use the BIRNA-BERT model based on its ease of use, as it can be well adapted to the task and saves computing resources and experimental time.</li>\n<li>b)Protein Encoder: ESM-2 is a large language model for protein sequences. The model can learn the evolution pattern in protein sequences by training on the modeling target of masked language and transform it into a deep understanding of protein structure. ProteinBERT is also a language model specially designed for protein sequences. ProteinBERT enhances the efficiency and performance of protein sequence analysis by integrating a novel pre-training task that combines language modeling and gene ontology annotation prediction. The model has performed well in several protein task benchmarks, approaching or exceeding the performance of existing large-scale models [18]. We compared the above language models and finally adopt the ESM-2 model. In this study, we employ a protein language model based on the ESM-2 architecture with 150M parameters, and train it on the UniRef50 dataset [25].</li>\n</ul>\n</li>\n<li>(2)Graph Construction<br>\nIn DTI-LM, the feature expression is improved by calculating the similarity protein-protein and drug. The similarity function is used in CCGNN to construct the similarity map between protein and RNA. FMSRT constructs multi-source similarity map according to similarity [26] . The success of these studies shows that similarity information can provide rich information for the study of biomolecular interaction. In this study, we will use the cosine similarity calculation method to calculate the similarity of RNA feature vectors and protein feature vectors respectively, and convert them into RNA-RNA similarity graph and Protein-Protein similarity graph. At the same time, NPI-GNN model uses the bipartite graph of ncRNA-protein to infer the interaction [12], and GANLDA model uses the bipartite graph of lncRNA and diseases to predict the interaction [27]. These methods combine the topological information of PRI networks, and effectively improve the performance of the model in link prediction tasks. In this study, we construct an RNA-protein interaction bipartite graph for feature optimization in graph neural networks.</li>\n<li>(3)Feature Optimization\n<ul>\n<li>a)Graph convolution network: GCN is a spectral-based graph neural network that processes graph-structured data by aggregating neighborhood information to learn low-dimensional node representations. It efficiently preserves local structure while supporting tasks like node classification and link prediction.  The RNA feature matrix and protein feature matrix will enter the graph convolution network with three graphs, and finally get the updated node feature vector.</li>\n<li>b)Cross Attention: To better capture the interaction and similarity information between RNA and proteins, a cross-attention mechanism is introduced. Its core lies in dynamically learning the interactions between the two through the attention mechanism, so a cross-attention module is designed in the model to handle such information. This module contains 4 attention heads, each with a feature dimension of 64. Fig. 1 © illustrates the calculation process of the cross-attention module, where, for the RNA and protein feature vector matrices obtained from the interaction graph, RNA serves as the Query and protein acts as the Key and Value, and vice versa. Meanwhile, the RNA and protein matrices derived from the similarity graph are also optimized and updated using this method.</li>\n<li>c)Loss Function: In this study, we use binary cross entropy as the loss function, calculating the average of all samples when calculating the loss.</li>\n</ul>\n</li>\n</ul>\n<p>FrameWork:<br>\n<img src=\"image4.png\" alt=\"image1\"></p>\n<h2 id=\"references\"><a class=\"markdownIt-Anchor\" href=\"#references\">#</a> References</h2>\n<p>[1]E. Jankowsky, and M. E. J. N. r. M. c. b. Harris, “Specificity and nonspecificity in RNA–protein interactions,” Nature Reviews Molecular Cell Biology, vol. 16, no. 9, pp. 533-544, 2015.<br>\n[2]M. Philip, T. Chen, and S. Tyagi, “A Survey of Current Resources to Study lncRNA-Protein Interactions,” Noncoding RNA, vol. 7, no. 2, pp. 33, Jun 8, 2021.<br>\n[3]L. Peng, J. Tan, X. Tian, and L. Zhou, “EnANNDeep: An Ensemble-based lncRNA-protein Interaction Prediction Framework with Adaptive k-Nearest Neighbor Classifier and Deep Models,” Interdisciplinary Sciences: Computational Life Sciences, vol. 14, no. 1, pp. 209-232, Mar, 2022.<br>\n[4]X. Tian, L. Shen, Z. Wang, L. Zhou, and L. Peng, “A novel lncRNA-protein interaction prediction method based on deep forest with cascade forest structure,” Scientific Reports, vol. 11, no. 1, pp. 18881, Sep 23, 2021.<br>\n[5]Z. Zhou, Z. Du, J. Wei, L. Zhuo, S. Pan, X. Fu, and X. Lian, “MHAM-NPI: Predicting ncRNA-protein interactions based on multi-head attention mechanism,” Computers in Biology and Medicine, vol. 163, pp. 107143, Sep, 2023.<br>\n[6]L. Huang, S. Q. Jiao, S. Yang, S. Q. Zhang, X. P. Zhu, R. Guo, and Y. Wang, “LGFC-CNN: Prediction of lncRNA-Protein Interactions by Using Multiple Types of Features through Deep Learning,” Genes, vol. 12, no. 11, pp. 1689, Nov, 2021.<br>\n[7]S. W. Zhang, X. X. Zhang, X. N. Fan, and W. N. Li, “LPI-CNNCP: Prediction of lncRNA-protein interactions by using convolutional neural network with the copy-padding trick,” Analytical Biochemistry, vol. 601, pp. 113767, Jul 15, 2020.<br>\n[8]N. Emami, and R. Ferdousi, “AptaNet as a deep learning approach for aptamer–protein interaction prediction,” Scientific reports, vol. 11, no. 1, pp. 6074, 2021.<br>\n[9]I. Shin, K. Kang, J. Kim, S. Sel, J. Choi, J.-W. Lee, H. Y. Kang, and G. Song, “AptaTrans: a deep neural network for predicting aptamer-protein interaction using pretrained encoders,” BMC Bioinformatics, vol. 24, no. 1, pp. 447, 2023/11/27, 2023.<br>\n[10]J. Zhao, J. Sun, S. C. Shuai, Q. Zhao, and J. Shuai, “Predicting potential interactions between lncRNAs and proteins via combined graph auto-encoder methods,” Briefings in Bioinformatics, vol. 24, no. 1, Jan 19, 2023.<br>\n[11]Y. Ma, H. Zhang, C. Jin, and C. Kang, “Predicting lncRNA-protein interactions with bipartite graph embedding and deep graph neural networks,” Frontiers in Genetics, vol. 14, pp. 1136672, 2023.<br>\n[12]Z.-A. Shen, T. Luo, Y.-K. Zhou, H. Yu, and P.-F. J. B. i. b. Du, “NPI-GNN: Predicting ncRNA–protein interactions with deep graph neural networks,” Briefings in Bioinformatics, vol. 22, no. 5, pp. bbab051, 2021.<br>\n[13]H. Li, B. Wu, M. Sun, Z. Zhu, K. Chen, and H. Ge, “Cross-domain contrastive graph neural network for lncRNA–protein interaction prediction,” Knowledge-Based Systems, vol. 296, 2024.<br>\n[14]E. Nguyen, M. Poli, M. G. Durrant, B. Kang, D. Katrekar, D. B. Li, L. J. Bartie, A. W. Thomas, S. H. King, G. Brixi, J. Sullivan, M. Y. Ng, A. Lewis, A. Lou, S. Ermon, S. A. Baccus, T. Hernandez-Boussard, C. Ré, P. D. Hsu, and B. L. Hie, “Sequence modeling and design from molecular to genome scale with Evo,” Science, vol. 386, no. 6723, pp. eado9336, 2024.<br>\n[15]M. T. Tahmid, H. S. Shahgir, S. Mahbub, Y. Dong, and M. S. J. b. Bayzid, “BiRNA-BERT allows efficient RNA language modeling with adaptive tokenization,” bioRxiv:2024.07.02.601703, 2024.<br>\n[16]J. Chen, Z. Hu, S. Sun, Q. Tan, Y. Wang, Q. Yu, L. Zong, L. Hong, J. Xiao, and T. J. a. p. a. Shen, “Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions,” arXiv preprint  arXiv:2204.00300, 2022.<br>\n[17]Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. Dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, “Evolutionary-scale prediction of atomic-level protein structure with a language model,” Science, vol. 379, no. 6637, pp. 1123-1130, Mar 17, 2023.<br>\n[18]N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial, “ProteinBERT: a universal deep-learning model of protein sequence and function,” Bioinformatics, vol. 38, no. 8, pp. 2102-2110, Apr 12, 2022.<br>\n[19]J. Zhang, Z. Yan, H. Zeng, and Z. Zhu, “PAIR: protein-aptamer interaction prediction based on language models and contrastive learning framework.” 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), Lisbon, Portugal, 2024, pp. 5426-5432<br>\n[20]K. T. Ahmed, M. I. Ansari, and W. J. B. Zhang, “DTI-LM: language model powered drug–target interaction prediction,” Bioinformatics, vol. 40, no. 9, pp. btae533, 2024.<br>\n[21]S. Chithrananda, G. Grand, and B. J. a. p. a. Ramsundar, “ChemBERTa: large-scale self-supervised pretraining for molecular property prediction,” arXiv preprint, arXiv:2010.09885, 2020.<br>\n[22]J. Yan, and L. Kurgan, “DRNApred, fast sequence-based method that accurately predicts and discriminates DNA- and RNA-binding residues,” Nucleic Acids Research, vol. 45, no. 10, pp. e84-e84, Jun 2, 2017.<br>\n[23]Y. Li, H. Sun, S. Feng, Q. Zhang, S. Han, and W. Du, “Capsule-LPI: a LncRNA-protein interaction predicting tool based on a capsule network,” BMC Bioinformatics, vol. 22, no. 1, pp. 246, May 13, 2021.<br>\n[24]H.-Y. Huang, Y.-C.-D. Lin, J. Li, K.-Y. Huang, S. Shrestha, H.-C. Hong, Y. Tang, Y.-G. Chen, C.-N. Jin, Y. Yu, J.-T. Xu, Y.-M. Li, X.-X. Cai, Z.-Y. Zhou, X.-H. Chen, Y.-Y. Pei, L. Hu, J.-J. Su, S.-D. Cui, F. Wang, Y.-Y. Xie, S.-Y. Ding, M.-F. Luo, C.-H. Chou, N.-W. Chang, K.-W. Chen, Y.-H. Cheng, X.-H. Wan, W.-L. Hsu, T.-Y. Lee, F.-X. Wei, and H.-D. Huang, “miRTarBase 2020: updates to the experimentally validated microRNA–target interaction database,” Nucleic Acids Research, vol. 48, no. D1, pp. D148-D154, 2019.<br>\n[25]B. E. Suzek, H. Huang, P. McGarvey, R. Mazumder, and C. H. Wu, “UniRef: comprehensive and non-redundant UniProt reference clusters,” Bioinformatics, vol. 23, no. 10, pp. 1282-8, May 15, 2007.<br>\n[26]X. Zhang, M. Liu, Z. Li, L. Zhuo, X. Fu, and Q. Zou, “Fusion of multi-source relationships and topology to infer lncRNA-protein interactions,” Molecular Therapy Nucleic Acids, vol. 35, no. 2, pp. 102187, Jun 11, 2024.<br>\n[27]W. Lan, X. M. Wu, Q. F. Chen, W. Peng, J. X. Wang, and Y. P. Chen, “GANLDA: Graph attention network for lncRNA-disease associations prediction,” Neurocomputing, vol. 469, pp. 384-393, Jan 16, 2022.<br>\n[28]Q. Le, and T. Mikolov, “Distributed representations of sentences and documents,” 2014 International Conference on Machine Learning, Beijing,China, 2014,pp. 1188-1196.<br>\n[29]S. Yang, Y. Wang, Y. Lin, D. Shao, K. He, and L. J. M. Huang, “LncMirNet: predicting LncRNA–miRNA interaction based on deep learning of ribonucleic acid sequences,” Molecules, vol. 25, no. 19, pp. 4372, 2020.<br>\n[30]Z. Wang, S. Liang, S. Liu, Z. Meng, J. Wang, and S. Liang, “Sequence pre-training-based graph neural network for predicting lncRNA-miRNA associations,” Briefings in Bioinformatics, vol. 24, no. 5, pp. bbad317, Sep 20, 2023.<br>\n[31]Y. Han, and S. W. Zhang, “ncRPI-LGAT: Prediction of ncRNA-protein interactions with line graph attention network framework,” Computational and Structural Biotechnology Journal, vol. 21, pp. 2286-2295, 2023.<br>\n[32]H. Ji, X. Wang, C. Shi, B. Wang, P. S. J. I. T. o. K. Yu, and D. Engineering, “Heterogeneous graph propagation network,” IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 521-532, 2021.<br>\n[33]L. v. d. Maaten, and G. J. J. o. m. l. r. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579-2605, 2008.</p>\n",
            "tags": [
                "LLM"
            ]
        },
        {
            "id": "http://example.com/2025/05/13/Transformer/",
            "url": "http://example.com/2025/05/13/Transformer/",
            "title": "Transformer",
            "date_published": "2025-05-13T07:07:29.000Z",
            "content_html": "<h2 id=\"transformer四层结构\"><a class=\"markdownIt-Anchor\" href=\"#transformer四层结构\">#</a> Transformer 四层结构</h2>\n<p>Transformer 结构：</p>\n<p><img src=\"image1.png\" alt=\"image1\"></p>\n<ol>\n<li>\n<p>嵌入表示层</p>\n<p>Transformer 的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：</p>\n<p><img src=\"image2.png\" alt=\"image1\"></p>\n<p>根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#transformer位置编码</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model,max_seq_len = <span class=\"number\">80</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 根据pos和i创建一个常量PE矩阵</span></span><br><span class=\"line\">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> pos <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>):</span><br><span class=\"line\">                pe[pos, i] = math.sin(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">                pe[pos, i + <span class=\"number\">1</span>] = math.cos(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">        pe = pe.unsqueeze(<span class=\"number\">0</span>) <span class=\"comment\">#形状 (1, seq_len, d_model)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">&#x27;pe&#x27;</span>, pe)   <span class=\"comment\">#将 pe 保存为模型的一部分（不参与梯度更新，但会随模型保存/加载）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"comment\">#x:(batch_size, seq_len, d_model)</span></span><br><span class=\"line\">        seq_len = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = x + <span class=\"variable language_\">self</span>.pe[:,:seq_len].cuda()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>使用正余弦的原因是，函数的范围是 [-1，1] 与词向量相加不会太偏离原始语义，同时第 pos+k 个位置的编码是第 pos 个位置编码的线性组合（根据三角函数和角公式决定），这就蕴含了单词之间的距离信息：</p>\n<p><img src=\"image3.png\" alt=\"image2\"></p>\n</li>\n<li>\n<p>自注意力层</p>\n<p>自注意力机制，即自己作为 QKV 进行计算，但是解码器有两个注意力模块，一个是掩码多头，一个是交叉多头注意力，但是原理其实和下面代码差不多，直接用代码展示比较能说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#transformer多头自注意力机制</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, heads, d_model,dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.q_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.k_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.v_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out = nn.Linear(d_model,d_model)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">q, k, v, d_k, mask = <span class=\"literal\">None</span>, dropout = <span class=\"literal\">None</span> </span>):</span><br><span class=\"line\">        <span class=\"comment\"># 转置k相乘 ​​除以 math.sqrt(d_k)​​ 的操作是缩放点积注意力，防止点积数值过大​</span></span><br><span class=\"line\">        scores = torch.matmul(q,k.transpose(-<span class=\"number\">2</span>,-<span class=\"number\">1</span>)) / math.sqrt(d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">            scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>) <span class=\"comment\">#掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        scores = F.sofmax(scores,dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            scores = dropout(scores)</span><br><span class=\"line\"></span><br><span class=\"line\">        output = torch.matmul(scores,v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        batch_size = q.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 利用线性计算划分成h个头</span></span><br><span class=\"line\">        q = <span class=\"variable language_\">self</span>.q_linear(q).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        k = <span class=\"variable language_\">self</span>.k_linear(k).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        v = <span class=\"variable language_\">self</span>.v_linear(v).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#转置头和seq_len位置</span></span><br><span class=\"line\">        k = k.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        q = q.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        v = v.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        scores = <span class=\"variable language_\">self</span>.attention(q, k, v, <span class=\"variable language_\">self</span>.d_k, mask, <span class=\"variable language_\">self</span>.dropout)</span><br><span class=\"line\">        <span class=\"comment\"># 拼接多头输出并线性变换</span></span><br><span class=\"line\">        concat = scores.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(batch_size, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        output = <span class=\"variable language_\">self</span>.out(concat) </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>前馈层</p>\n<p>接收注意力层的输出，通过带有 ReLU 的 2 层全连接网络，第一层会映射到高纬度，因为隐藏层维度的增大有利于提高质量（实验证明）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#前馈层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FeedForward</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff = <span class=\"number\">2038</span>, dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear2 = nn.Linear(d_ff,d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.linear1(x)))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.linear2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>残差连接和归一化</p>\n<p>​\t由 Transformer 结构组成的网络结构通常都非常庞大。编码器和解码器均由很多层基本的 Transformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究人员在 Transformer 块中进一步引入了残差连接与层归一化技术，以进一步提升训练的稳定性。具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。</p>\n</li>\n</ol>\n<h2 id=\"解码器与编码器\"><a class=\"markdownIt-Anchor\" href=\"#解码器与编码器\">#</a> 解码器与编码器</h2>\n<p>​\t编码器端较容易实现。相比于编码器端，解码器端更复杂。具体来说，解码器的每个 Transformer 块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力部分。这主要是因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息。解码器端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的，因此这一额外增加的掩码是用来掩盖后续的文本信息的，以防模型在训练阶段直接看到后续的文本序列，进而无法得到有效的训练。此外，解码器端额外增加了一个多头交叉注意力模块，使用交叉注意力方法，同时接收来自编码器端的输出和当前 Transformer 块的前一个掩码注意力层的输出。查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。</p>\n<p>​\t解码器端以自回归的方式生成目标语言文本，即在每个时间步 <em>t</em>，根据编码器端输出的源语言文本表示，以及前 t <em>−</em> 1 个时刻生成的目标语言文本，生成当前时刻的目标语言单词（以我的理解来说，训练阶段是没有显示时间步概念的，通过<strong>一次性输入完整序列 + 掩码矩阵</strong>，在单次前向传播中并行计算出所有位置的输出，同时利用掩码强制模型行为与自回归一致，而推理时必须显式按时间步生成，因为未来词未知（无法并行））。</p>\n<h2 id=\"以推理生成中文翻译-我爱你-为例\"><a class=\"markdownIt-Anchor\" href=\"#以推理生成中文翻译-我爱你-为例\">#</a> 以推理生成中文翻译  <code>&quot;我爱你&quot;</code>  为例：</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">时间步</th>\n<th style=\"text-align:center\">图 1 中对应的模块流程</th>\n<th style=\"text-align:center\">具体操作</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>t=1</code></td>\n<td style=\"text-align:center\">词元嵌入 → 位置编码 → 掩码多头注意力 → 编码器 - 解码器注意力 → 前馈网络 → Softmax</td>\n<td style=\"text-align:center\">输入  <code>&lt;start&gt;</code> ，输出  <code>&quot;我&quot;</code>  的概率分布。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>t=2</code></td>\n<td style=\"text-align:center\">词元嵌入 ( <code>&lt;start&gt; 我</code> ) → 位置编码 → 掩码多头注意力 → … → Softmax</td>\n<td style=\"text-align:center\">输入  <code>&lt;start&gt; 我</code> ，输出  <code>&quot;爱&quot;</code>  的概率分布。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>t=3</code></td>\n<td style=\"text-align:center\">词元嵌入 ( <code>&lt;start&gt; 我爱</code> ) → 位置编码 → … → Softmax</td>\n<td style=\"text-align:center\">输入  <code>&lt;start&gt; 我爱</code> ，输出  <code>&quot;你&quot;</code>  的概率分布。</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>t=4</code></td>\n<td style=\"text-align:center\">词元嵌入 ( <code>&lt;start&gt; 我爱你</code> ) → … → Softmax</td>\n<td style=\"text-align:center\">输入  <code>&lt;start&gt; 我爱你</code> ，输出  <code>&lt;end&gt;</code>  的概率分布，终止生成。</td>\n</tr>\n</tbody>\n</table>\n<p>可参考文章：<a href=\"https://blog.csdn.net/m0_64148253/article/details/140422497\">https://blog.csdn.net/m0_64148253/article/details/140422497</a></p>\n",
            "tags": [
                "LLM",
                "transformer"
            ]
        },
        {
            "id": "http://example.com/2025/05/12/LLM-concept/",
            "url": "http://example.com/2025/05/12/LLM-concept/",
            "title": "LLM-concept",
            "date_published": "2025-05-12T07:54:00.000Z",
            "content_html": "<h1 id=\"大模型基本概念\"><a class=\"markdownIt-Anchor\" href=\"#大模型基本概念\">#</a> 大模型基本概念</h1>\n<h2 id=\"目标\"><a class=\"markdownIt-Anchor\" href=\"#目标\">#</a> 目标</h2>\n<ul>\n<li>\n<p>语言模型就是对自然语言的概率分布进行建模，即 P (w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p>\n</li>\n<li>\n<p>计算下一个词的概率 P (wn | w1 w2 w3… wn-1)</p>\n<p><img src=\"image1.png\" alt=\"image\"></p>\n</li>\n</ul>\n<h2 id=\"发展历程\"><a class=\"markdownIt-Anchor\" href=\"#发展历程\">#</a> 发展历程</h2>\n<p>从 n-gram:</p>\n<p><img src=\"image2.png\" alt=\"image\"></p>\n<p>到 neural language model: 每个词都映射成一个低维向量</p>\n<p><img src=\"image3.png\" alt=\"image\"></p>\n<p>再到后面的 transformer 出现，transformer 的出现，NLP 进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是 PLM。</p>\n<p>随着 OpenAI 发布的 1750 亿个参数（GPT-3），开启 LLM 时代</p>\n<h2 id=\"问题发现\"><a class=\"markdownIt-Anchor\" href=\"#问题发现\">#</a> 问题发现</h2>\n<p>・大模型（如 GPT-3）参数量极大（1750 亿 +），传统 “预训练 + 微调” 范式成本过高（需为每个任务调整海量参数）。</p>\n<ol>\n<li>\n<p>解决方案：<br>\n・开发新范式（ICL/Prompt），通过输入指令或示例直接引导模型，避免微调。</p>\n<p>・但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p>\n</li>\n<li>\n<p>模型构建的关键：<br>\n・预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握 ICL/Prompt 所需的能力（如任务模式识别、指令遵循）。</p>\n<p>・后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p>\n</li>\n<li>\n<p>结论：<br>\n・新范式（ICL/Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p>\n</li>\n</ol>\n<h2 id=\"llm的构建流程\"><a class=\"markdownIt-Anchor\" href=\"#llm的构建流程\">#</a> LLM 的构建流程</h2>\n<ul>\n<li>预训练： 利用海量训练数据构建多样化内容，构建基础模型 ——&gt; 对长文本建模，使模型具有语言生成能力</li>\n<li>有监督微调 SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li>\n<li>奖励建模 RM：训练一个能够判断文本质量的裁判，对同个提示词，比较 SFT 生成的多个输出的质量</li>\n<li>强化学习 RLHF (human feedback)：基于 RM，优化 SFT 模型</li>\n</ul>\n<p>SFT 相当于学生学会答题，RM 是评分老师，判断 answer 好坏，RLHF 是学生根据老师评分改进答题策略</p>\n<h2 id=\"补充\"><a class=\"markdownIt-Anchor\" href=\"#补充\">#</a> 补充</h2>\n<p><strong>N-gram 模型详解</strong><br>\n N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1 个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p>\n<p>N-gram 的定义：</p>\n<p>・指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p>\n<p>・例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class=\"line\"></span><br><span class=\"line\">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure>\n<p>・核心假设：</p>\n<p>・马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>如何计算概率？</strong><br>\nN-gram 通过统计语料库中词序列的频率来估计概率：</p>\n<p>计算  <code>P(sat | the cat)</code> ：</p>\n<p>P(sat∣the cat)=Count(“the cat”)Count(“the cat sat”)</p>\n<p>若语料中 “the cat” 出现 100 次，“the cat sat” 出现 30 次，则  <code>P(sat | the cat) = 0.3</code> 。</p>\n<p><strong>N-gram 的优缺点</strong></p>\n<table>\n<thead>\n<tr>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>简单高效，计算速度快。</td>\n<td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td>\n</tr>\n<tr>\n<td>小规模数据即可训练。</td>\n<td>数据稀疏性（罕见 n-gram 概率不准确）。</td>\n</tr>\n<tr>\n<td>曾广泛用于机器翻译、拼写检查等任务。</td>\n<td>无法理解语义（仅统计共现频率）。</td>\n</tr>\n</tbody>\n</table>\n",
            "tags": [
                "LLM",
                "concept"
            ]
        }
    ]
}