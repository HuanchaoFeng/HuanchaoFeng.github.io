{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"gpt\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/17/GPT-BertBuild/",
            "url": "http://example.com/2025/05/17/GPT-BertBuild/",
            "title": "GPT&BertBuild",
            "date_published": "2025-05-16T16:37:02.000Z",
            "content_html": "<h2 id=\"生成式预训练语言模型gpt\"><a class=\"markdownIt-Anchor\" href=\"#生成式预训练语言模型gpt\">#</a> 生成式预训练语言模型 GPT</h2>\n<p>GPT 的模型结构，由多个 transformer 的 decoder 组成，结构如下：</p>\n<p><img src=\"image1.png\" alt=\"image1\"></p>\n<p>该模型利用的是 transformer 的解码器部分，训练和推理的过程是类似的，12 层 transformer 模块都在做类似的事情，到最后一层后输出预测的分数（映射到词分类中，得到预测的分数 / 置信度）</p>\n<ul>\n<li>\n<p>有监督下游任务微调：</p>\n<p>在进行下游任务微调时，仅使用 GPT 的最后一层的最后一个词的隐藏状态，同个全连接层映射到标签空间。因为每个词的隐藏状态都聚合了左侧所有历史词的信息，因此最后一个词的隐藏状态天然编码了整个序列的全局语义。</p>\n<p>同时再进行微调的时候，可能会出现模型遗忘预训练阶段学习的通用只是表示，损失通用与泛化能力，出现灾难性遗忘的问题，所以通常采用混合预训练任务损失和下游微调损失缓解这一问题，Loss Function 如下:<br>\n<img src=\"image2.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"bert模型构建\"><a class=\"markdownIt-Anchor\" href=\"#bert模型构建\">#</a> Bert 模型构建</h2>\n<p>Bert 模型一般是基于 Transformer 的编码器部分构建的，是一个相当于完形填空 (mask) 的模型，是考虑双向的模型（同时利用两侧信息）</p>\n<p>例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>:the cat site on the mat</span><br><span class=\"line\">tokenizer(分词):（假设不加入CLS标记和SEP标记）</span><br><span class=\"line\">    code:[<span class=\"number\">1996</span>,<span class=\"number\">4248</span>,<span class=\"number\">2825</span>,<span class=\"number\">2007</span>,<span class=\"number\">1996</span>,<span class=\"number\">3829</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>] <span class=\"comment\"># 0为pad的编码，max假设为8</span></span><br><span class=\"line\">    attention_mask:[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>] <span class=\"comment\"># 0表示填充的位置</span></span><br><span class=\"line\">random mask:选取到了site作为掩码，替换为mask的编码-&gt;<span class=\"number\">103</span></span><br><span class=\"line\">\tcode:[<span class=\"number\">1996</span>,<span class=\"number\">4248</span>,<span class=\"number\">103</span>,<span class=\"number\">2007</span>,<span class=\"number\">1996</span>,<span class=\"number\">3829</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    attention_mask:[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    label:[-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,<span class=\"number\">2825</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>,-<span class=\"number\">100</span>] <span class=\"comment\">#这个是用于计算掩码的真实标签，-100表示不需要计算，只保留被掩码的位置</span></span><br><span class=\"line\">train:</span><br><span class=\"line\">    将上述数据送入Bert模型中</span><br><span class=\"line\">output:</span><br><span class=\"line\">    [</span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        [..,..,..,..,], <span class=\"comment\">#取第二个位置进行计算交叉熵</span></span><br><span class=\"line\">        [..,..,..,..,],</span><br><span class=\"line\">        ........</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>训练过程：加载 dataset, 切分 traindataset 和 testdataset --&gt; 用 traindataset 训练词元分析器，形成 vocab 词表 --&gt; 加载预训练词元分析器 --&gt; 分词序列 --&gt; 加载随即权重的 BERT 模型，设置掩码比例，进行训练，下面是代码训练过程：</p>\n<p>dataset 获取与镜像设置：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#下载数据集：</span></span><br><span class=\"line\">pip install huggingface_hub</span><br><span class=\"line\">huggingface-cli download --repo-<span class=\"built_in\">type</span> dataset legacy-datasets/wikipedia  --local-<span class=\"built_in\">dir</span> wikipedia</span><br><span class=\"line\"><span class=\"comment\"># mirror</span></span><br><span class=\"line\">export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>\n<p>environment (有部分可能不需要）:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">dependencies:</span><br><span class=\"line\">  - _libgcc_mutex=<span class=\"number\">0.1</span>=main</span><br><span class=\"line\">  - _openmp_mutex=<span class=\"number\">5.1</span>=1_gnu</span><br><span class=\"line\">  - bzip2=<span class=\"number\">1.0</span><span class=\"number\">.8</span>=h5eee18b_6</span><br><span class=\"line\">  - ca-certificates=<span class=\"number\">2025.2</span><span class=\"number\">.25</span>=h06a4308_0</span><br><span class=\"line\">  - expat=<span class=\"number\">2.7</span><span class=\"number\">.1</span>=h6a678d5_0</span><br><span class=\"line\">  - ld_impl_linux-<span class=\"number\">64</span>=<span class=\"number\">2.40</span>=h12ee557_0</span><br><span class=\"line\">  - libffi=<span class=\"number\">3.4</span><span class=\"number\">.4</span>=h6a678d5_1</span><br><span class=\"line\">  - libgcc-ng=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libgomp=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libstdcxx-ng=<span class=\"number\">11.2</span><span class=\"number\">.0</span>=h1234567_1</span><br><span class=\"line\">  - libuuid=<span class=\"number\">1.41</span><span class=\"number\">.5</span>=h5eee18b_0</span><br><span class=\"line\">  - ncurses=<span class=\"number\">6.4</span>=h6a678d5_0</span><br><span class=\"line\">  - openssl=<span class=\"number\">3.0</span><span class=\"number\">.16</span>=h5eee18b_0</span><br><span class=\"line\">  - pip=<span class=\"number\">25.1</span>=pyhc872135_2</span><br><span class=\"line\">  - python=<span class=\"number\">3.12</span><span class=\"number\">.9</span>=h5148396_0</span><br><span class=\"line\">  - readline=<span class=\"number\">8.2</span>=h5eee18b_0</span><br><span class=\"line\">  - setuptools=<span class=\"number\">78.1</span><span class=\"number\">.1</span>=py312h06a4308_0</span><br><span class=\"line\">  - sqlite=<span class=\"number\">3.45</span><span class=\"number\">.3</span>=h5eee18b_0</span><br><span class=\"line\">  - tk=<span class=\"number\">8.6</span><span class=\"number\">.14</span>=h39e8969_0</span><br><span class=\"line\">  - wheel=<span class=\"number\">0.45</span><span class=\"number\">.1</span>=py312h06a4308_0</span><br><span class=\"line\">  - xz=<span class=\"number\">5.6</span><span class=\"number\">.4</span>=h5eee18b_1</span><br><span class=\"line\">  - zlib=<span class=\"number\">1.2</span><span class=\"number\">.13</span>=h5eee18b_1</span><br><span class=\"line\">  - pip:</span><br><span class=\"line\">      - accelerate==<span class=\"number\">0.26</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - aiohappyeyeballs==<span class=\"number\">2.6</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - aiohttp==<span class=\"number\">3.11</span><span class=\"number\">.18</span></span><br><span class=\"line\">      - aiosignal==<span class=\"number\">1.3</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - attrs==<span class=\"number\">25.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - certifi==<span class=\"number\">2025.4</span><span class=\"number\">.26</span></span><br><span class=\"line\">      - charset-normalizer==<span class=\"number\">3.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - datasets==<span class=\"number\">3.6</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - dill==<span class=\"number\">0.3</span><span class=\"number\">.8</span></span><br><span class=\"line\">      - filelock==<span class=\"number\">3.18</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - frozenlist==<span class=\"number\">1.6</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - fsspec==<span class=\"number\">2025.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - huggingface-hub==<span class=\"number\">0.31</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - idna==<span class=\"number\">3.10</span></span><br><span class=\"line\">      - jinja2==<span class=\"number\">3.1</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - markupsafe==<span class=\"number\">3.0</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - mpmath==<span class=\"number\">1.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - multidict==<span class=\"number\">6.4</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - multiprocess==<span class=\"number\">0.70</span><span class=\"number\">.16</span></span><br><span class=\"line\">      - mwparserfromhell==<span class=\"number\">0.6</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - networkx==<span class=\"number\">3.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - numpy==<span class=\"number\">2.2</span><span class=\"number\">.5</span></span><br><span class=\"line\">      - nvidia-cublas-cu12==<span class=\"number\">12.6</span><span class=\"number\">.4</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - nvidia-cuda-cupti-cu12==<span class=\"number\">12.6</span><span class=\"number\">.80</span></span><br><span class=\"line\">      - nvidia-cuda-nvrtc-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cuda-runtime-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cudnn-cu12==<span class=\"number\">9.5</span><span class=\"number\">.1</span><span class=\"number\">.17</span></span><br><span class=\"line\">      - nvidia-cufft-cu12==<span class=\"number\">11.3</span><span class=\"number\">.0</span><span class=\"number\">.4</span></span><br><span class=\"line\">      - nvidia-cufile-cu12==<span class=\"number\">1.11</span><span class=\"number\">.1</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - nvidia-curand-cu12==<span class=\"number\">10.3</span><span class=\"number\">.7</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - nvidia-cusolver-cu12==<span class=\"number\">11.7</span><span class=\"number\">.1</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-cusparse-cu12==<span class=\"number\">12.5</span><span class=\"number\">.4</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-cusparselt-cu12==<span class=\"number\">0.6</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - nvidia-nccl-cu12==<span class=\"number\">2.26</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - nvidia-nvjitlink-cu12==<span class=\"number\">12.6</span><span class=\"number\">.85</span></span><br><span class=\"line\">      - nvidia-nvtx-cu12==<span class=\"number\">12.6</span><span class=\"number\">.77</span></span><br><span class=\"line\">      - packaging==<span class=\"number\">25.0</span></span><br><span class=\"line\">      - pandas==<span class=\"number\">2.2</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - propcache==<span class=\"number\">0.3</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - psutil==<span class=\"number\">7.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - pyarrow==<span class=\"number\">20.0</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - python-dateutil==<span class=\"number\">2.9</span><span class=\"number\">.0</span>.post0</span><br><span class=\"line\">      - pytz==<span class=\"number\">2025.2</span></span><br><span class=\"line\">      - pyyaml==<span class=\"number\">6.0</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - regex==<span class=\"number\">2024.11</span><span class=\"number\">.6</span></span><br><span class=\"line\">      - requests==<span class=\"number\">2.32</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - safetensors==<span class=\"number\">0.5</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - six==<span class=\"number\">1.17</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - sympy==<span class=\"number\">1.14</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - tokenizers==<span class=\"number\">0.21</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - torch==<span class=\"number\">2.7</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - tqdm==<span class=\"number\">4.67</span><span class=\"number\">.1</span></span><br><span class=\"line\">      - transformers==<span class=\"number\">4.51</span><span class=\"number\">.3</span></span><br><span class=\"line\">      - triton==<span class=\"number\">3.3</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - typing-extensions==<span class=\"number\">4.13</span><span class=\"number\">.2</span></span><br><span class=\"line\">      - tzdata==<span class=\"number\">2025.2</span></span><br><span class=\"line\">      - urllib3==<span class=\"number\">2.4</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - xxhash==<span class=\"number\">3.5</span><span class=\"number\">.0</span></span><br><span class=\"line\">      - yarl==<span class=\"number\">1.20</span><span class=\"number\">.0</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> BertTokenizerFast,BertConfig,BertForMaskedLM,DataCollatorForLanguageModeling,TrainingArguments,Trainer</span><br><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> tokenizers <span class=\"keyword\">import</span> BertWordPieceTokenizer</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> chain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class=\"string\">&#x27;2&#x27;</span> <span class=\"comment\"># 这个得加，多gpu跑的话会有问题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载本地数据</span></span><br><span class=\"line\">wiki = load_dataset(<span class=\"string\">&quot;/data/hcfeng/learnLLM/wikipedia&quot;</span>, <span class=\"string\">&quot;20220301.en&quot;</span>, split=<span class=\"string\">&quot;train&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#仅保留text列</span></span><br><span class=\"line\">wiki = wiki.remove_columns([col <span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> wiki.column_names <span class=\"keyword\">if</span> col!=<span class=\"string\">&quot;text&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\">#切割数据</span></span><br><span class=\"line\">dataset = wiki</span><br><span class=\"line\">d = dataset.train_test_split(train_size=<span class=\"number\">0.9</span>,test_size=<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dataset_to_text</span>(<span class=\"params\">dataset,filename</span>):</span><br><span class=\"line\">    outpur_path=<span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/&quot;</span>+filename <span class=\"comment\">#拼接地址</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(outpur_path,<span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> dataset[<span class=\"string\">&#x27;text&#x27;</span>]:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(t,file=f)</span><br><span class=\"line\">dataset_to_text(d[<span class=\"string\">&#x27;train&#x27;</span>],<span class=\"string\">&#x27;train.txt&#x27;</span>)</span><br><span class=\"line\">dataset_to_text(d[<span class=\"string\">&#x27;test&#x27;</span>],<span class=\"string\">&#x27;test.txt&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">special_tokens = [</span><br><span class=\"line\"><span class=\"string\">&quot;[PAD]&quot;</span>, <span class=\"string\">&quot;[UNK]&quot;</span>, <span class=\"string\">&quot;[CLS]&quot;</span>, <span class=\"string\">&quot;[SEP]&quot;</span>, <span class=\"string\">&quot;[MASK]&quot;</span>, <span class=\"string\">&quot;&lt;S&gt;&quot;</span>, <span class=\"string\">&quot;&lt;T&gt;&quot;</span></span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果根据训练和测试两个集合训练词元分析器，则需要修改files</span></span><br><span class=\"line\"><span class=\"comment\"># files = [&quot;train.txt&quot;, &quot;test.txt&quot;]</span></span><br><span class=\"line\"><span class=\"comment\"># 仅根据训练集合训练词元分析器</span></span><br><span class=\"line\">files = [<span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/train.txt&quot;</span>]</span><br><span class=\"line\"><span class=\"comment\"># BERT中采用的默认词表大小为30522，可以随意修改</span></span><br><span class=\"line\">vocab_size = <span class=\"number\">30522</span></span><br><span class=\"line\"><span class=\"comment\"># 最大序列长度，该值越小，训练速度越快</span></span><br><span class=\"line\">max_length = <span class=\"number\">512</span></span><br><span class=\"line\"><span class=\"comment\"># 是否将长样本截断</span></span><br><span class=\"line\">truncate_longer_samples = <span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"comment\"># 初始化WordPiece词元分析器</span></span><br><span class=\"line\">tokenizer = BertWordPieceTokenizer()</span><br><span class=\"line\"><span class=\"comment\"># 训练词元分析器，设定的 vocab_size 是最大允许值，但实际生成的词表大小可能更小</span></span><br><span class=\"line\">tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens) </span><br><span class=\"line\"><span class=\"comment\"># 允许截断达到最大512个词元</span></span><br><span class=\"line\">tokenizer.enable_truncation(max_length=max_length)</span><br><span class=\"line\">model_path = <span class=\"string\">&quot;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果文件夹不存在，则先创建文件夹</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.isdir(model_path):</span><br><span class=\"line\">    os.mkdir(model_path)</span><br><span class=\"line\"><span class=\"comment\"># 保存词元分析器模型</span></span><br><span class=\"line\">tokenizer.save_model(model_path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将一些词元分析器中的配置保存到配置文件，包括特殊词元、转换为小写、最大序列长度等</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(os.path.join(model_path, <span class=\"string\">&quot;config.json&quot;</span>), <span class=\"string\">&quot;w&quot;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    tokenizer_cfg = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;do_lower_case&quot;</span>: <span class=\"literal\">True</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;unk_token&quot;</span>: <span class=\"string\">&quot;[UNK]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;sep_token&quot;</span>: <span class=\"string\">&quot;[SEP]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;pad_token&quot;</span>: <span class=\"string\">&quot;[PAD]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;cls_token&quot;</span>: <span class=\"string\">&quot;[CLS]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;mask_token&quot;</span>: <span class=\"string\">&quot;[MASK]&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;model_max_length&quot;</span>: max_length,</span><br><span class=\"line\">    <span class=\"string\">&quot;max_len&quot;</span>: max_length,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    json.dump(tokenizer_cfg, f)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tokenizer处理序列会自动添加CLS和SEP，但是tokenizer.tokenize()只会进行基础分词，不会添加特殊分词</span></span><br><span class=\"line\">tokenizer = BertTokenizerFast.from_pretrained(<span class=\"string\">&#x27;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;tokenier:<span class=\"subst\">&#123;tokenizer&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">encode_with_truncation</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; 使用词元分析对句子进行处理并截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], truncation=<span class=\"literal\">True</span>, padding=<span class=\"string\">&quot;max_length&quot;</span>,max_length=max_length, return_special_tokens_mask=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">encode_without_truncation</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; 使用词元分析对句子进行处理且不截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], return_special_tokens_mask=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 编码函数将依赖于truncate_longer_samples变量</span></span><br><span class=\"line\">encode = encode_with_truncation <span class=\"keyword\">if</span> truncate_longer_samples <span class=\"keyword\">else</span> encode_without_truncation</span><br><span class=\"line\"><span class=\"comment\"># 对训练数据集进行分词处理</span></span><br><span class=\"line\">train_dataset = d[<span class=\"string\">&quot;train&quot;</span>].<span class=\"built_in\">map</span>(encode, batched=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># 对测试数据集进行分词处理</span></span><br><span class=\"line\">test_dataset = d[<span class=\"string\">&quot;test&quot;</span>].<span class=\"built_in\">map</span>(encode, batched=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> truncate_longer_samples:</span><br><span class=\"line\">    <span class=\"comment\"># 移除其他列，将input_ids和attention_mask设置为PyTorch张量</span></span><br><span class=\"line\">    train_dataset.set_format(<span class=\"built_in\">type</span>=<span class=\"string\">&quot;torch&quot;</span>, columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>])</span><br><span class=\"line\">    test_dataset.set_format(<span class=\"built_in\">type</span>=<span class=\"string\">&quot;torch&quot;</span>, columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>])</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 移除其他列，将它们保留为Python列表</span></span><br><span class=\"line\">    test_dataset.set_format(columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>, <span class=\"string\">&quot;special_tokens_mask&quot;</span>])</span><br><span class=\"line\">    train_dataset.set_format(columns=[<span class=\"string\">&quot;input_ids&quot;</span>, <span class=\"string\">&quot;attention_mask&quot;</span>, <span class=\"string\">&quot;special_tokens_mask&quot;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用于不截断的情况</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">group_texts</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 拼接所有文本</span></span><br><span class=\"line\">    concatenated_examples = &#123;k: <span class=\"built_in\">list</span>(chain(*examples[k])) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> examples.keys()&#125;</span><br><span class=\"line\">    total_length = <span class=\"built_in\">len</span>(concatenated_examples[<span class=\"built_in\">list</span>(examples.keys())[<span class=\"number\">0</span>]])</span><br><span class=\"line\">    <span class=\"comment\"># 舍弃了剩余部分，如果模型支持填充而不是舍弃，则可以根据需要自定义这部分</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> total_length &gt;= max_length:</span><br><span class=\"line\">        total_length = (total_length // max_length) * max_length</span><br><span class=\"line\">    <span class=\"comment\"># 按照最大长度分割成块</span></span><br><span class=\"line\">    result = &#123;</span><br><span class=\"line\">    k: [t[i : i + max_length] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, total_length, max_length)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k, t <span class=\"keyword\">in</span> concatenated_examples.items()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> truncate_longer_samples:</span><br><span class=\"line\">    train_dataset = train_dataset.<span class=\"built_in\">map</span>(group_texts, batched=<span class=\"literal\">True</span>,desc=<span class=\"string\">f&quot;Grouping texts in chunks of <span class=\"subst\">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class=\"line\">    test_dataset = test_dataset.<span class=\"built_in\">map</span>(group_texts, batched=<span class=\"literal\">True</span>,desc=<span class=\"string\">f&quot;Grouping texts in chunks of <span class=\"subst\">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 将它们从列表转换为PyTorch张量</span></span><br><span class=\"line\">    train_dataset.set_format(<span class=\"string\">&quot;torch&quot;</span>)</span><br><span class=\"line\">    test_dataset.set_format(<span class=\"string\">&quot;torch&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;tokenizer.vocab_size:<span class=\"subst\">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">#这里vocab_size一般设为tokenizer.vocab_size，其实只要大于这个数字都是可以的，只是会占用了显存空间（training)，但是不能设小，会出现索引越界的问题</span></span><br><span class=\"line\">model_config = BertConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=max_length) </span><br><span class=\"line\">model = BertForMaskedLM(config=model_config)</span><br><span class=\"line\"></span><br><span class=\"line\">data_collator = DataCollatorForLanguageModeling(</span><br><span class=\"line\">    tokenizer=tokenizer, mlm=<span class=\"literal\">True</span>, mlm_probability=<span class=\"number\">0.2</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">training_args = TrainingArguments(</span><br><span class=\"line\">    output_dir=model_path, <span class=\"comment\"># 输出目录，用于保存模型检查点</span></span><br><span class=\"line\">    eval_strategy=<span class=\"string\">&quot;steps&quot;</span>, <span class=\"comment\"># 每隔`logging_steps`步进行一次评估</span></span><br><span class=\"line\">    overwrite_output_dir=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    num_train_epochs=<span class=\"number\">100</span>, <span class=\"comment\"># 训练时的轮数，可以根据需要进行调整</span></span><br><span class=\"line\">    per_device_train_batch_size=<span class=\"number\">10</span>, <span class=\"comment\"># 训练批量大小，可以根据GPU内存容量将其设置得尽可能大</span></span><br><span class=\"line\">    gradient_accumulation_steps=<span class=\"number\">8</span>, <span class=\"comment\"># 在更新权重之前累积梯度</span></span><br><span class=\"line\">    per_device_eval_batch_size=<span class=\"number\">64</span>, <span class=\"comment\"># 评估批量大小</span></span><br><span class=\"line\">    logging_steps=<span class=\"number\">1000</span>, <span class=\"comment\"># 每隔1000步进行一次评估，记录并保存模型检查点</span></span><br><span class=\"line\">    save_steps=<span class=\"number\">1000</span>,</span><br><span class=\"line\">    <span class=\"comment\"># load_best_model_at_end=True, # 是否在训练结束时加载最佳模型（根据损失）</span></span><br><span class=\"line\">    <span class=\"comment\"># save_total_limit=3, # 如果磁盘空间有限，则可以限制只保存3个模型权重</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    model=model,</span><br><span class=\"line\">    args=training_args,</span><br><span class=\"line\">    data_collator=data_collator,</span><br><span class=\"line\">    train_dataset=train_dataset,</span><br><span class=\"line\">    eval_dataset=test_dataset,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#检查点</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids dtype:&quot;</span>, train_dataset[<span class=\"number\">0</span>][<span class=\"string\">&quot;input_ids&quot;</span>].dtype)  <span class=\"comment\"># 应该是 torch.int64 (long)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;attention_mask dtype:&quot;</span>, train_dataset[<span class=\"number\">0</span>][<span class=\"string\">&quot;attention_mask&quot;</span>].dtype)  <span class=\"comment\"># 应该是 torch.int64 (long)</span></span><br><span class=\"line\">batch = data_collator([train_dataset[<span class=\"number\">0</span>], train_dataset[<span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids shape:&quot;</span>, batch[<span class=\"string\">&quot;input_ids&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;batch[&quot;input_ids&quot;]:<span class=\"subst\">&#123;batch[<span class=\"string\">&quot;input_ids&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;labels shape:&quot;</span>, batch[<span class=\"string\">&quot;labels&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"comment\"># print(&quot;labels min/max:&quot;, torch.min(batch[&quot;labels&quot;]), torch.max(batch[&quot;labels&quot;]))</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;input_ids min/max:&quot;</span>, torch.<span class=\"built_in\">min</span>(batch[<span class=\"string\">&quot;input_ids&quot;</span>]), torch.<span class=\"built_in\">max</span>(batch[<span class=\"string\">&quot;input_ids&quot;</span>]))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.__version__)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.version.cuda)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.is_available()) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练模型</span></span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Bert",
                "GPT"
            ]
        }
    ]
}