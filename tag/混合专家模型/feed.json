{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"混合专家模型\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "url": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "title": "LLaMA&混合专家模型",
            "date_published": "2025-05-19T10:50:11.000Z",
            "content_html": "<h2 id=\"llama-model-structure\"><a class=\"markdownIt-Anchor\" href=\"#llama-model-structure\">#</a> LLaMA  model  structure</h2>\n<p>​\tLLaMA 是基于 transformer 的 decoder 部分构建的，采用前置层归一化、使用 RMSNorm 规划函数，激活函数更改为 SwiGLU，使用旋转位置嵌入更改的 decoder 模型。更改的位置如下所示：</p>\n<p><img src=\"image1.png\" alt=\"image1\"></p>\n<ul>\n<li>\n<p>RMSNorm 函数：</p>\n<p><img src=\"image2.png\" alt=\"image2\"></p>\n<p>原始层归一化函数：</p>\n<p><img src=\"image3.png\" alt=\"image2\"></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"><strong>对比</strong></th>\n<th style=\"text-align:center\"><strong>LayerNorm</strong></th>\n<th style=\"text-align:center\"><strong>RMSNorm</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><strong>归一化目标</strong></td>\n<td style=\"text-align:center\">均值中心化 + 方差缩放</td>\n<td style=\"text-align:center\">仅均方根（RMS）缩放</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>计算复杂度</strong></td>\n<td style=\"text-align:center\">较高（需计算均值和方差）</td>\n<td style=\"text-align:center\">较低（仅需均方值）</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>参数数量</strong></td>\n<td style=\"text-align:center\"><em>γ</em>+<em>β</em>（2d 参数）</td>\n<td style=\"text-align:center\">仅 <em>γ</em>（d 参数）</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p>SwiGLU</p>\n<p>SwiGLU 是门控线性单元（GLU）的变体，公式如下：<br>\n<img src=\"image4.png\" alt=\"image2\"></p>\n<p><img src=\"image5.png\" alt=\"image2\"></p>\n<p>第二个公式的激活函数是 sigmoid，sigmoid 函数特点：</p>\n</li>\n</ul>\n<p><img src=\"image6.png\" alt=\"image2\"></p>\n<p>当 β 趋向于 0 时，相当于 y=x/2，线性函数，当 β 趋向于无穷时（x&gt;0,x&lt;0,x=0)，相当于 ReLU 函数，当 β=1，swish 光滑且非单调。</p>\n<p>Swish (xW) 为门控权重（相当于选择遗忘比例），用权重对 xV 逐元素加权，用 W2 映射回原维度。</p>\n<ul>\n<li>\n<p>RoPE（待）</p>\n<p>传统 PE，model 需要学习隐式位置关系，而 RoPE 通过旋转矩阵直接编码位置，即将位置信息通过旋转矩阵融合 key\\query 向量中，直接建模相对位置依赖关系，value 是不需要旋转的:</p>\n<p><img src=\"image7.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<p>以上三个改变与原 decoder 结合实现了 LLaMA</p>\n<h2 id=\"注意力机制优化\"><a class=\"markdownIt-Anchor\" href=\"#注意力机制优化\">#</a> 注意力机制优化</h2>\n<p>在 Transformer 结构中，自注意力机制的时间和存储复杂度与序列的长度呈平方的关系，因此占用了大量的计算设备内存并消耗了大量的计算资源，比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>: Q K </span><br><span class=\"line\">Q = [](seq_len*vector_len)</span><br><span class=\"line\">K = [](seq_len*vector_len)</span><br><span class=\"line\">Q*K(转置) = seq_len * seq_len</span><br></pre></td></tr></table></figure>\n<p>那么就有这种情况：</p>\n<ul>\n<li><strong>显存占用</strong>：</li>\n<li><strong>计算时间</strong>：<br>\n每次注意力计算需 seq_len 方 * d 次操作，<em>seq_len=32</em>k*, <em>d</em>=1024 时约为 10 的 12 次方次操作。</li>\n</ul>\n<p>所以需要方法去优化这一问题，以下举例两种方法：</p>\n<ul>\n<li>\n<p>稀疏注意力</p>\n<ul>\n<li>全局注意力：在稀疏注意力中保留少量全局节点（如 [CLS] token 或特定位置），这些节点可以与序列中<strong>所有其他位置</strong>交互。</li>\n<li>带状注意力：每个 Query 只与<strong>固定宽度邻域内</strong>的 Key 交互（类似对角带状矩阵）</li>\n<li>膨胀注意力：以<strong>固定间隔跳跃采样</strong> Key</li>\n<li>随机注意力：每个 Query 随机选择 <em>r</em> 个位置进行交互</li>\n<li>局部块注意力：多个不重叠块交互</li>\n</ul>\n<p>一般利用上述的复合模式 s</p>\n</li>\n<li>\n<p>低秩注意力:</p>\n<p><img src=\"image8.jpg\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"混合专家模型\"><a class=\"markdownIt-Anchor\" href=\"#混合专家模型\">#</a> 混合专家模型</h2>\n<p>​\t混合专家模型 (MixedExpert Models，MoEs) 日益受到关注。依据大模型缩放法则，模型规模是提升性能的关键，然而规模扩大必然使计算资源大幅增加。因此，在有限计算资源预算下，如何用更少训练步数训练更大模型成为关键问题。为解决该问题，混合专家模型基于一个简洁的思想：模型不同部分（即 “专家”）专注不同任务或数据层面。</p>\n<p>其实就是把模型内部的一组专用子网络，每个子网络负责处理数据中特定类型的任务，如：</p>\n<ul>\n<li>输入句子是数学问题 → 激活 “数学专家”</li>\n<li>输入是诗歌 → 激活 “文学专家”</li>\n</ul>\n<p>优势：若 model 有 100 个专家，每次输入仅用 2 个，计算量少，而且每个专家通过训练集中学习特定模式，比通用模块更高效。</p>\n<p>混合专家模型按照门控网络（Gate）类型，可以从广义上讲可以分为三个大类：稀疏混合专家模型（Sparse MoE）、稠密混合专家模型（Dense MoE）、软混合专家模型（Soft MoE）：</p>\n<ul>\n<li>\n<p>稀疏混合专家模型：input 之后，门控网络仅激活少数专家</p>\n</li>\n<li>\n<p>稠密混合专家模型：所有专家激活，甲醛组合输出，这个是要计算每个 wi,bi 与 x 的结果</p>\n</li>\n<li>\n<p>软混合专家模型：门控网络分配的权重直接融合不同专家的参数，得到 w (融合)，b (融合)，融合与 x 计算 = w*x+b</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"><strong>类型</strong></th>\n<th style=\"text-align:center\"><strong>专家激活方式</strong></th>\n<th style=\"text-align:center\"><strong>计算量</strong></th>\n<th style=\"text-align:center\"><strong>参数量扩展性</strong></th>\n<th style=\"text-align:center\"><strong>典型场景</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><strong>稀疏 MoE</strong></td>\n<td style=\"text-align:center\">硬性 Top-k 选择（如 k=2）</td>\n<td style=\"text-align:center\"><em>O</em>(<em>k</em>⋅FFN)</td>\n<td style=\"text-align:center\">极高（万亿级）</td>\n<td style=\"text-align:center\">大规模预训练（Mixtral, GPT-4）</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>稠密 MoE</strong></td>\n<td style=\"text-align:center\">所有专家加权求和</td>\n<td style=\"text-align:center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td style=\"text-align:center\">低（十亿级）</td>\n<td style=\"text-align:center\">小规模多任务学习</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>软 MoE</strong></td>\n<td style=\"text-align:center\">软性稀疏权重</td>\n<td style=\"text-align:center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td style=\"text-align:center\">中（百亿级）</td>\n<td style=\"text-align:center\">平衡效率与稳定性需求</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n",
            "tags": [
                "混合专家模型",
                "LLaMA"
            ]
        }
    ]
}