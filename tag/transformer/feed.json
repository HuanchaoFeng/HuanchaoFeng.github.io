{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"transformer\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/13/Transformer/",
            "url": "http://example.com/2025/05/13/Transformer/",
            "title": "Transformer",
            "date_published": "2025-05-13T07:07:29.000Z",
            "content_html": "<h2 id=\"Transformer四层结构\"><a href=\"#Transformer四层结构\" class=\"headerlink\" title=\"Transformer四层结构\"></a>Transformer四层结构</h2><p>Transformer结构：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ol>\n<li><p>嵌入表示层</p>\n<p>Transformer的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：</p>\n<p><img src=\"/image2.png\" alt=\"image1\"></p>\n<p>根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#transformer位置编码</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model,max_seq_len = <span class=\"number\">80</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 根据pos和i创建一个常量PE矩阵</span></span><br><span class=\"line\">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> pos <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>):</span><br><span class=\"line\">                pe[pos, i] = math.sin(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">                pe[pos, i + <span class=\"number\">1</span>] = math.cos(pos / (<span class=\"number\">10000</span> ** (i/d_model)))</span><br><span class=\"line\">        pe = pe.unsqueeze(<span class=\"number\">0</span>) <span class=\"comment\">#形状 (1, seq_len, d_model)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">&#x27;pe&#x27;</span>, pe)   <span class=\"comment\">#将 pe 保存为模型的一部分（不参与梯度更新，但会随模型保存/加载）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"comment\">#x:(batch_size, seq_len, d_model)</span></span><br><span class=\"line\">        seq_len = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = x + <span class=\"variable language_\">self</span>.pe[:,:seq_len].cuda()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>使用正余弦的原因是，函数的范围是[-1，1]与词向量相加不会太偏离原始语义，同时第pos+k个位置的编码是第pos个位置编码的线性组合（根据三角函数和角公式决定），这就蕴含了单词之间的距离信息：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n</li>\n<li><p>自注意力层</p>\n<p>自注意力机制，即自己作为QKV进行计算，但是解码器有两个注意力模块，一个是掩码多头，一个是交叉多头注意力，但是原理其实和下面代码差不多，直接用代码展示比较能说明：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#transformer多头自注意力机制</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, heads, d_model,dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.q_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.k_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.v_linear = nn.Linear(d_model,d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out = nn.Linear(d_model,d_model)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">q, k, v, d_k, mask = <span class=\"literal\">None</span>, dropout = <span class=\"literal\">None</span> </span>):</span><br><span class=\"line\">        <span class=\"comment\"># 转置k相乘 ​​除以 math.sqrt(d_k)​​ 的操作是缩放点积注意力，防止点积数值过大​</span></span><br><span class=\"line\">        scores = torch.matmul(q,k.transpose(-<span class=\"number\">2</span>,-<span class=\"number\">1</span>)) / math.sqrt(d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">            scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>) <span class=\"comment\">#掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        scores = F.sofmax(scores,dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            scores = dropout(scores)</span><br><span class=\"line\"></span><br><span class=\"line\">        output = torch.matmul(scores,v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        batch_size = q.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 利用线性计算划分成h个头</span></span><br><span class=\"line\">        q = <span class=\"variable language_\">self</span>.q_linear(q).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        k = <span class=\"variable language_\">self</span>.k_linear(k).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\">        v = <span class=\"variable language_\">self</span>.v_linear(v).view(batch_size,-<span class=\"number\">1</span>,<span class=\"variable language_\">self</span>.h,<span class=\"variable language_\">self</span>.d_k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#转置头和seq_len位置</span></span><br><span class=\"line\">        k = k.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        q = q.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">        v = v.transpose(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        scores = <span class=\"variable language_\">self</span>.attention(q, k, v, <span class=\"variable language_\">self</span>.d_k, mask, <span class=\"variable language_\">self</span>.dropout)</span><br><span class=\"line\">        <span class=\"comment\"># 拼接多头输出并线性变换</span></span><br><span class=\"line\">        concat = scores.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(batch_size, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        output = <span class=\"variable language_\">self</span>.out(concat) </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>前馈层</p>\n<p>接收注意力层的输出，通过带有ReLU的2层全连接网络，第一层会映射到高纬度，因为隐藏层维度的增大有利于提高质量（实验证明）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#前馈层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FeedForward</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff = <span class=\"number\">2038</span>, dropout = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.linear2 = nn.Linear(d_ff,d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.dropout(F.relu(<span class=\"variable language_\">self</span>.linear1(x)))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.linear2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>残差连接和归一化</p>\n<p>​    由Transformer结构组成的网络结构通常都非常庞大。编码器和解码器均由很多层基本的Transformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究人员在 Transformer 块中进一步引入了残差连接与层归一化技术，以进一步提升训练的稳定性。具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。</p>\n</li>\n</ol>\n<h2 id=\"解码器与编码器\"><a href=\"#解码器与编码器\" class=\"headerlink\" title=\"解码器与编码器\"></a>解码器与编码器</h2><p>​\t编码器端较容易实现。相比于编码器端，解码器端更复杂。具体来说，解码器的每个 Transformer 块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力部分。这主要是因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息。解码器端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的，因此这一额外增加的掩码是用来掩盖后续的文本信息的，以防模型在训练阶段直接看到后续的文本序列，进而无法得到有效的训练。此外，解码器端额外增加了一个多头交叉注意力模块，使用交叉注意力方法，同时接收来自编码器端的输出和当前 Transformer 块的前一个掩码注意力层的输出。查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。</p>\n<p>​\t解码器端以自回归的方式生成目标语言文本，即在每个时间步 <em>t</em>，根据编码器端输出的源语言文本表示，以及前t <em>−</em> 1 个时刻生成的目标语言文本，生成当前时刻的目标语言单词（以我的理解来说，训练阶段是没有显示时间步概念的，通过<strong>一次性输入完整序列 + 掩码矩阵</strong>，在单次前向传播中并行计算出所有位置的输出，同时利用掩码强制模型行为与自回归一致，而推理时必须显式按时间步生成，因为未来词未知（无法并行））。</p>\n<h2 id=\"以推理生成中文翻译-我爱你-为例：\"><a href=\"#以推理生成中文翻译-我爱你-为例：\" class=\"headerlink\" title=\"以推理生成中文翻译 &quot;我爱你&quot; 为例：\"></a>以推理生成中文翻译 <code>&quot;我爱你&quot;</code> 为例：</h2><table>\n<thead>\n<tr>\n<th align=\"center\">时间步</th>\n<th align=\"center\">图1中对应的模块流程</th>\n<th align=\"center\">具体操作</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><code>t=1</code></td>\n<td align=\"center\">词元嵌入 → 位置编码 → 掩码多头注意力 → 编码器-解码器注意力 → 前馈网络 → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt;</code>，输出 <code>&quot;我&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=2</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我</code>) → 位置编码 → 掩码多头注意力 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我</code>，输出 <code>&quot;爱&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=3</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱</code>) → 位置编码 → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱</code>，输出 <code>&quot;你&quot;</code> 的概率分布。</td>\n</tr>\n<tr>\n<td align=\"center\"><code>t=4</code></td>\n<td align=\"center\">词元嵌入 (<code>&lt;start&gt; 我爱你</code>) → … → Softmax</td>\n<td align=\"center\">输入 <code>&lt;start&gt; 我爱你</code>，输出 <code>&lt;end&gt;</code> 的概率分布，终止生成。</td>\n</tr>\n</tbody></table>\n",
            "tags": [
                "LLM",
                "transformer"
            ]
        }
    ]
}