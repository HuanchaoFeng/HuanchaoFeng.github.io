{
    "version": "https://jsonfeed.org/version/1",
    "title": "Phoenix • All posts by \"llama\" tag",
    "description": "Every day is a chance to learn something new",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "url": "http://example.com/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/",
            "title": "LLaMA&混合专家模型",
            "date_published": "2025-05-19T10:50:11.000Z",
            "content_html": "<h2 id=\"LLaMA-model-structure\"><a href=\"#LLaMA-model-structure\" class=\"headerlink\" title=\"LLaMA  model  structure\"></a>LLaMA  model  structure</h2><p>​\tLLaMA是基于transformer的decoder部分构建的，采用前置层归一化、使用RMSNorm规划函数，激活函数更改为SwiGLU，使用旋转位置嵌入更改的decoder模型。更改的位置如下所示：</p>\n<p><img src=\"/image1.png\" alt=\"image1\"></p>\n<ul>\n<li><p>RMSNorm函数：</p>\n<p><img src=\"/image2.png\" alt=\"image2\"></p>\n<p>原始层归一化函数：</p>\n<p><img src=\"/image3.png\" alt=\"image2\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><strong>对比</strong></th>\n<th align=\"center\"><strong>LayerNorm</strong></th>\n<th align=\"center\"><strong>RMSNorm</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>归一化目标</strong></td>\n<td align=\"center\">均值中心化 + 方差缩放</td>\n<td align=\"center\">仅均方根（RMS）缩放</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>计算复杂度</strong></td>\n<td align=\"center\">较高（需计算均值和方差）</td>\n<td align=\"center\">较低（仅需均方值）</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>参数数量</strong></td>\n<td align=\"center\"><em>γ</em>+<em>β</em>（2d 参数）</td>\n<td align=\"center\">仅 <em>γ</em>（d 参数）</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>SwiGLU</p>\n<p>SwiGLU是门控线性单元（GLU）的变体，公式如下：<br><img src=\"/image4.png\" alt=\"image2\"></p>\n<p><img src=\"/image5.png\" alt=\"image2\"></p>\n<p>第二个公式的激活函数是sigmoid，sigmoid函数特点：</p>\n</li>\n</ul>\n<p><img src=\"/image6.png\" alt=\"image2\"></p>\n<p>当β趋向于0时，相当于y&#x3D;x&#x2F;2，线性函数，当β趋向于无穷时（x&gt;0,x&lt;0,x&#x3D;0)，相当于ReLU函数，当β&#x3D;1，swish光滑且非单调。</p>\n<p>Swish(xW)为门控权重（相当于选择遗忘比例），用权重对xV逐元素加权，用W2映射回原维度。</p>\n<ul>\n<li><p>RoPE（待）</p>\n<p>传统PE，model需要学习隐式位置关系，而RoPE通过旋转矩阵直接编码位置，即将位置信息通过旋转矩阵融合key\\query向量中，直接建模相对位置依赖关系，value是不需要旋转的:</p>\n<p><img src=\"/image7.png\" alt=\"image2\"></p>\n</li>\n</ul>\n<p>以上三个改变与原decoder结合实现了LLaMA</p>\n<h2 id=\"注意力机制优化\"><a href=\"#注意力机制优化\" class=\"headerlink\" title=\"注意力机制优化\"></a>注意力机制优化</h2><p>在 Transformer 结构中，自注意力机制的时间和存储复杂度与序列的长度呈平方的关系，因此占用了大量的计算设备内存并消耗了大量的计算资源，比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span>: Q K </span><br><span class=\"line\">Q = [](seq_len*vector_len)</span><br><span class=\"line\">K = [](seq_len*vector_len)</span><br><span class=\"line\">Q*K(转置) = seq_len * seq_len</span><br></pre></td></tr></table></figure>\n\n<p>那么就有这种情况：</p>\n<ul>\n<li><strong>显存占用</strong>：</li>\n<li><strong>计算时间</strong>：<br>每次注意力计算需 seq_len方 * d 次操作，<em>seq_len&#x3D;32</em>k*, <em>d</em>&#x3D;1024 时约为 10的12 次方次操作。</li>\n</ul>\n<p>所以需要方法去优化这一问题，以下举例两种方法：</p>\n<ul>\n<li><p>稀疏注意力</p>\n<ul>\n<li>全局注意力：在稀疏注意力中保留少量全局节点（如 [CLS] token 或特定位置），这些节点可以与序列中<strong>所有其他位置</strong>交互。</li>\n<li>带状注意力：每个 Query 只与<strong>固定宽度邻域内</strong>的 Key 交互（类似对角带状矩阵）</li>\n<li>膨胀注意力：以<strong>固定间隔跳跃采样</strong> Key</li>\n<li>随机注意力：每个 Query 随机选择 <em>r</em> 个位置进行交互</li>\n<li>局部块注意力：多个不重叠块交互</li>\n</ul>\n<p>一般利用上述的复合模式s</p>\n</li>\n<li><p>低秩注意力:</p>\n<p><img src=\"/image8.jpg\" alt=\"image2\"></p>\n</li>\n</ul>\n<h2 id=\"混合专家模型\"><a href=\"#混合专家模型\" class=\"headerlink\" title=\"混合专家模型\"></a>混合专家模型</h2><p>​\t混合专家模型 (MixedExpert Models，MoEs) 日益受到关注。依据大模型缩放法则，模型规模是提升性能的关键，然而规模扩大必然使计算资源大幅增加。因此，在有限计算资源预算下，如何用更少训练步数训练更大模型成为关键问题。为解决该问题，混合专家模型基于一个简洁的思想：模型不同部分（即“专家”）专注不同任务或数据层面。</p>\n<p>其实就是把模型内部的一组专用子网络，每个子网络负责处理数据中特定类型的任务，如：</p>\n<ul>\n<li>输入句子是数学问题 → 激活“数学专家”</li>\n<li>输入是诗歌 → 激活“文学专家”</li>\n</ul>\n<p>优势：若model有100个专家，每次输入仅用2个，计算量少，而且每个专家通过训练集中学习特定模式，比通用模块更高效。</p>\n<p>混合专家模型按照门控网络（Gate）类型，可以从广义上讲可以分为三个大类：稀疏混合专家模型（Sparse MoE）、稠密混合专家模型（Dense MoE）、软混合专家模型（Soft MoE）：</p>\n<ul>\n<li><p>稀疏混合专家模型：input之后，门控网络仅激活少数专家</p>\n</li>\n<li><p>稠密混合专家模型：所有专家激活，甲醛组合输出，这个是要计算每个wi,bi与x的结果</p>\n</li>\n<li><p>软混合专家模型：门控网络分配的权重直接融合不同专家的参数，得到w(融合)，b(融合)，融合与x计算&#x3D;w*x+b</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><strong>类型</strong></th>\n<th align=\"center\"><strong>专家激活方式</strong></th>\n<th align=\"center\"><strong>计算量</strong></th>\n<th align=\"center\"><strong>参数量扩展性</strong></th>\n<th align=\"center\"><strong>典型场景</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>稀疏MoE</strong></td>\n<td align=\"center\">硬性Top-k选择（如k&#x3D;2）</td>\n<td align=\"center\"><em>O</em>(<em>k</em>⋅FFN)</td>\n<td align=\"center\">极高（万亿级）</td>\n<td align=\"center\">大规模预训练（Mixtral, GPT-4）</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>稠密MoE</strong></td>\n<td align=\"center\">所有专家加权求和</td>\n<td align=\"center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td align=\"center\">低（十亿级）</td>\n<td align=\"center\">小规模多任务学习</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>软MoE</strong></td>\n<td align=\"center\">软性稀疏权重</td>\n<td align=\"center\"><em>O</em>(<em>N</em>⋅FFN)</td>\n<td align=\"center\">中（百亿级）</td>\n<td align=\"center\">平衡效率与稳定性需求</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n",
            "tags": [
                "混合专家模型",
                "LLaMA"
            ]
        }
    ]
}