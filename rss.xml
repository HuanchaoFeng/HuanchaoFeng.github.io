<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Phoenix</title>
        <link>http://example.com</link>
        <description>Every day is a chance to learn something new</description>
        <language>en</language>
        <pubDate>Mon, 12 May 2025 15:54:00 +0800</pubDate>
        <lastBuildDate>Mon, 12 May 2025 15:54:00 +0800</lastBuildDate>
        <category>LLM</category>
        <category>concept</category>
        <category>resume</category>
        <item>
            <guid isPermalink="true">http://example.com/2025/05/12/LLM-concept/</guid>
            <title>LLM-concept</title>
            <link>http://example.com/2025/05/12/LLM-concept/</link>
            <category>LLM</category>
            <category>concept</category>
            <pubDate>Mon, 12 May 2025 15:54:00 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;大模型基本概念&#34;&gt;&lt;a href=&#34;#大模型基本概念&#34; class=&#34;headerlink&#34; title=&#34;大模型基本概念&#34;&gt;&lt;/a&gt;大模型基本概念&lt;/h1&gt;&lt;h2 id=&#34;目标&#34;&gt;&lt;a href=&#34;#目标&#34; class=&#34;headerlink&#34; title=&#34;目标&#34;&gt;&lt;/a&gt;目标&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;语言模型就是对自然语言的概率分布进行建模，即P(w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算下一个词的概率 P(wn | w1 w2 w3… wn-1) &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/.com//image1.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;发展历程&#34;&gt;&lt;a href=&#34;#发展历程&#34; class=&#34;headerlink&#34; title=&#34;发展历程&#34;&gt;&lt;/a&gt;发展历程&lt;/h2&gt;&lt;p&gt;从n-gram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/.com//image2.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;到neural language model: 每个词都映射成一个低维向量&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/.com//image3.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;再到后面的transformer出现，transformer的出现，NLP进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是PLM。&lt;/p&gt;
&lt;p&gt;随着OpenAI发布的1750亿个参数（GPT-3），开启LLM时代&lt;/p&gt;
&lt;h2 id=&#34;问题发现&#34;&gt;&lt;a href=&#34;#问题发现&#34; class=&#34;headerlink&#34; title=&#34;问题发现&#34;&gt;&lt;/a&gt;问题发现&lt;/h2&gt;&lt;p&gt; • 大模型（如GPT-3）参数量极大（1750亿+），传统“预训练+微调”范式成本过高（需为每个任务调整海量参数）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解决方案：&lt;br&gt;• 开发新范式（ICL&amp;#x2F;Prompt），通过输入指令或示例直接引导模型，避免微调。&lt;/p&gt;
&lt;p&gt;• 但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;模型构建的关键：&lt;br&gt;• 预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握ICL&amp;#x2F;Prompt所需的能力（如任务模式识别、指令遵循）。&lt;/p&gt;
&lt;p&gt;• 后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;结论：&lt;br&gt;• 新范式（ICL&amp;#x2F;Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;LLM的构建流程&#34;&gt;&lt;a href=&#34;#LLM的构建流程&#34; class=&#34;headerlink&#34; title=&#34;LLM的构建流程&#34;&gt;&lt;/a&gt;LLM的构建流程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;预训练： 利用海量训练数据构建多样化内容，构建基础模型——&amp;gt;对长文本建模，使模型具有语言生成能力&lt;/li&gt;
&lt;li&gt;有监督微调SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果&lt;/li&gt;
&lt;li&gt;奖励建模RM：训练一个能够判断文本质量的裁判，对同个提示词，比较SFT生成的多个输出的质量&lt;/li&gt;
&lt;li&gt;强化学习RLHF(human feedback)：基于RM，优化SFT模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SFT相当于学生学会答题，RM是评分老师，判断answer好坏，RLHF是学生根据老师评分改进答题策略&lt;/p&gt;
&lt;h2 id=&#34;补充&#34;&gt;&lt;a href=&#34;#补充&#34; class=&#34;headerlink&#34; title=&#34;补充&#34;&gt;&lt;/a&gt;补充&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;N-gram 模型详解&lt;/strong&gt;&lt;br&gt; N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。&lt;/p&gt;
&lt;p&gt; N-gram的定义：&lt;/p&gt;
&lt;p&gt;• 指文本中连续的 &lt;em&gt;n&lt;/em&gt; 个词（或字符）组成的序列。&lt;/p&gt;
&lt;p&gt;• 例如：&lt;/p&gt;
&lt;figure class=&#34;highlight plaintext&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;◦ Unigram (1-gram): &amp;quot;the&amp;quot;、&amp;quot;cat&amp;quot;、&amp;quot;sat&amp;quot;（单个词）。  &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;◦ Bigram (2-gram): &amp;quot;the cat&amp;quot;、&amp;quot;cat sat&amp;quot;、&amp;quot;sat on&amp;quot;（两个连续词）。  &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;◦ Trigram (3-gram): &amp;quot;the cat sat&amp;quot;、&amp;quot;cat sat on&amp;quot;（三个连续词）。  &lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;• 核心假设：&lt;/p&gt;
&lt;p&gt;• 马尔可夫假设：当前词的概率仅依赖于前 &lt;em&gt;n-1&lt;/em&gt; 个词，而非整个历史。&lt;/p&gt;
&lt;figure class=&#34;highlight plaintext&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;如何计算概率？&lt;/strong&gt;&lt;br&gt; N-gram 通过统计语料库中词序列的频率来估计概率：&lt;/p&gt;
&lt;p&gt;计算 &lt;code&gt;P(sat | the cat)&lt;/code&gt;：&lt;/p&gt;
&lt;p&gt;P(sat∣the cat)&amp;#x3D;Count(“the cat”)Count(“the cat sat”)&lt;/p&gt;
&lt;p&gt;若语料中 “the cat” 出现 100 次，”the cat sat” 出现 30 次，则 &lt;code&gt;P(sat | the cat) = 0.3&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N-gram 的优缺点&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;优点&lt;/th&gt;
&lt;th&gt;缺点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;简单高效，计算速度快。&lt;/td&gt;
&lt;td&gt;无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;小规模数据即可训练。&lt;/td&gt;
&lt;td&gt;数据稀疏性（罕见 n-gram 概率不准确）。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;曾广泛用于机器翻译、拼写检查等任务。&lt;/td&gt;
&lt;td&gt;无法理解语义（仅统计共现频率）。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2025/05/11/resume/</guid>
            <title>resume</title>
            <link>http://example.com/2025/05/11/resume/</link>
            <category>resume</category>
            <pubDate>Sun, 11 May 2025 15:54:37 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;HuanchaoFeng-Resume&#34;&gt;&lt;a href=&#34;#HuanchaoFeng-Resume&#34; class=&#34;headerlink&#34; title=&#34;HuanchaoFeng Resume&#34;&gt;&lt;/a&gt;HuanchaoFeng Resume&lt;/h1&gt;&lt;h2 id=&#34;教育经历&#34;&gt;&lt;a href=&#34;#教育经历&#34; class=&#34;headerlink&#34; title=&#34;教育经历&#34;&gt;&lt;/a&gt;教育经历&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;深圳大学（硕士） 2024-2027 计算机技术 硕士一等奖学金&lt;/li&gt;
&lt;li&gt;广东财经大学（本科） 2020-2024 计算机科学与技术 学业奖学金  CET-6&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;实习经历&#34;&gt;&lt;a href=&#34;#实习经历&#34; class=&#34;headerlink&#34; title=&#34;实习经历&#34;&gt;&lt;/a&gt;实习经历&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深圳迅策科技股份有限公司 	2024.11-2025.02    	后端研发实习生 技术支持中心—政府项目组&lt;/p&gt;
&lt;p&gt;后端研发实习生 技术支持中心—政府项目组&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;工作描述：负责智慧交通平台道路信息服务的后端研发&lt;/li&gt;
&lt;li&gt;参与问题定位开发：使用AOP并结合自定义注解获取全局请求与处理信息，增强API的可追踪性和调试效率，通过此配置团队成员可以快速定位问题和分析系统行为，提高开发效率&lt;/li&gt;
&lt;li&gt;数据表设计与查询优化：独立完成交通流量、违法信息、道路信息等5个模块的数据设计，包含数据表设计、字段抽象与设计，同时在百万级数据的交通流量表设计上建立索引，将平均回表次数从160w次优化为200次&lt;/li&gt;
&lt;li&gt;道路模块开发：负责相关需求开发，并基于内部CI&amp;#x2F;CD平台搭建自动化测试流水线，保证在提交测试前各接口单元测试覆盖率达到70%以上，核心链路全覆盖&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;项目经历&#34;&gt;&lt;a href=&#34;#项目经历&#34; class=&#34;headerlink&#34; title=&#34;项目经历&#34;&gt;&lt;/a&gt;项目经历&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;生物大语言模型集成平台	2024.10 - 2024.12	论文转化成果网站（后端开发成员）&lt;ul&gt;
&lt;li&gt;项目描述：生物语言模型集成平台是一个面向生物医学研究领域的工具类网站，旨在整合实验室研发的多种大语言模型，为研究人员提供便捷的模型调用和数据分析服务，并且还提供实验室研究成果展示等功能。本人负责部分模块后端开发，同时负责工作分配以及把控进度。&lt;/li&gt;
&lt;li&gt;项目技术栈：SpringCloud + SpringBoot + Mybatis&lt;/li&gt;
&lt;li&gt;服务拆分：基于跨语言与团队成员擅长技术的需求，将平台拆分为网关、模型处理、数据管理服务三个独立模块，并结合Nacos注册中心、Feign远程调用技术&lt;/li&gt;
&lt;li&gt;登录与用户管理：利用Spring Cloud Gateway接口，解析和验证JWT令牌，并传递用户信息至下游服务，实现服务间用户信息共享。对用户密码采取BCrypt密码加密方式，有效保障用户账号安全&lt;/li&gt;
&lt;li&gt;代码重构：分析所负责项目中多个相似的查询请求，通过自动化Mapper接口与实体类的映射，同时结合动态构建查询条件，实现了通用查询框架，提高了代码的复用性，减少了至少7个Mapper接口编码工作&lt;/li&gt;
&lt;li&gt;异步执行与存储优化：将模型调用的同步执行操作，结合Mq技术，转化为异步操作，实现平均响应时间从2000ms+降低至300ms内，并基于MinIO部署专用文件存储服务器，实现文件转存，减少本地服务器压力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;异构数据源同步平台      2023.10 - 2023.12     后端开发&lt;/li&gt;
&lt;li&gt;项目描述：该系统是一个基于Flink的异构数据源流转服务，用来作为数据源之间的数据同步工具，通过抽象异构数据源驱动，同时屏蔽数据源间不同的通信协议，通过页面化配置数据同步任务的方式简化数据库同步流程，并实现定时调度。&lt;/li&gt;
&lt;li&gt;项目技术栈：Flink + Chunjun + Xxl-Job + Redis + Retrofit2&lt;/li&gt;
&lt;li&gt;数据源Driver设计：使用工厂设计模式、模板设计模式抽象化数据源的交互逻辑，同时支持灵活扩展数据源&lt;/li&gt;
&lt;li&gt;流转进度推送：基于Redis stream搭建轻量级消息队列，同时结合SSE实例实现任务流转进度即时刷新&lt;/li&gt;
&lt;li&gt;任务调度：借助Xxl-Job实现任务调度管理，确保异构数据源的定时同步和实时更新需求，支持批量任务管理&lt;/li&gt;
&lt;li&gt;数据同步：使用钩子方法覆盖流转任务完整生命周期，并定义模板方法以高度可扩展的方式串联任务生命周期&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;专业技能&#34;&gt;&lt;a href=&#34;#专业技能&#34; class=&#34;headerlink&#34; title=&#34;专业技能&#34;&gt;&lt;/a&gt;专业技能&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;熟悉Java基础编程，具有良好的面向对象编程思想、熟悉多线程、集合等知识&lt;/li&gt;
&lt;li&gt;熟悉SpringBoot、SpringCloud、Mybatis等开发框架，了解微服务架构以及Nacos、Gateway等组件的使用&lt;/li&gt;
&lt;li&gt;熟悉Mysql数据库的使用，对索引、锁机制、事务、日志、数据库范式有一定理解&lt;/li&gt;
&lt;li&gt;熟悉Redis的使用，熟悉五种常见数据类型，对缓存持久化、缓存穿透、缓存击穿有一定理解&lt;/li&gt;
&lt;li&gt;熟悉Linux系统以及常用命令的使用，对Docker、Kubernetes有一定的理解&lt;/li&gt;
&lt;li&gt;熟悉计算机网络原理，对OSI七层模型、TCP&amp;#x2F;UDP、HTTP&amp;#x2F;HTTPS协议有一定理解&lt;/li&gt;
&lt;li&gt;熟悉操作系统基础知识，对进程调度、内存管理、虚拟内存等有一定理解&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2025/05/11/MyFirst-blog/</guid>
            <title>MyFirst blog</title>
            <link>http://example.com/2025/05/11/MyFirst-blog/</link>
            <pubDate>Sun, 11 May 2025 14:17:00 +0800</pubDate>
            <description><![CDATA[  ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2025/05/11/hello-world/</guid>
            <title>Hello World</title>
            <link>http://example.com/2025/05/11/hello-world/</link>
            <pubDate>Sun, 11 May 2025 12:39:40 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;Welcome to &lt;a href=&#34;https://hexo.io/&#34;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&#34;https://hexo.io/docs/&#34;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&#34;https://hexo.io/docs/troubleshooting.html&#34;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&#34;https://github.com/hexojs/hexo/issues&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;Quick-Start&#34;&gt;&lt;a href=&#34;#Quick-Start&#34; class=&#34;headerlink&#34; title=&#34;Quick Start&#34;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&#34;Create-a-new-post&#34;&gt;&lt;a href=&#34;#Create-a-new-post&#34; class=&#34;headerlink&#34; title=&#34;Create a new post&#34;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo new &lt;span class=&#34;string&#34;&gt;&amp;quot;My New Post&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;More info: &lt;a href=&#34;https://hexo.io/docs/writing.html&#34;&gt;Writing&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;Run-server&#34;&gt;&lt;a href=&#34;#Run-server&#34; class=&#34;headerlink&#34; title=&#34;Run server&#34;&gt;&lt;/a&gt;Run server&lt;/h3&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;More info: &lt;a href=&#34;https://hexo.io/docs/server.html&#34;&gt;Server&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;Generate-static-files&#34;&gt;&lt;a href=&#34;#Generate-static-files&#34; class=&#34;headerlink&#34; title=&#34;Generate static files&#34;&gt;&lt;/a&gt;Generate static files&lt;/h3&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo generate&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;More info: &lt;a href=&#34;https://hexo.io/docs/generating.html&#34;&gt;Generating&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;Deploy-to-remote-sites&#34;&gt;&lt;a href=&#34;#Deploy-to-remote-sites&#34; class=&#34;headerlink&#34; title=&#34;Deploy to remote sites&#34;&gt;&lt;/a&gt;Deploy to remote sites&lt;/h3&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo deploy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;More info: &lt;a href=&#34;https://hexo.io/docs/one-command-deployment.html&#34;&gt;Deployment&lt;/a&gt;&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
