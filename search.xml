<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>mentorRecommend</title>
      <link href="/2025/05/12/mentorRecommend/"/>
      <url>/2025/05/12/mentorRecommend/</url>
      
        <content type="html"><![CDATA[<h1 id="下面内容仅适合混子的想读博没必要看读博随便都行"><a class="markdownIt-Anchor" href="#下面内容仅适合混子的想读博没必要看读博随便都行">#</a> 下面内容仅适合混子的，想读博没必要看，读博随便都行</h1><h2 id="szu实验室导师推荐以下只列出熟悉的老师"><a class="markdownIt-Anchor" href="#szu实验室导师推荐以下只列出熟悉的老师">#</a> SZU 实验室导师推荐（以下只列出熟悉的老师）</h2><ul><li>李雪亮（确实是很好）</li><li>罗成文（有横向，但是佛系一点，从他的学生口里知道的）</li><li>潘颖慧（上课的时候确实人好，从面相看不错，实际不太清楚）</li></ul><h2 id="szu实验室导师不推荐"><a class="markdownIt-Anchor" href="#szu实验室导师不推荐">#</a> SZU 实验室导师不推荐</h2><ul><li><p>交叉所某几个导师，H 学校和 J 学校毕业的，我不知道为什么小红书会有那么多人推荐的，想读博可以去，但是不想读真没必要，能够骂哭人的 mentor 是一个好 mentor 吗？换句话说，能来 szu 读硕士的，mentor 自己心里没有点 B 数是怎么回事？闭口发论文，开口发论文，两眼一睁就是发论文，几百块钱的补贴猛猛干，心口不一？在那随随便便，一出口就是中心，一出口就是实验室？倒不如自由点，想读博的你们就疯狂 Pua 他们，不想读的能毕业就好，这样对我们，难道不怕我们在论文里面 P 几个数据吗？自己的 idea 都已经穷尽了，也就只能指导一下怎么写毕业论文，知道为什么吗？因为你们以为从普本跳到了博士，或者是去海外转了一圈，就可以摇身一变变成了一个牛人？不可能的，你们也就只有指导我写毕业论文的经验了，知道吗？</p></li><li><p>工业所可以多去了解</p></li><li><p>剩下的所，自己慢慢 “领悟”，想去的可以去努力的吃苦</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> szu_mentors </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mentor </tag>
            
            <tag> szu_bdsc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLM-concept</title>
      <link href="/2025/05/12/LLM-concept/"/>
      <url>/2025/05/12/LLM-concept/</url>
      
        <content type="html"><![CDATA[<h1 id="大模型基本概念"><a class="markdownIt-Anchor" href="#大模型基本概念">#</a> 大模型基本概念</h1><h2 id="目标"><a class="markdownIt-Anchor" href="#目标">#</a> 目标</h2><ul><li><p>语言模型就是对自然语言的概率分布进行建模，即 P (w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p></li><li><p>计算下一个词的概率 P (wn | w1 w2 w3… wn-1)</p><p><img src="image1.png" alt="image"></p></li></ul><h2 id="发展历程"><a class="markdownIt-Anchor" href="#发展历程">#</a> 发展历程</h2><p>从 n-gram:</p><p><img src="image2.png" alt="image"></p><p>到 neural language model: 每个词都映射成一个低维向量</p><p><img src="image3.png" alt="image"></p><p>再到后面的 transformer 出现，transformer 的出现，NLP 进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是 PLM。</p><p>随着 OpenAI 发布的 1750 亿个参数（GPT-3），开启 LLM 时代</p><h2 id="问题发现"><a class="markdownIt-Anchor" href="#问题发现">#</a> 问题发现</h2><p>・大模型（如 GPT-3）参数量极大（1750 亿 +），传统 “预训练 + 微调” 范式成本过高（需为每个任务调整海量参数）。</p><ol><li><p>解决方案：<br>・开发新范式（ICL/Prompt），通过输入指令或示例直接引导模型，避免微调。</p><p>・但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p></li><li><p>模型构建的关键：<br>・预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握 ICL/Prompt 所需的能力（如任务模式识别、指令遵循）。</p><p>・后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p></li><li><p>结论：<br>・新范式（ICL/Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p></li></ol><h2 id="llm的构建流程"><a class="markdownIt-Anchor" href="#llm的构建流程">#</a> LLM 的构建流程</h2><ul><li>预训练： 利用海量训练数据构建多样化内容，构建基础模型 ——&gt; 对长文本建模，使模型具有语言生成能力</li><li>有监督微调 SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li><li>奖励建模 RM：训练一个能够判断文本质量的裁判，对同个提示词，比较 SFT 生成的多个输出的质量</li><li>强化学习 RLHF (human feedback)：基于 RM，优化 SFT 模型</li></ul><p>SFT 相当于学生学会答题，RM 是评分老师，判断 answer 好坏，RLHF 是学生根据老师评分改进答题策略</p><h2 id="补充"><a class="markdownIt-Anchor" href="#补充">#</a> 补充</h2><p><strong>N-gram 模型详解</strong><br> N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1 个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p><p>N-gram 的定义：</p><p>・指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p><p>・例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class="line"></span><br><span class="line">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class="line"></span><br><span class="line">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure><p>・核心假设：</p><p>・马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure><hr><p><strong>如何计算概率？</strong><br>N-gram 通过统计语料库中词序列的频率来估计概率：</p><p>计算  <code>P(sat | the cat)</code> ：</p><p>P(sat∣the cat)=Count(“the cat”)Count(“the cat sat”)</p><p>若语料中 “the cat” 出现 100 次，“the cat sat” 出现 30 次，则  <code>P(sat | the cat) = 0.3</code> 。</p><p><strong>N-gram 的优缺点</strong></p><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>简单高效，计算速度快。</td><td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td></tr><tr><td>小规模数据即可训练。</td><td>数据稀疏性（罕见 n-gram 概率不准确）。</td></tr><tr><td>曾广泛用于机器翻译、拼写检查等任务。</td><td>无法理解语义（仅统计共现频率）。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>resume</title>
      <link href="/2025/05/11/resume/"/>
      <url>/2025/05/11/resume/</url>
      
        <content type="html"><![CDATA[<h1 id="huanchaofeng-resume"><a class="markdownIt-Anchor" href="#huanchaofeng-resume">#</a> HuanchaoFeng Resume</h1><h2 id="教育经历"><a class="markdownIt-Anchor" href="#教育经历">#</a> 教育经历</h2><ul><li>深圳大学（硕士） 2024-2027 计算机技术 硕士一等奖学金</li><li>广东财经大学（本科） 2020-2024 计算机科学与技术 学业奖学金  CET-6</li></ul><h2 id="实习经历"><a class="markdownIt-Anchor" href="#实习经历">#</a> 实习经历</h2><ul><li><p>深圳迅策科技股份有限公司     后端研发实习生 技术支持中心 — 政府项目组</p><p>后端研发实习生 技术支持中心 — 政府项目组</p><ul><li>工作描述：负责智慧交通平台道路信息服务的后端研发</li><li>参与问题定位开发：使用 AOP 并结合自定义注解获取全局请求与处理信息，增强 API 的可追踪性和调试效率，通过此配置团队成员可以快速定位问题和分析系统行为，提高开发效率</li><li>数据表设计与查询优化：独立完成交通流量、违法信息、道路信息等 5 个模块的数据设计，包含数据表设计、字段抽象与设计，同时在百万级数据的交通流量表设计上建立索引，将平均回表次数从 160w 次优化为 200 次</li><li>道路模块开发：负责相关需求开发，并基于内部 CI/CD 平台搭建自动化测试流水线，保证在提交测试前各接口单元测试覆盖率达到 70% 以上，核心链路全覆盖</li></ul></li></ul><h2 id="项目经历"><a class="markdownIt-Anchor" href="#项目经历">#</a> 项目经历</h2><ul><li>生物大语言模型集成平台2024.10 - 2024.12论文转化成果网站（后端开发成员）<ul><li>项目描述：生物语言模型集成平台是一个面向生物医学研究领域的工具类网站，旨在整合实验室研发的多种大语言模型，为研究人员提供便捷的模型调用和数据分析服务，并且还提供实验室研究成果展示等功能。本人负责部分模块后端开发，同时负责工作分配以及把控进度。</li><li>项目技术栈：SpringCloud + SpringBoot + Mybatis</li><li>服务拆分：基于跨语言与团队成员擅长技术的需求，将平台拆分为网关、模型处理、数据管理服务三个独立模块，并结合 Nacos 注册中心、Feign 远程调用技术</li><li>登录与用户管理：利用 Spring Cloud Gateway 接口，解析和验证 JWT 令牌，并传递用户信息至下游服务，实现服务间用户信息共享。对用户密码采取 BCrypt 密码加密方式，有效保障用户账号安全</li><li>代码重构：分析所负责项目中多个相似的查询请求，通过自动化 Mapper 接口与实体类的映射，同时结合动态构建查询条件，实现了通用查询框架，提高了代码的复用性，减少了至少 7 个 Mapper 接口编码工作</li><li>异步执行与存储优化：将模型调用的同步执行操作，结合 Mq 技术，转化为异步操作，实现平均响应时间从 2000ms + 降低至 300ms 内，并基于 MinIO 部署专用文件存储服务器，实现文件转存，减少本地服务器压力</li></ul></li><li>异构数据源同步平台      2023.10 - 2023.12     后端开发</li><li>项目描述：该系统是一个基于 Flink 的异构数据源流转服务，用来作为数据源之间的数据同步工具，通过抽象异构数据源驱动，同时屏蔽数据源间不同的通信协议，通过页面化配置数据同步任务的方式简化数据库同步流程，并实现定时调度。</li><li>项目技术栈：Flink + Chunjun + Xxl-Job + Redis + Retrofit2</li><li>数据源 Driver 设计：使用工厂设计模式、模板设计模式抽象化数据源的交互逻辑，同时支持灵活扩展数据源</li><li>流转进度推送：基于 Redis stream 搭建轻量级消息队列，同时结合 SSE 实例实现任务流转进度即时刷新</li><li>任务调度：借助 Xxl-Job 实现任务调度管理，确保异构数据源的定时同步和实时更新需求，支持批量任务管理</li><li>数据同步：使用钩子方法覆盖流转任务完整生命周期，并定义模板方法以高度可扩展的方式串联任务生命周期</li></ul><h2 id="专业技能"><a class="markdownIt-Anchor" href="#专业技能">#</a> 专业技能</h2><ul><li>熟悉 Java 基础编程，具有良好的面向对象编程思想、熟悉多线程、集合等知识</li><li>熟悉 SpringBoot、SpringCloud、Mybatis 等开发框架，了解微服务架构以及 Nacos、Gateway 等组件的使用</li><li>熟悉 Mysql 数据库的使用，对索引、锁机制、事务、日志、数据库范式有一定理解</li><li>熟悉 Redis 的使用，熟悉五种常见数据类型，对缓存持久化、缓存穿透、缓存击穿有一定理解</li><li>熟悉 Linux 系统以及常用命令的使用，对 Docker、Kubernetes 有一定的理解</li><li>熟悉计算机网络原理，对 OSI 七层模型、TCP/UDP、HTTP/HTTPS 协议有一定理解</li><li>熟悉操作系统基础知识，对进程调度、内存管理、虚拟内存等有一定理解</li></ul>]]></content>
      
      
      <categories>
          
          <category> information </category>
          
      </categories>
      
      
        <tags>
            
            <tag> resume </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
