<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Datalinkx模块讲解1</title>
      <link href="/2025/11/25/Datalinkx%E6%A8%A1%E5%9D%97%E8%AE%B2%E8%A7%A31/"/>
      <url>/2025/11/25/Datalinkx%E6%A8%A1%E5%9D%97%E8%AE%B2%E8%A7%A31/</url>
      
        <content type="html"><![CDATA[<h2 id="datalinkx-messagehub与datalinkx-sse"><a class="markdownIt-Anchor" href="#datalinkx-messagehub与datalinkx-sse">#</a> Datalinkx-messageHub 与 Datalinkx-SSE</h2><ul><li>用于 datalinkx-job 和 datalinkx-server 之间的异步通信，这个模块只有在推送流转进度中才使用，job 是生成者，检测流转任务的进度数据后，调用生产者 api 把数据发送到 redis 中，datlainkx-sse 是消费者，注册一个常驻代理线程，实时监控 redis 中的数据，监听到后，回调到标记了 @MessageHub 注解的方法中</li></ul><h3 id="redis-stream结构作为消息队列"><a class="markdownIt-Anchor" href="#redis-stream结构作为消息队列">#</a> Redis stream 结构作为消息队列</h3><ul><li>一条消息只能由一个消费者组中的一个消费者消费。</li></ul><h3 id="topicreloadtask类"><a class="markdownIt-Anchor" href="#topicreloadtask类">#</a> TopicReloadTask 类</h3><ul><li>是一个定时任务，用来周期性从数据库中读取所有 topic，并把他们更新到 redis 的白名单集合中，更新时采用删除原来的名单，再写入新的名单（lua 脚本控制原子性，避免删除后失败，未写入新名单）<br>用于几个地方：发送端检查是否有该 topic、消费端是否订阅了存在的 topic。实际上本项目只需要用 1 个 topic 即可</li></ul><h3 id="topicdaemonwarden类"><a class="markdownIt-Anchor" href="#topicdaemonwarden类">#</a> TopicDaemonWarden 类</h3><ul><li>用于注册定时执行器 Timer 并周期性调度任务</li></ul><h3 id="service包"><a class="markdownIt-Anchor" href="#service包">#</a> Service 包</h3><ul><li>主要是 redis 包里面的 stream 类：封装了消息队列的发送和接收，提供给数据流转时的调用，其中发送是数据流转的时候推送进消息队列中，而接收这个消费者动作，是一开始就初始化了一个常驻线程去监控 redis 数据（ConsumerConfig 类负责发现消费者方法【带有 messageHub 注解的方法，都在 SSE 模块下】+ 注册消费者），RedisStreamProcessor 负责真正的订阅 stream + 启动线程持续消费消息。</li></ul><h3 id="datalinkx-sse模块"><a class="markdownIt-Anchor" href="#datalinkx-sse模块">#</a> Datalinkx-sse 模块</h3><ul><li>这一块主要是用于维护 SSE 的连接、传输，与 messagehub 连接紧密，因为 messagehub 模块是用于把消息生产到 topic 中，然后 SSE 中的三个消费者（已在 messagehub 中注册为消费者）把消息读取，并通过 SSE 途径推送到前端。这个 SSE 连接，是用户向 server 模块的控制层请求建立的。这一块 SSE 监督的是新更新的信息，但是后端只会把信息推送到前端，前端负责是要更新哪个任务的状态，在任务结束时，后端会写入数据库。</li></ul><h2 id="datalinkx-client模块"><a class="markdownIt-Anchor" href="#datalinkx-client模块">#</a> Datalinkx-client 模块</h2><ul><li>主要功能是为项目的每个模块提供远程调用功能，主要是通过 retrofit2 进行远程调用，包括对 flink、datalinkxjob 模块、seatunnel 进行调用。</li></ul><h3 id="对flink模块的调用"><a class="markdownIt-Anchor" href="#对flink模块的调用">#</a> 对 flink 模块的调用</h3><p>通过 HTTP 调用 Flink 的 API，获取作业状态、作业指标、异常信息以及停止作业等操作。对于 client 的返回值，做了一个统一的封装处理，使用了 JsonNode 形式作为响应数据，会以 json 形式返回，类似：<br>{<br>“user-task-accumulators”: [<br>{<br>“name”: “numRecordsOut”,<br>“type”: “Long”,<br>“value”: “123456”<br>},<br>{<br>“name”: “numRecordsIn”,<br>“type”: “Long”,<br>“value”: “120000”<br>}<br>]<br>}<br> 那么你如果有多种返回数据形式时，定义好 response 类，然后加上这个标志：<br>@Data<br>public class FlinkJobAccumulators {</p><pre><code>@JsonProperty(&quot;user-task-accumulators&quot;)List&lt;UserTaskAccumulator&gt; userTaskAccumulators;@Datapublic static class UserTaskAccumulator &#123;    private String name;    private String type;    private String value;&#125;</code></pre><p>}<br> 他就会自动封装到你这个类型中了<br>但是，可以直接不用统一的返回值，编写远程调用的时候，返回值直接用自己封装的类</p><h3 id="向外暴露一个datalinkxclientutils类"><a class="markdownIt-Anchor" href="#向外暴露一个datalinkxclientutils类">#</a> 向外暴露一个 DatalinkXClientUtils 类</h3><ul><li>用于创建远程服务客户端的动态代理 + 注入 Spring 容器，供其他类 使用。<br>因为目前定义 Retrofit 客户端接口（ FlinkClient、DatalinkXJobClient）本身不具备逻辑，它们需要 Retrofit 动态代理才能工作，所以必须主动创建并注册为 Bean。所以同个 utils 这个类，来创建实例。</li><li>其他模块如果想要用某个远程接口，那么就调用这个 utils，会把 client 进行动态代理（封装请求之类的操作，因为之前我们只是定义了接口，并不是属于 retrofit 的操作，进行动态代理封装之后，才能够进行请求操作），然后返回实例给调用方。</li></ul><h2 id="common模块"><a class="markdownIt-Anchor" href="#common模块">#</a> Common 模块</h2><ul><li>这个模块主要放一些静态变量、异常处理、统一响应结果以及工具类</li></ul><h2 id="connect模块"><a class="markdownIt-Anchor" href="#connect模块">#</a> Connect 模块</h2><ul><li>这个模块用于数据库的连接、构建读取、写入信息。首先按照数据源的类型分为 JDBC 和非 JDBC。比如 JDBC 类定义了加载数据库驱动程序、创建连接对象、获取数据库操作对象、执行查询、释放资源、构建 JDBC source（读取）或者 sink（写入）的操作。然后 mysql /oracle 属于 JDBC 类型，就继承 JDBC，把不同的地方进行重写即可。这里只是按照模板模式的方法，先定义了父类，子类只需要实现不同点就行，如果觉得麻烦，可以自己编写每个数据源的读取、写入的方法，然后给数据流转类调用即可。</li><li>对于读取、写入操作：是需要根据全量、增量、数据表信息、字段等信息，从数据库读取 / 写入此操作，这里封装的是提交给 flinkx 的 Unit 单元，最终会形成这样：<br>{<br>“job”: {<br>“content”: [<br>{<br>“reader”: {<br>“name”: “mysqlreader”,<br>“parameter”: {<br>“connection”: […],<br>“column”: […],<br>“where”: “id &gt; 100”,<br>“username”: “…”,<br>“password”: “…”<br>}<br>},<br>“writer”: {<br>“name”: “mysqlwriter”,<br>“parameter”: {<br>“connection”: […],<br>“column”: […],<br>“writeMode”: “insert”<br>}<br>}<br>}<br>]<br>}<br>}<br> 然后提交给 flinkx 去执行</li></ul><p>对于如何获取这个 Driver 类，DsDriverFactory 是一个静态工厂类，提供给外界创建 Driver 的实例，外界（数据流转时）会给一个 connectId 给工厂类，根据这个 id 解析出属于哪一个类型，然后把类（MysqlDriver / OracleDriver 等）实例化出来返回给数据流转操作，然后就会根据这个类去获取读取源以及写入源。</p><p>所以这个模块看起来多，其实内容就两个：1、定义一个数据库实例； 2、向外开放一个工厂类，返回实例给外界</p><p>新增数据源：新增一个包（与 Driver 同层）然后实现读写即可，因为加载类的时候，会根据包路径来加载，把包放在可以加载到的地方即可。</p>]]></content>
      
      
      <categories>
          
          <category> datalinkx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> datalinkx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BIBM paper: CFPLM</title>
      <link href="/2025/11/11/BIBM-paper-CFPLM/"/>
      <url>/2025/11/11/BIBM-paper-CFPLM/</url>
      
        <content type="html"><![CDATA[<h1 id="cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models"><a class="markdownIt-Anchor" href="#cfplm-improve-protein-rna-interaction-prediction-with-a-collaborative-framework-powered-by-language-models">#</a> CFPLM: Improve protein-RNA interaction prediction with a collaborative framework powered by language models</h1><h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract">#</a> Abstract</h2><p>Protein-RNA interactions (PRIs) play pivotal roles in biological processes such as gene regulation, making their prediction essential for therapeutic and mechanistic studies. While traditional wet-lab methods are time-consuming and challenging, computational approaches offer efficient alternatives. Graph-based methods show promise by capturing both direct interactive domains (protein-RNA interaction) and indirect collaborative domains (functional similarity among proteins/RNAs). However, integrating these domains and learning meaningful node representations remain critical challenges. To address this, we propose ​​CFPLM​​, a collaborative framework fusing large language models, graph convolutional networks, and cross-attention mechanisms to improve PRI prediction. Experiment results demonstrate that CFPLM achieves robust, state-of-the-art performance across three benchmark datasets. It’s anticipated to have applicability to similar other interaction prediction tasks.</p><h2 id="iintroduction"><a class="markdownIt-Anchor" href="#iintroduction">#</a> Ⅰ.Introduction</h2><p>As two essential biological macromolecules, the interaction between proteins and RNA plays a crucial role in cellular activities, such as gene expression regulation, translation control, and viral replication [1].Traditional biological experiments can identify direct interactions between RNA and protein with confidence. However, they are complicated and challenging, making them expensive to verify each potential interaction individually [2].<br>In response to these challenges, researchers are encouraged to develop more efficient and economical computational methods to aid in screening and predicting potential interactions, thereby enhancing research efficiency and productivity. Among the various calculation methods currently available, the machine learning-based method is the most widely used in this field. These methods can be divided into two groups: one involves feature learning and classification, and the other is graph-based association prediction. The former extracts features based on protein and RNA sequences or structures, and constructs a classification model based on these features to determine whether a given protein can interact with an RNA. For example, EnANNDeep combines an adaptive K-nearest neighbor classifier, a deep neural network, and a deep forest model, and integrates the prediction results of the three models using a weighted average to improve classification performance [3]. LPIDF utilizes self-encoder technology to extract sequence features of lncRNA and protein, and combines an ensemble learning method to predict possible interactions [4]. In MHAM, a deep learning model based on a multi-head attention mechanism and a residual link is used to predict the interaction between RNA and protein [5]. LGFC-CNN combines the original sequence, artificial, and structural features to predict PRI through deep learning [6]. LPI-CNNCP uses convolutional neural networks and replication filling techniques to predict the interaction between lncRNA and protein [7]. AptaNet [8] and AptaTrans [9] employ deep neural networks and transformer-based encoders, respectively, to capture the complex interactions between proteins and nucleic acid aptamers. The advantage of this type of method is that it can predict the potential interaction between any given protein and RNA. However, the interaction patterns between proteins and RNA are complex, and currently known data on protein-RNA interactions is limited, making it difficult to train a reliable end-to-end prediction model. Therefore, existing methods are difficult to generalize excellent predictive performance to different datasets.<br>Graph-based methods have emerged as particularly promising approaches. Because it can better utilize existing knowledge through molecular similarity networks and interaction networks. For example, LPICGAE is a combination method based on a graph self-encoder, which is used to predict PRI [10]. BiHo-GNN is the first method to integrate isomorphic and heterogeneous networks through bipartite graph embedding to predict PRI [11]. NPI-GNN was proposed for predicting noncoding RNA–protein interactions using graph neural networks[12]. CCGNN proposed a new method to predict PRI by incorporating information from the interaction domain and cooperation domain [13]. From the above research, it is evident that collaborative domain construction and node representation learning are key factors in determining this method. However, when constructing collaborative domains and node representation, most existing graph-based methods fail to fully capture the complex information in RNA and protein sequences, which results in limited performance of the finally trained models.<br>In recent years, the breakthrough progress of large language models (LLMs) has transformed various aspects of life, including scientific research paradigms. Currently, numerous large language models have been trained on RNA and protein sequences, such as EVO [14], BIRNA-BERT [15], RNA-FM [16], ESM [17], and ProteinBERT [18], which have been widely utilized in biological tasks, including interaction prediction, and have achieved notable success. For example, PAIR used an RNA-FM language model to get the embedding vector of nucleic acid aptamer, and performed well in protein-nucleic acid aptamer interaction task [19]; In DTI-LM [20], ChemBERTa [21] and ESM language models are used to generate embedding vectors for drug molecules and protein, and show good performance in their prediction tasks . These studies demonstrate that LLMs have the potential to offer a more powerful approach to constructing collaborative domains and characterizing the nodes. However, there is currently a lack of research to explore this point. Moreover, most graph-based methods operate on information from the collaborative domain and interaction information within RNA or protein separately, thereby ignoring some useful cross-information and dependence between RNA and protein. How to better combine LLMs with other technologies to address the challenges in PRI prediction remains to be explored.<br>To address these gaps, we propose a novel framework, named CFPLM, for identifying protein-RNA interactions that utilizes a cross-attention fusion neural network in conjunction with large language models. The contributions of this study are as follows:</p><ul><li>(1) We introduce a new strategy for constructing collaborative domains based on both protein and RNA language models, which provides a more reliable method for knowledge construction in graph-based approaches.</li><li>(2) We design a new learning model for protein-RNA interactions by combining a graph convolutional neural network (GCN) with a cross-attention mechanism, thereby overcoming the limitations of existing graph learning models in this field.<br>The experimental results demonstrate the proposed framework is more efficient and accurate for predicting protein-RNA interactions, offering valuable insights into the interaction mechanisms between RNA and protein, as well as related biological processes.</li></ul><h2 id="iimaterial-and-method"><a class="markdownIt-Anchor" href="#iimaterial-and-method">#</a> II.Material and Method</h2><ul><li>A.Datasets<br>The benchmark dataset used in this study was constructed based on the protein-RNA complex released from the PDB (Protein Data Bank) database between 1989 and 2023. We labeled a protein chain interacts with an RNA according to the spatial distance between in the complex. According to the previous research [22], the distance threshold is set to 3.5 angstroms (protein-RNA pairs with spatial distance less than 3.5 angstroms are interacted). To reduce data redundancy and ensure objective experimental results, we used BLAST to cluster proteins and RNAs, respectively, based on sequence similarity (with threshold S=90). We then randomly select a sequence from each cluster to build a benchmark database. To avoid noise caused by incomplete protein fragments, we filter out protein sequences with lengths less than 50. Finally, 2,465 RNA sequences, 5,897 protein sequences, and 8,579 pairs of interaction information were obtained. According to previous work [23], we adopt the following protocol for processing the dataset: randomly sampling the same number of protein-RNA pairs that are not interactions as negative samples.<br>For a comprehensive evaluation, we also employed another two datasets: miRTarBase (version 8.0) [24] and a protein-lncRNA dataset [10]. There are 3,924 miRNAs, 20,992 target genes, and 186,416 verified interactions in miRTarBase. While 3,046 lncRNAs, 136 proteins, and 8,112 interactions in the protein-lncRNA dataset.</li><li>B.Framework of CFPLM<br>CFPLM includes three modules (Fig. 1): 1) Feature extraction, which uses large language models to extract initial feature vectors of RNA and protein; 2) Graph structure, which uses cosine similarity to calculate the similarity of RNA-RNA and protein-protein, and then obtains the similarity graph, and at the same time transforms the RNA-protein interaction pair into an interaction graph; 3) Feature optimization, which uses graph convolution network to capture the complex features of RNA and protein, and uses cross attention to fuse the feature vectors of similarity domain and interaction domain. The first and second points are shown in Fig. 1 (a), and the third point is shown in Fig. 1 (b-c).</li><li>(1)Feature Extraction<ul><li>a)RNA Encoder: BIRNA-BERT is an RNA language model based on Transformer architecture, which is characterized by adopting a double labeling scheme, combining nucleotide-level labeling and byte-pair coding labeling, and can dynamically adjust the labeling strategy according to the sequence length and computing resources, so that the model can handle long-sequence tasks while retaining the ability to handle short-sequence tasks using nucleotide-level information. RNA-FM, the basic RNA model based on self-supervised learning, has been proven effective in multiple tasks for the first time. Another example is the latest Evo, which demonstrates the powerful ability of genome language models in function-oriented design. Regarding the choice of language model, we will compare the above language models in the results section. Ultimately, we chose to use the BIRNA-BERT model based on its ease of use, as it can be well adapted to the task and saves computing resources and experimental time.</li><li>b)Protein Encoder: ESM-2 is a large language model for protein sequences. The model can learn the evolution pattern in protein sequences by training on the modeling target of masked language and transform it into a deep understanding of protein structure. ProteinBERT is also a language model specially designed for protein sequences. ProteinBERT enhances the efficiency and performance of protein sequence analysis by integrating a novel pre-training task that combines language modeling and gene ontology annotation prediction. The model has performed well in several protein task benchmarks, approaching or exceeding the performance of existing large-scale models [18]. We compared the above language models and finally adopt the ESM-2 model. In this study, we employ a protein language model based on the ESM-2 architecture with 150M parameters, and train it on the UniRef50 dataset [25].</li></ul></li><li>(2)Graph Construction<br>In DTI-LM, the feature expression is improved by calculating the similarity protein-protein and drug. The similarity function is used in CCGNN to construct the similarity map between protein and RNA. FMSRT constructs multi-source similarity map according to similarity [26] . The success of these studies shows that similarity information can provide rich information for the study of biomolecular interaction. In this study, we will use the cosine similarity calculation method to calculate the similarity of RNA feature vectors and protein feature vectors respectively, and convert them into RNA-RNA similarity graph and Protein-Protein similarity graph. At the same time, NPI-GNN model uses the bipartite graph of ncRNA-protein to infer the interaction [12], and GANLDA model uses the bipartite graph of lncRNA and diseases to predict the interaction [27]. These methods combine the topological information of PRI networks, and effectively improve the performance of the model in link prediction tasks. In this study, we construct an RNA-protein interaction bipartite graph for feature optimization in graph neural networks.</li><li>(3)Feature Optimization<ul><li>a)Graph convolution network: GCN is a spectral-based graph neural network that processes graph-structured data by aggregating neighborhood information to learn low-dimensional node representations. It efficiently preserves local structure while supporting tasks like node classification and link prediction.  The RNA feature matrix and protein feature matrix will enter the graph convolution network with three graphs, and finally get the updated node feature vector.</li><li>b)Cross Attention: To better capture the interaction and similarity information between RNA and proteins, a cross-attention mechanism is introduced. Its core lies in dynamically learning the interactions between the two through the attention mechanism, so a cross-attention module is designed in the model to handle such information. This module contains 4 attention heads, each with a feature dimension of 64. Fig. 1 © illustrates the calculation process of the cross-attention module, where, for the RNA and protein feature vector matrices obtained from the interaction graph, RNA serves as the Query and protein acts as the Key and Value, and vice versa. Meanwhile, the RNA and protein matrices derived from the similarity graph are also optimized and updated using this method.</li><li>c)Loss Function: In this study, we use binary cross entropy as the loss function, calculating the average of all samples when calculating the loss.</li></ul></li></ul><p>FrameWork:<br><img src="image4.png" alt="image1"></p><h2 id="references"><a class="markdownIt-Anchor" href="#references">#</a> References</h2><p>[1]E. Jankowsky, and M. E. J. N. r. M. c. b. Harris, “Specificity and nonspecificity in RNA–protein interactions,” Nature Reviews Molecular Cell Biology, vol. 16, no. 9, pp. 533-544, 2015.<br>[2]M. Philip, T. Chen, and S. Tyagi, “A Survey of Current Resources to Study lncRNA-Protein Interactions,” Noncoding RNA, vol. 7, no. 2, pp. 33, Jun 8, 2021.<br>[3]L. Peng, J. Tan, X. Tian, and L. Zhou, “EnANNDeep: An Ensemble-based lncRNA-protein Interaction Prediction Framework with Adaptive k-Nearest Neighbor Classifier and Deep Models,” Interdisciplinary Sciences: Computational Life Sciences, vol. 14, no. 1, pp. 209-232, Mar, 2022.<br>[4]X. Tian, L. Shen, Z. Wang, L. Zhou, and L. Peng, “A novel lncRNA-protein interaction prediction method based on deep forest with cascade forest structure,” Scientific Reports, vol. 11, no. 1, pp. 18881, Sep 23, 2021.<br>[5]Z. Zhou, Z. Du, J. Wei, L. Zhuo, S. Pan, X. Fu, and X. Lian, “MHAM-NPI: Predicting ncRNA-protein interactions based on multi-head attention mechanism,” Computers in Biology and Medicine, vol. 163, pp. 107143, Sep, 2023.<br>[6]L. Huang, S. Q. Jiao, S. Yang, S. Q. Zhang, X. P. Zhu, R. Guo, and Y. Wang, “LGFC-CNN: Prediction of lncRNA-Protein Interactions by Using Multiple Types of Features through Deep Learning,” Genes, vol. 12, no. 11, pp. 1689, Nov, 2021.<br>[7]S. W. Zhang, X. X. Zhang, X. N. Fan, and W. N. Li, “LPI-CNNCP: Prediction of lncRNA-protein interactions by using convolutional neural network with the copy-padding trick,” Analytical Biochemistry, vol. 601, pp. 113767, Jul 15, 2020.<br>[8]N. Emami, and R. Ferdousi, “AptaNet as a deep learning approach for aptamer–protein interaction prediction,” Scientific reports, vol. 11, no. 1, pp. 6074, 2021.<br>[9]I. Shin, K. Kang, J. Kim, S. Sel, J. Choi, J.-W. Lee, H. Y. Kang, and G. Song, “AptaTrans: a deep neural network for predicting aptamer-protein interaction using pretrained encoders,” BMC Bioinformatics, vol. 24, no. 1, pp. 447, 2023/11/27, 2023.<br>[10]J. Zhao, J. Sun, S. C. Shuai, Q. Zhao, and J. Shuai, “Predicting potential interactions between lncRNAs and proteins via combined graph auto-encoder methods,” Briefings in Bioinformatics, vol. 24, no. 1, Jan 19, 2023.<br>[11]Y. Ma, H. Zhang, C. Jin, and C. Kang, “Predicting lncRNA-protein interactions with bipartite graph embedding and deep graph neural networks,” Frontiers in Genetics, vol. 14, pp. 1136672, 2023.<br>[12]Z.-A. Shen, T. Luo, Y.-K. Zhou, H. Yu, and P.-F. J. B. i. b. Du, “NPI-GNN: Predicting ncRNA–protein interactions with deep graph neural networks,” Briefings in Bioinformatics, vol. 22, no. 5, pp. bbab051, 2021.<br>[13]H. Li, B. Wu, M. Sun, Z. Zhu, K. Chen, and H. Ge, “Cross-domain contrastive graph neural network for lncRNA–protein interaction prediction,” Knowledge-Based Systems, vol. 296, 2024.<br>[14]E. Nguyen, M. Poli, M. G. Durrant, B. Kang, D. Katrekar, D. B. Li, L. J. Bartie, A. W. Thomas, S. H. King, G. Brixi, J. Sullivan, M. Y. Ng, A. Lewis, A. Lou, S. Ermon, S. A. Baccus, T. Hernandez-Boussard, C. Ré, P. D. Hsu, and B. L. Hie, “Sequence modeling and design from molecular to genome scale with Evo,” Science, vol. 386, no. 6723, pp. eado9336, 2024.<br>[15]M. T. Tahmid, H. S. Shahgir, S. Mahbub, Y. Dong, and M. S. J. b. Bayzid, “BiRNA-BERT allows efficient RNA language modeling with adaptive tokenization,” bioRxiv:2024.07.02.601703, 2024.<br>[16]J. Chen, Z. Hu, S. Sun, Q. Tan, Y. Wang, Q. Yu, L. Zong, L. Hong, J. Xiao, and T. J. a. p. a. Shen, “Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions,” arXiv preprint  arXiv:2204.00300, 2022.<br>[17]Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. Dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, “Evolutionary-scale prediction of atomic-level protein structure with a language model,” Science, vol. 379, no. 6637, pp. 1123-1130, Mar 17, 2023.<br>[18]N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial, “ProteinBERT: a universal deep-learning model of protein sequence and function,” Bioinformatics, vol. 38, no. 8, pp. 2102-2110, Apr 12, 2022.<br>[19]J. Zhang, Z. Yan, H. Zeng, and Z. Zhu, “PAIR: protein-aptamer interaction prediction based on language models and contrastive learning framework.” 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), Lisbon, Portugal, 2024, pp. 5426-5432<br>[20]K. T. Ahmed, M. I. Ansari, and W. J. B. Zhang, “DTI-LM: language model powered drug–target interaction prediction,” Bioinformatics, vol. 40, no. 9, pp. btae533, 2024.<br>[21]S. Chithrananda, G. Grand, and B. J. a. p. a. Ramsundar, “ChemBERTa: large-scale self-supervised pretraining for molecular property prediction,” arXiv preprint, arXiv:2010.09885, 2020.<br>[22]J. Yan, and L. Kurgan, “DRNApred, fast sequence-based method that accurately predicts and discriminates DNA- and RNA-binding residues,” Nucleic Acids Research, vol. 45, no. 10, pp. e84-e84, Jun 2, 2017.<br>[23]Y. Li, H. Sun, S. Feng, Q. Zhang, S. Han, and W. Du, “Capsule-LPI: a LncRNA-protein interaction predicting tool based on a capsule network,” BMC Bioinformatics, vol. 22, no. 1, pp. 246, May 13, 2021.<br>[24]H.-Y. Huang, Y.-C.-D. Lin, J. Li, K.-Y. Huang, S. Shrestha, H.-C. Hong, Y. Tang, Y.-G. Chen, C.-N. Jin, Y. Yu, J.-T. Xu, Y.-M. Li, X.-X. Cai, Z.-Y. Zhou, X.-H. Chen, Y.-Y. Pei, L. Hu, J.-J. Su, S.-D. Cui, F. Wang, Y.-Y. Xie, S.-Y. Ding, M.-F. Luo, C.-H. Chou, N.-W. Chang, K.-W. Chen, Y.-H. Cheng, X.-H. Wan, W.-L. Hsu, T.-Y. Lee, F.-X. Wei, and H.-D. Huang, “miRTarBase 2020: updates to the experimentally validated microRNA–target interaction database,” Nucleic Acids Research, vol. 48, no. D1, pp. D148-D154, 2019.<br>[25]B. E. Suzek, H. Huang, P. McGarvey, R. Mazumder, and C. H. Wu, “UniRef: comprehensive and non-redundant UniProt reference clusters,” Bioinformatics, vol. 23, no. 10, pp. 1282-8, May 15, 2007.<br>[26]X. Zhang, M. Liu, Z. Li, L. Zhuo, X. Fu, and Q. Zou, “Fusion of multi-source relationships and topology to infer lncRNA-protein interactions,” Molecular Therapy Nucleic Acids, vol. 35, no. 2, pp. 102187, Jun 11, 2024.<br>[27]W. Lan, X. M. Wu, Q. F. Chen, W. Peng, J. X. Wang, and Y. P. Chen, “GANLDA: Graph attention network for lncRNA-disease associations prediction,” Neurocomputing, vol. 469, pp. 384-393, Jan 16, 2022.<br>[28]Q. Le, and T. Mikolov, “Distributed representations of sentences and documents,” 2014 International Conference on Machine Learning, Beijing,China, 2014,pp. 1188-1196.<br>[29]S. Yang, Y. Wang, Y. Lin, D. Shao, K. He, and L. J. M. Huang, “LncMirNet: predicting LncRNA–miRNA interaction based on deep learning of ribonucleic acid sequences,” Molecules, vol. 25, no. 19, pp. 4372, 2020.<br>[30]Z. Wang, S. Liang, S. Liu, Z. Meng, J. Wang, and S. Liang, “Sequence pre-training-based graph neural network for predicting lncRNA-miRNA associations,” Briefings in Bioinformatics, vol. 24, no. 5, pp. bbad317, Sep 20, 2023.<br>[31]Y. Han, and S. W. Zhang, “ncRPI-LGAT: Prediction of ncRNA-protein interactions with line graph attention network framework,” Computational and Structural Biotechnology Journal, vol. 21, pp. 2286-2295, 2023.<br>[32]H. Ji, X. Wang, C. Shi, B. Wang, P. S. J. I. T. o. K. Yu, and D. Engineering, “Heterogeneous graph propagation network,” IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 521-532, 2021.<br>[33]L. v. d. Maaten, and G. J. J. o. m. l. r. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579-2605, 2008.</p>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLaMA&amp;混合专家模型</title>
      <link href="/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/"/>
      <url>/2025/05/19/LLaMA-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="llama-model-structure"><a class="markdownIt-Anchor" href="#llama-model-structure">#</a> LLaMA  model  structure</h2><p>​LLaMA 是基于 transformer 的 decoder 部分构建的，采用前置层归一化、使用 RMSNorm 规划函数，激活函数更改为 SwiGLU，使用旋转位置嵌入更改的 decoder 模型。更改的位置如下所示：</p><p><img src="image1.png" alt="image1"></p><ul><li><p>RMSNorm 函数：</p><p><img src="image2.png" alt="image2"></p><p>原始层归一化函数：</p><p><img src="image3.png" alt="image2"></p><table><thead><tr><th style="text-align:center"><strong>对比</strong></th><th style="text-align:center"><strong>LayerNorm</strong></th><th style="text-align:center"><strong>RMSNorm</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong>归一化目标</strong></td><td style="text-align:center">均值中心化 + 方差缩放</td><td style="text-align:center">仅均方根（RMS）缩放</td></tr><tr><td style="text-align:center"><strong>计算复杂度</strong></td><td style="text-align:center">较高（需计算均值和方差）</td><td style="text-align:center">较低（仅需均方值）</td></tr><tr><td style="text-align:center"><strong>参数数量</strong></td><td style="text-align:center"><em>γ</em>+<em>β</em>（2d 参数）</td><td style="text-align:center">仅 <em>γ</em>（d 参数）</td></tr></tbody></table></li><li><p>SwiGLU</p><p>SwiGLU 是门控线性单元（GLU）的变体，公式如下：<br><img src="image4.png" alt="image2"></p><p><img src="image5.png" alt="image2"></p><p>第二个公式的激活函数是 sigmoid，sigmoid 函数特点：</p></li></ul><p><img src="image6.png" alt="image2"></p><p>当 β 趋向于 0 时，相当于 y=x/2，线性函数，当 β 趋向于无穷时（x&gt;0,x&lt;0,x=0)，相当于 ReLU 函数，当 β=1，swish 光滑且非单调。</p><p>Swish (xW) 为门控权重（相当于选择遗忘比例），用权重对 xV 逐元素加权，用 W2 映射回原维度。</p><ul><li><p>RoPE（待）</p><p>传统 PE，model 需要学习隐式位置关系，而 RoPE 通过旋转矩阵直接编码位置，即将位置信息通过旋转矩阵融合 key\query 向量中，直接建模相对位置依赖关系，value 是不需要旋转的:</p><p><img src="image7.png" alt="image2"></p></li></ul><p>以上三个改变与原 decoder 结合实现了 LLaMA</p><h2 id="注意力机制优化"><a class="markdownIt-Anchor" href="#注意力机制优化">#</a> 注意力机制优化</h2><p>在 Transformer 结构中，自注意力机制的时间和存储复杂度与序列的长度呈平方的关系，因此占用了大量的计算设备内存并消耗了大量的计算资源，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>: Q K </span><br><span class="line">Q = [](seq_len*vector_len)</span><br><span class="line">K = [](seq_len*vector_len)</span><br><span class="line">Q*K(转置) = seq_len * seq_len</span><br></pre></td></tr></table></figure><p>那么就有这种情况：</p><ul><li><strong>显存占用</strong>：</li><li><strong>计算时间</strong>：<br>每次注意力计算需 seq_len 方 * d 次操作，<em>seq_len=32</em>k*, <em>d</em>=1024 时约为 10 的 12 次方次操作。</li></ul><p>所以需要方法去优化这一问题，以下举例两种方法：</p><ul><li><p>稀疏注意力</p><ul><li>全局注意力：在稀疏注意力中保留少量全局节点（如 [CLS] token 或特定位置），这些节点可以与序列中<strong>所有其他位置</strong>交互。</li><li>带状注意力：每个 Query 只与<strong>固定宽度邻域内</strong>的 Key 交互（类似对角带状矩阵）</li><li>膨胀注意力：以<strong>固定间隔跳跃采样</strong> Key</li><li>随机注意力：每个 Query 随机选择 <em>r</em> 个位置进行交互</li><li>局部块注意力：多个不重叠块交互</li></ul><p>一般利用上述的复合模式 s</p></li><li><p>低秩注意力:</p><p><img src="image8.jpg" alt="image2"></p></li></ul><h2 id="混合专家模型"><a class="markdownIt-Anchor" href="#混合专家模型">#</a> 混合专家模型</h2><p>​混合专家模型 (MixedExpert Models，MoEs) 日益受到关注。依据大模型缩放法则，模型规模是提升性能的关键，然而规模扩大必然使计算资源大幅增加。因此，在有限计算资源预算下，如何用更少训练步数训练更大模型成为关键问题。为解决该问题，混合专家模型基于一个简洁的思想：模型不同部分（即 “专家”）专注不同任务或数据层面。</p><p>其实就是把模型内部的一组专用子网络，每个子网络负责处理数据中特定类型的任务，如：</p><ul><li>输入句子是数学问题 → 激活 “数学专家”</li><li>输入是诗歌 → 激活 “文学专家”</li></ul><p>优势：若 model 有 100 个专家，每次输入仅用 2 个，计算量少，而且每个专家通过训练集中学习特定模式，比通用模块更高效。</p><p>混合专家模型按照门控网络（Gate）类型，可以从广义上讲可以分为三个大类：稀疏混合专家模型（Sparse MoE）、稠密混合专家模型（Dense MoE）、软混合专家模型（Soft MoE）：</p><ul><li><p>稀疏混合专家模型：input 之后，门控网络仅激活少数专家</p></li><li><p>稠密混合专家模型：所有专家激活，甲醛组合输出，这个是要计算每个 wi,bi 与 x 的结果</p></li><li><p>软混合专家模型：门控网络分配的权重直接融合不同专家的参数，得到 w (融合)，b (融合)，融合与 x 计算 = w*x+b</p><table><thead><tr><th style="text-align:center"><strong>类型</strong></th><th style="text-align:center"><strong>专家激活方式</strong></th><th style="text-align:center"><strong>计算量</strong></th><th style="text-align:center"><strong>参数量扩展性</strong></th><th style="text-align:center"><strong>典型场景</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong>稀疏 MoE</strong></td><td style="text-align:center">硬性 Top-k 选择（如 k=2）</td><td style="text-align:center"><em>O</em>(<em>k</em>⋅FFN)</td><td style="text-align:center">极高（万亿级）</td><td style="text-align:center">大规模预训练（Mixtral, GPT-4）</td></tr><tr><td style="text-align:center"><strong>稠密 MoE</strong></td><td style="text-align:center">所有专家加权求和</td><td style="text-align:center"><em>O</em>(<em>N</em>⋅FFN)</td><td style="text-align:center">低（十亿级）</td><td style="text-align:center">小规模多任务学习</td></tr><tr><td style="text-align:center"><strong>软 MoE</strong></td><td style="text-align:center">软性稀疏权重</td><td style="text-align:center"><em>O</em>(<em>N</em>⋅FFN)</td><td style="text-align:center">中（百亿级）</td><td style="text-align:center">平衡效率与稳定性需求</td></tr></tbody></table></li></ul>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 混合专家模型 </tag>
            
            <tag> LLaMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT&amp;BertBuild</title>
      <link href="/2025/05/17/GPT-BertBuild/"/>
      <url>/2025/05/17/GPT-BertBuild/</url>
      
        <content type="html"><![CDATA[<h2 id="生成式预训练语言模型gpt"><a class="markdownIt-Anchor" href="#生成式预训练语言模型gpt">#</a> 生成式预训练语言模型 GPT</h2><p>GPT 的模型结构，由多个 transformer 的 decoder 组成，结构如下：</p><p><img src="image1.png" alt="image1"></p><p>该模型利用的是 transformer 的解码器部分，训练和推理的过程是类似的，12 层 transformer 模块都在做类似的事情，到最后一层后输出预测的分数（映射到词分类中，得到预测的分数 / 置信度）</p><ul><li><p>有监督下游任务微调：</p><p>在进行下游任务微调时，仅使用 GPT 的最后一层的最后一个词的隐藏状态，同个全连接层映射到标签空间。因为每个词的隐藏状态都聚合了左侧所有历史词的信息，因此最后一个词的隐藏状态天然编码了整个序列的全局语义。</p><p>同时再进行微调的时候，可能会出现模型遗忘预训练阶段学习的通用只是表示，损失通用与泛化能力，出现灾难性遗忘的问题，所以通常采用混合预训练任务损失和下游微调损失缓解这一问题，Loss Function 如下:<br><img src="image2.png" alt="image2"></p></li></ul><h2 id="bert模型构建"><a class="markdownIt-Anchor" href="#bert模型构建">#</a> Bert 模型构建</h2><p>Bert 模型一般是基于 Transformer 的编码器部分构建的，是一个相当于完形填空 (mask) 的模型，是考虑双向的模型（同时利用两侧信息）</p><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>:the cat site on the mat</span><br><span class="line">tokenizer(分词):（假设不加入CLS标记和SEP标记）</span><br><span class="line">    code:[<span class="number">1996</span>,<span class="number">4248</span>,<span class="number">2825</span>,<span class="number">2007</span>,<span class="number">1996</span>,<span class="number">3829</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># 0为pad的编码，max假设为8</span></span><br><span class="line">    attention_mask:[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># 0表示填充的位置</span></span><br><span class="line">random mask:选取到了site作为掩码，替换为mask的编码-&gt;<span class="number">103</span></span><br><span class="line">code:[<span class="number">1996</span>,<span class="number">4248</span>,<span class="number">103</span>,<span class="number">2007</span>,<span class="number">1996</span>,<span class="number">3829</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">    attention_mask:[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">    label:[-<span class="number">100</span>,-<span class="number">100</span>,<span class="number">2825</span>,-<span class="number">100</span>,-<span class="number">100</span>,-<span class="number">100</span>,-<span class="number">100</span>,-<span class="number">100</span>] <span class="comment">#这个是用于计算掩码的真实标签，-100表示不需要计算，只保留被掩码的位置</span></span><br><span class="line">train:</span><br><span class="line">    将上述数据送入Bert模型中</span><br><span class="line">output:</span><br><span class="line">    [</span><br><span class="line">        [..,..,..,..,],</span><br><span class="line">        [..,..,..,..,],</span><br><span class="line">        [..,..,..,..,], <span class="comment">#取第二个位置进行计算交叉熵</span></span><br><span class="line">        [..,..,..,..,],</span><br><span class="line">        ........</span><br><span class="line">    ]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>训练过程：加载 dataset, 切分 traindataset 和 testdataset --&gt; 用 traindataset 训练词元分析器，形成 vocab 词表 --&gt; 加载预训练词元分析器 --&gt; 分词序列 --&gt; 加载随即权重的 BERT 模型，设置掩码比例，进行训练，下面是代码训练过程：</p><p>dataset 获取与镜像设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下载数据集：</span></span><br><span class="line">pip install huggingface_hub</span><br><span class="line">huggingface-cli download --repo-<span class="built_in">type</span> dataset legacy-datasets/wikipedia  --local-<span class="built_in">dir</span> wikipedia</span><br><span class="line"><span class="comment"># mirror</span></span><br><span class="line">export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure><p>environment (有部分可能不需要）:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">dependencies:</span><br><span class="line">  - _libgcc_mutex=<span class="number">0.1</span>=main</span><br><span class="line">  - _openmp_mutex=<span class="number">5.1</span>=1_gnu</span><br><span class="line">  - bzip2=<span class="number">1.0</span><span class="number">.8</span>=h5eee18b_6</span><br><span class="line">  - ca-certificates=<span class="number">2025.2</span><span class="number">.25</span>=h06a4308_0</span><br><span class="line">  - expat=<span class="number">2.7</span><span class="number">.1</span>=h6a678d5_0</span><br><span class="line">  - ld_impl_linux-<span class="number">64</span>=<span class="number">2.40</span>=h12ee557_0</span><br><span class="line">  - libffi=<span class="number">3.4</span><span class="number">.4</span>=h6a678d5_1</span><br><span class="line">  - libgcc-ng=<span class="number">11.2</span><span class="number">.0</span>=h1234567_1</span><br><span class="line">  - libgomp=<span class="number">11.2</span><span class="number">.0</span>=h1234567_1</span><br><span class="line">  - libstdcxx-ng=<span class="number">11.2</span><span class="number">.0</span>=h1234567_1</span><br><span class="line">  - libuuid=<span class="number">1.41</span><span class="number">.5</span>=h5eee18b_0</span><br><span class="line">  - ncurses=<span class="number">6.4</span>=h6a678d5_0</span><br><span class="line">  - openssl=<span class="number">3.0</span><span class="number">.16</span>=h5eee18b_0</span><br><span class="line">  - pip=<span class="number">25.1</span>=pyhc872135_2</span><br><span class="line">  - python=<span class="number">3.12</span><span class="number">.9</span>=h5148396_0</span><br><span class="line">  - readline=<span class="number">8.2</span>=h5eee18b_0</span><br><span class="line">  - setuptools=<span class="number">78.1</span><span class="number">.1</span>=py312h06a4308_0</span><br><span class="line">  - sqlite=<span class="number">3.45</span><span class="number">.3</span>=h5eee18b_0</span><br><span class="line">  - tk=<span class="number">8.6</span><span class="number">.14</span>=h39e8969_0</span><br><span class="line">  - wheel=<span class="number">0.45</span><span class="number">.1</span>=py312h06a4308_0</span><br><span class="line">  - xz=<span class="number">5.6</span><span class="number">.4</span>=h5eee18b_1</span><br><span class="line">  - zlib=<span class="number">1.2</span><span class="number">.13</span>=h5eee18b_1</span><br><span class="line">  - pip:</span><br><span class="line">      - accelerate==<span class="number">0.26</span><span class="number">.0</span></span><br><span class="line">      - aiohappyeyeballs==<span class="number">2.6</span><span class="number">.1</span></span><br><span class="line">      - aiohttp==<span class="number">3.11</span><span class="number">.18</span></span><br><span class="line">      - aiosignal==<span class="number">1.3</span><span class="number">.2</span></span><br><span class="line">      - attrs==<span class="number">25.3</span><span class="number">.0</span></span><br><span class="line">      - certifi==<span class="number">2025.4</span><span class="number">.26</span></span><br><span class="line">      - charset-normalizer==<span class="number">3.4</span><span class="number">.2</span></span><br><span class="line">      - datasets==<span class="number">3.6</span><span class="number">.0</span></span><br><span class="line">      - dill==<span class="number">0.3</span><span class="number">.8</span></span><br><span class="line">      - filelock==<span class="number">3.18</span><span class="number">.0</span></span><br><span class="line">      - frozenlist==<span class="number">1.6</span><span class="number">.0</span></span><br><span class="line">      - fsspec==<span class="number">2025.3</span><span class="number">.0</span></span><br><span class="line">      - huggingface-hub==<span class="number">0.31</span><span class="number">.2</span></span><br><span class="line">      - idna==<span class="number">3.10</span></span><br><span class="line">      - jinja2==<span class="number">3.1</span><span class="number">.6</span></span><br><span class="line">      - markupsafe==<span class="number">3.0</span><span class="number">.2</span></span><br><span class="line">      - mpmath==<span class="number">1.3</span><span class="number">.0</span></span><br><span class="line">      - multidict==<span class="number">6.4</span><span class="number">.3</span></span><br><span class="line">      - multiprocess==<span class="number">0.70</span><span class="number">.16</span></span><br><span class="line">      - mwparserfromhell==<span class="number">0.6</span><span class="number">.6</span></span><br><span class="line">      - networkx==<span class="number">3.4</span><span class="number">.2</span></span><br><span class="line">      - numpy==<span class="number">2.2</span><span class="number">.5</span></span><br><span class="line">      - nvidia-cublas-cu12==<span class="number">12.6</span><span class="number">.4</span><span class="number">.1</span></span><br><span class="line">      - nvidia-cuda-cupti-cu12==<span class="number">12.6</span><span class="number">.80</span></span><br><span class="line">      - nvidia-cuda-nvrtc-cu12==<span class="number">12.6</span><span class="number">.77</span></span><br><span class="line">      - nvidia-cuda-runtime-cu12==<span class="number">12.6</span><span class="number">.77</span></span><br><span class="line">      - nvidia-cudnn-cu12==<span class="number">9.5</span><span class="number">.1</span><span class="number">.17</span></span><br><span class="line">      - nvidia-cufft-cu12==<span class="number">11.3</span><span class="number">.0</span><span class="number">.4</span></span><br><span class="line">      - nvidia-cufile-cu12==<span class="number">1.11</span><span class="number">.1</span><span class="number">.6</span></span><br><span class="line">      - nvidia-curand-cu12==<span class="number">10.3</span><span class="number">.7</span><span class="number">.77</span></span><br><span class="line">      - nvidia-cusolver-cu12==<span class="number">11.7</span><span class="number">.1</span><span class="number">.2</span></span><br><span class="line">      - nvidia-cusparse-cu12==<span class="number">12.5</span><span class="number">.4</span><span class="number">.2</span></span><br><span class="line">      - nvidia-cusparselt-cu12==<span class="number">0.6</span><span class="number">.3</span></span><br><span class="line">      - nvidia-nccl-cu12==<span class="number">2.26</span><span class="number">.2</span></span><br><span class="line">      - nvidia-nvjitlink-cu12==<span class="number">12.6</span><span class="number">.85</span></span><br><span class="line">      - nvidia-nvtx-cu12==<span class="number">12.6</span><span class="number">.77</span></span><br><span class="line">      - packaging==<span class="number">25.0</span></span><br><span class="line">      - pandas==<span class="number">2.2</span><span class="number">.3</span></span><br><span class="line">      - propcache==<span class="number">0.3</span><span class="number">.1</span></span><br><span class="line">      - psutil==<span class="number">7.0</span><span class="number">.0</span></span><br><span class="line">      - pyarrow==<span class="number">20.0</span><span class="number">.0</span></span><br><span class="line">      - python-dateutil==<span class="number">2.9</span><span class="number">.0</span>.post0</span><br><span class="line">      - pytz==<span class="number">2025.2</span></span><br><span class="line">      - pyyaml==<span class="number">6.0</span><span class="number">.2</span></span><br><span class="line">      - regex==<span class="number">2024.11</span><span class="number">.6</span></span><br><span class="line">      - requests==<span class="number">2.32</span><span class="number">.3</span></span><br><span class="line">      - safetensors==<span class="number">0.5</span><span class="number">.3</span></span><br><span class="line">      - six==<span class="number">1.17</span><span class="number">.0</span></span><br><span class="line">      - sympy==<span class="number">1.14</span><span class="number">.0</span></span><br><span class="line">      - tokenizers==<span class="number">0.21</span><span class="number">.1</span></span><br><span class="line">      - torch==<span class="number">2.7</span><span class="number">.0</span></span><br><span class="line">      - tqdm==<span class="number">4.67</span><span class="number">.1</span></span><br><span class="line">      - transformers==<span class="number">4.51</span><span class="number">.3</span></span><br><span class="line">      - triton==<span class="number">3.3</span><span class="number">.0</span></span><br><span class="line">      - typing-extensions==<span class="number">4.13</span><span class="number">.2</span></span><br><span class="line">      - tzdata==<span class="number">2025.2</span></span><br><span class="line">      - urllib3==<span class="number">2.4</span><span class="number">.0</span></span><br><span class="line">      - xxhash==<span class="number">3.5</span><span class="number">.0</span></span><br><span class="line">      - yarl==<span class="number">1.20</span><span class="number">.0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast,BertConfig,BertForMaskedLM,DataCollatorForLanguageModeling,TrainingArguments,Trainer</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;2&#x27;</span> <span class="comment"># 这个得加，多gpu跑的话会有问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载本地数据</span></span><br><span class="line">wiki = load_dataset(<span class="string">&quot;/data/hcfeng/learnLLM/wikipedia&quot;</span>, <span class="string">&quot;20220301.en&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"><span class="comment">#仅保留text列</span></span><br><span class="line">wiki = wiki.remove_columns([col <span class="keyword">for</span> col <span class="keyword">in</span> wiki.column_names <span class="keyword">if</span> col!=<span class="string">&quot;text&quot;</span>])</span><br><span class="line"><span class="comment">#切割数据</span></span><br><span class="line">dataset = wiki</span><br><span class="line">d = dataset.train_test_split(train_size=<span class="number">0.9</span>,test_size=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_to_text</span>(<span class="params">dataset,filename</span>):</span><br><span class="line">    outpur_path=<span class="string">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/&quot;</span>+filename <span class="comment">#拼接地址</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(outpur_path,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> dataset[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(t,file=f)</span><br><span class="line">dataset_to_text(d[<span class="string">&#x27;train&#x27;</span>],<span class="string">&#x27;train.txt&#x27;</span>)</span><br><span class="line">dataset_to_text(d[<span class="string">&#x27;test&#x27;</span>],<span class="string">&#x27;test.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">special_tokens = [</span><br><span class="line"><span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>, <span class="string">&quot;&lt;S&gt;&quot;</span>, <span class="string">&quot;&lt;T&gt;&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果根据训练和测试两个集合训练词元分析器，则需要修改files</span></span><br><span class="line"><span class="comment"># files = [&quot;train.txt&quot;, &quot;test.txt&quot;]</span></span><br><span class="line"><span class="comment"># 仅根据训练集合训练词元分析器</span></span><br><span class="line">files = [<span class="string">&quot;/data/hcfeng/learnLLM/TrainBert/small_dataset/train.txt&quot;</span>]</span><br><span class="line"><span class="comment"># BERT中采用的默认词表大小为30522，可以随意修改</span></span><br><span class="line">vocab_size = <span class="number">30522</span></span><br><span class="line"><span class="comment"># 最大序列长度，该值越小，训练速度越快</span></span><br><span class="line">max_length = <span class="number">512</span></span><br><span class="line"><span class="comment"># 是否将长样本截断</span></span><br><span class="line">truncate_longer_samples = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 初始化WordPiece词元分析器</span></span><br><span class="line">tokenizer = BertWordPieceTokenizer()</span><br><span class="line"><span class="comment"># 训练词元分析器，设定的 vocab_size 是最大允许值，但实际生成的词表大小可能更小</span></span><br><span class="line">tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens) </span><br><span class="line"><span class="comment"># 允许截断达到最大512个词元</span></span><br><span class="line">tokenizer.enable_truncation(max_length=max_length)</span><br><span class="line">model_path = <span class="string">&quot;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果文件夹不存在，则先创建文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(model_path):</span><br><span class="line">    os.mkdir(model_path)</span><br><span class="line"><span class="comment"># 保存词元分析器模型</span></span><br><span class="line">tokenizer.save_model(model_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一些词元分析器中的配置保存到配置文件，包括特殊词元、转换为小写、最大序列长度等</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(model_path, <span class="string">&quot;config.json&quot;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    tokenizer_cfg = &#123;</span><br><span class="line">    <span class="string">&quot;do_lower_case&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&quot;unk_token&quot;</span>: <span class="string">&quot;[UNK]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;sep_token&quot;</span>: <span class="string">&quot;[SEP]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;[PAD]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;cls_token&quot;</span>: <span class="string">&quot;[CLS]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;mask_token&quot;</span>: <span class="string">&quot;[MASK]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;model_max_length&quot;</span>: max_length,</span><br><span class="line">    <span class="string">&quot;max_len&quot;</span>: max_length,</span><br><span class="line">    &#125;</span><br><span class="line">    json.dump(tokenizer_cfg, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizer处理序列会自动添加CLS和SEP，但是tokenizer.tokenize()只会进行基础分词，不会添加特殊分词</span></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&#x27;/data/hcfeng/learnLLM/TrainBert/small_pretrained-bert&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;tokenier:<span class="subst">&#123;tokenizer&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_with_truncation</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 使用词元分析对句子进行处理并截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], truncation=<span class="literal">True</span>, padding=<span class="string">&quot;max_length&quot;</span>,max_length=max_length, return_special_tokens_mask=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_without_truncation</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 使用词元分析对句子进行处理且不截断的映射函数（Mapping function）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], return_special_tokens_mask=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码函数将依赖于truncate_longer_samples变量</span></span><br><span class="line">encode = encode_with_truncation <span class="keyword">if</span> truncate_longer_samples <span class="keyword">else</span> encode_without_truncation</span><br><span class="line"><span class="comment"># 对训练数据集进行分词处理</span></span><br><span class="line">train_dataset = d[<span class="string">&quot;train&quot;</span>].<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 对测试数据集进行分词处理</span></span><br><span class="line">test_dataset = d[<span class="string">&quot;test&quot;</span>].<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> truncate_longer_samples:</span><br><span class="line">    <span class="comment"># 移除其他列，将input_ids和attention_mask设置为PyTorch张量</span></span><br><span class="line">    train_dataset.set_format(<span class="built_in">type</span>=<span class="string">&quot;torch&quot;</span>, columns=[<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;attention_mask&quot;</span>])</span><br><span class="line">    test_dataset.set_format(<span class="built_in">type</span>=<span class="string">&quot;torch&quot;</span>, columns=[<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;attention_mask&quot;</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 移除其他列，将它们保留为Python列表</span></span><br><span class="line">    test_dataset.set_format(columns=[<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;attention_mask&quot;</span>, <span class="string">&quot;special_tokens_mask&quot;</span>])</span><br><span class="line">    train_dataset.set_format(columns=[<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;attention_mask&quot;</span>, <span class="string">&quot;special_tokens_mask&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于不截断的情况</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">group_texts</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="comment"># 拼接所有文本</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">list</span>(chain(*examples[k])) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># 舍弃了剩余部分，如果模型支持填充而不是舍弃，则可以根据需要自定义这部分</span></span><br><span class="line">    <span class="keyword">if</span> total_length &gt;= max_length:</span><br><span class="line">        total_length = (total_length // max_length) * max_length</span><br><span class="line">    <span class="comment"># 按照最大长度分割成块</span></span><br><span class="line">    result = &#123;</span><br><span class="line">    k: [t[i : i + max_length] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, max_length)]</span><br><span class="line">    <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> truncate_longer_samples:</span><br><span class="line">    train_dataset = train_dataset.<span class="built_in">map</span>(group_texts, batched=<span class="literal">True</span>,desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class="line">    test_dataset = test_dataset.<span class="built_in">map</span>(group_texts, batched=<span class="literal">True</span>,desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;max_length&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 将它们从列表转换为PyTorch张量</span></span><br><span class="line">    train_dataset.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">    test_dataset.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenizer.vocab_size:<span class="subst">&#123;tokenizer.vocab_size&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment">#这里vocab_size一般设为tokenizer.vocab_size，其实只要大于这个数字都是可以的，只是会占用了显存空间（training)，但是不能设小，会出现索引越界的问题</span></span><br><span class="line">model_config = BertConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=max_length) </span><br><span class="line">model = BertForMaskedLM(config=model_config)</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForLanguageModeling(</span><br><span class="line">    tokenizer=tokenizer, mlm=<span class="literal">True</span>, mlm_probability=<span class="number">0.2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=model_path, <span class="comment"># 输出目录，用于保存模型检查点</span></span><br><span class="line">    eval_strategy=<span class="string">&quot;steps&quot;</span>, <span class="comment"># 每隔`logging_steps`步进行一次评估</span></span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">100</span>, <span class="comment"># 训练时的轮数，可以根据需要进行调整</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">10</span>, <span class="comment"># 训练批量大小，可以根据GPU内存容量将其设置得尽可能大</span></span><br><span class="line">    gradient_accumulation_steps=<span class="number">8</span>, <span class="comment"># 在更新权重之前累积梯度</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">64</span>, <span class="comment"># 评估批量大小</span></span><br><span class="line">    logging_steps=<span class="number">1000</span>, <span class="comment"># 每隔1000步进行一次评估，记录并保存模型检查点</span></span><br><span class="line">    save_steps=<span class="number">1000</span>,</span><br><span class="line">    <span class="comment"># load_best_model_at_end=True, # 是否在训练结束时加载最佳模型（根据损失）</span></span><br><span class="line">    <span class="comment"># save_total_limit=3, # 如果磁盘空间有限，则可以限制只保存3个模型权重</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">    eval_dataset=test_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查点</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input_ids dtype:&quot;</span>, train_dataset[<span class="number">0</span>][<span class="string">&quot;input_ids&quot;</span>].dtype)  <span class="comment"># 应该是 torch.int64 (long)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;attention_mask dtype:&quot;</span>, train_dataset[<span class="number">0</span>][<span class="string">&quot;attention_mask&quot;</span>].dtype)  <span class="comment"># 应该是 torch.int64 (long)</span></span><br><span class="line">batch = data_collator([train_dataset[<span class="number">0</span>], train_dataset[<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input_ids shape:&quot;</span>, batch[<span class="string">&quot;input_ids&quot;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;batch[&quot;input_ids&quot;]:<span class="subst">&#123;batch[<span class="string">&quot;input_ids&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels shape:&quot;</span>, batch[<span class="string">&quot;labels&quot;</span>].shape)</span><br><span class="line"><span class="comment"># print(&quot;labels min/max:&quot;, torch.min(batch[&quot;labels&quot;]), torch.max(batch[&quot;labels&quot;]))</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input_ids min/max:&quot;</span>, torch.<span class="built_in">min</span>(batch[<span class="string">&quot;input_ids&quot;</span>]), torch.<span class="built_in">max</span>(batch[<span class="string">&quot;input_ids&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  </span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bert </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="/2025/05/13/Transformer/"/>
      <url>/2025/05/13/Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="transformer四层结构"><a class="markdownIt-Anchor" href="#transformer四层结构">#</a> Transformer 四层结构</h2><p>Transformer 结构：</p><p><img src="image1.png" alt="image1"></p><ol><li><p>嵌入表示层</p><p>Transformer 的自注意力机制是并行处理所有书如此，无法区分语序，所以需要进行位置编码，做法：先为每个单词生成向量嵌入表示，对每个单词所在位置对应一个位置向量，将两个向量进行相加。位置向量的生成公式如下：</p><p><img src="image2.png" alt="image1"></p><p>根据位置的就选择正弦余弦函数进行计算，这个计算是对每个单词里面的向量的每一维都进行计算，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#transformer位置编码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model,max_seq_len = <span class="number">80</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 根据pos和i创建一个常量PE矩阵</span></span><br><span class="line">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class="line">        <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, d_model, <span class="number">2</span>):</span><br><span class="line">                pe[pos, i] = math.sin(pos / (<span class="number">10000</span> ** (i/d_model)))</span><br><span class="line">                pe[pos, i + <span class="number">1</span>] = math.cos(pos / (<span class="number">10000</span> ** (i/d_model)))</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment">#形状 (1, seq_len, d_model)</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)   <span class="comment">#将 pe 保存为模型的一部分（不参与梯度更新，但会随模型保存/加载）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#x:(batch_size, seq_len, d_model)</span></span><br><span class="line">        seq_len = x.size(<span class="number">1</span>)</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:,:seq_len].cuda()</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>使用正余弦的原因是，函数的范围是 [-1，1] 与词向量相加不会太偏离原始语义，同时第 pos+k 个位置的编码是第 pos 个位置编码的线性组合（根据三角函数和角公式决定），这就蕴含了单词之间的距离信息：</p><p><img src="image3.png" alt="image2"></p></li><li><p>自注意力层</p><p>自注意力机制，即自己作为 QKV 进行计算，但是解码器有两个注意力模块，一个是掩码多头，一个是交叉多头注意力，但是原理其实和下面代码差不多，直接用代码展示比较能说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#transformer多头自注意力机制</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, heads, d_model,dropout = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.h = heads</span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // heads</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.q_linear = nn.Linear(d_model,d_model)</span><br><span class="line">        <span class="variable language_">self</span>.k_linear = nn.Linear(d_model,d_model)</span><br><span class="line">        <span class="variable language_">self</span>.v_linear = nn.Linear(d_model,d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(d_model,d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v, d_k, mask = <span class="literal">None</span>, dropout = <span class="literal">None</span> </span>):</span><br><span class="line">        <span class="comment"># 转置k相乘 ​​除以 math.sqrt(d_k)​​ 的操作是缩放点积注意力，防止点积数值过大​</span></span><br><span class="line">        scores = torch.matmul(q,k.transpose(-<span class="number">2</span>,-<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) <span class="comment">#掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0</span></span><br><span class="line">        </span><br><span class="line">        scores = F.sofmax(scores,dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = dropout(scores)</span><br><span class="line"></span><br><span class="line">        output = torch.matmul(scores,v)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask = <span class="literal">None</span></span>):</span><br><span class="line">        batch_size = q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用线性计算划分成h个头</span></span><br><span class="line">        q = <span class="variable language_">self</span>.q_linear(q).view(batch_size,-<span class="number">1</span>,<span class="variable language_">self</span>.h,<span class="variable language_">self</span>.d_k)</span><br><span class="line">        k = <span class="variable language_">self</span>.k_linear(k).view(batch_size,-<span class="number">1</span>,<span class="variable language_">self</span>.h,<span class="variable language_">self</span>.d_k)</span><br><span class="line">        v = <span class="variable language_">self</span>.v_linear(v).view(batch_size,-<span class="number">1</span>,<span class="variable language_">self</span>.h,<span class="variable language_">self</span>.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#转置头和seq_len位置</span></span><br><span class="line">        k = k.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        q = q.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        v = v.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        scores = <span class="variable language_">self</span>.attention(q, k, v, <span class="variable language_">self</span>.d_k, mask, <span class="variable language_">self</span>.dropout)</span><br><span class="line">        <span class="comment"># 拼接多头输出并线性变换</span></span><br><span class="line">        concat = scores.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.d_model)</span><br><span class="line">        output = <span class="variable language_">self</span>.out(concat) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li><li><p>前馈层</p><p>接收注意力层的输出，通过带有 ReLU 的 2 层全连接网络，第一层会映射到高纬度，因为隐藏层维度的增大有利于提高质量（实验证明）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#前馈层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff = <span class="number">2038</span>, dropout = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(d_ff,d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(F.relu(<span class="variable language_">self</span>.linear1(x)))</span><br><span class="line">        x = <span class="variable language_">self</span>.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>残差连接和归一化</p><p>​由 Transformer 结构组成的网络结构通常都非常庞大。编码器和解码器均由很多层基本的 Transformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究人员在 Transformer 块中进一步引入了残差连接与层归一化技术，以进一步提升训练的稳定性。具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。</p></li></ol><h2 id="解码器与编码器"><a class="markdownIt-Anchor" href="#解码器与编码器">#</a> 解码器与编码器</h2><p>​编码器端较容易实现。相比于编码器端，解码器端更复杂。具体来说，解码器的每个 Transformer 块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力部分。这主要是因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息。解码器端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的，因此这一额外增加的掩码是用来掩盖后续的文本信息的，以防模型在训练阶段直接看到后续的文本序列，进而无法得到有效的训练。此外，解码器端额外增加了一个多头交叉注意力模块，使用交叉注意力方法，同时接收来自编码器端的输出和当前 Transformer 块的前一个掩码注意力层的输出。查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。</p><p>​解码器端以自回归的方式生成目标语言文本，即在每个时间步 <em>t</em>，根据编码器端输出的源语言文本表示，以及前 t <em>−</em> 1 个时刻生成的目标语言文本，生成当前时刻的目标语言单词（以我的理解来说，训练阶段是没有显示时间步概念的，通过<strong>一次性输入完整序列 + 掩码矩阵</strong>，在单次前向传播中并行计算出所有位置的输出，同时利用掩码强制模型行为与自回归一致，而推理时必须显式按时间步生成，因为未来词未知（无法并行））。</p><h2 id="以推理生成中文翻译-我爱你-为例"><a class="markdownIt-Anchor" href="#以推理生成中文翻译-我爱你-为例">#</a> 以推理生成中文翻译  <code>&quot;我爱你&quot;</code>  为例：</h2><table><thead><tr><th style="text-align:center">时间步</th><th style="text-align:center">图 1 中对应的模块流程</th><th style="text-align:center">具体操作</th></tr></thead><tbody><tr><td style="text-align:center"><code>t=1</code></td><td style="text-align:center">词元嵌入 → 位置编码 → 掩码多头注意力 → 编码器 - 解码器注意力 → 前馈网络 → Softmax</td><td style="text-align:center">输入  <code>&lt;start&gt;</code> ，输出  <code>&quot;我&quot;</code>  的概率分布。</td></tr><tr><td style="text-align:center"><code>t=2</code></td><td style="text-align:center">词元嵌入 ( <code>&lt;start&gt; 我</code> ) → 位置编码 → 掩码多头注意力 → … → Softmax</td><td style="text-align:center">输入  <code>&lt;start&gt; 我</code> ，输出  <code>&quot;爱&quot;</code>  的概率分布。</td></tr><tr><td style="text-align:center"><code>t=3</code></td><td style="text-align:center">词元嵌入 ( <code>&lt;start&gt; 我爱</code> ) → 位置编码 → … → Softmax</td><td style="text-align:center">输入  <code>&lt;start&gt; 我爱</code> ，输出  <code>&quot;你&quot;</code>  的概率分布。</td></tr><tr><td style="text-align:center"><code>t=4</code></td><td style="text-align:center">词元嵌入 ( <code>&lt;start&gt; 我爱你</code> ) → … → Softmax</td><td style="text-align:center">输入  <code>&lt;start&gt; 我爱你</code> ，输出  <code>&lt;end&gt;</code>  的概率分布，终止生成。</td></tr></tbody></table><p>可参考文章：<a href="https://blog.csdn.net/m0_64148253/article/details/140422497">https://blog.csdn.net/m0_64148253/article/details/140422497</a></p>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DatalinkX</title>
      <link href="/2025/05/13/DatalinkX/"/>
      <url>/2025/05/13/DatalinkX/</url>
      
        <content type="html"><![CDATA[<p>DatalinkX 是一个基于 Flink 的异构数据源流转服务，用来作为数据源之间的数据同步工具，通过抽象异构数据源驱动加借助 FlinkX 开源组件屏蔽数据源间不同的通信协议，通过页面化配置数据同步任务的方式简化数据库同步流程，并通过 Xxl-Job 组件实现定时调度。</p><h2 id="模块设置"><a class="markdownIt-Anchor" href="#模块设置">#</a> 模块设置</h2><ul><li>业务数据管理模块：负责管理数据源和同步任务的增删改查。包含数据源管理、任务管理等。</li><li>同步任务管理模块：借助 xx1-job 实现任务调度管理，确保异构数据源的定时同步和实时更新需求，同时支持批量任务管理，提高任务调度的效率。</li><li>RPC 通信模块：利用 Retrofit2 实现分布式服务间的 RPC 调用，提供高效的数据请求和响应机制支持各模块间的数据传输和服务调用。</li><li>数据源驱动模块：负责异构数据源的驱动配置，兼容多类型数据源的连接、读取和写入操作，为业务数据管理和同步提供支持。</li><li>消息通道模块：采用 Redis Stream 实现分布式消息队列，保障各模块间的消息通信效率和数据传 5.<br> 输的可靠性。</li><li>实时刷新前端模块：通过 SSE 技术实现数据的实时推送，配合 Ant Design 在前端即时刷新数据提供用户友好的动态体验。</li><li>公共模块：包含系统基础功能、工具方法和通用接口，实现各模块间的共享，提升系统的可维护性和复用性。</li><li>FlinkX 模块：负责人规模数据的实时同步、数据清洗和格式转换，支持流式和批量数据的整合处理，为系统提供数据支持。<br>数据清洗与转换模块：基于 Flinkx 的清洗能力，对不同数据源的数据进行标准化、过滤和转换以确保进入系统的数据一致性。</li></ul><h2 id="重要模块"><a class="markdownIt-Anchor" href="#重要模块">#</a> 重要模块</h2><ul><li><h3 id="消息队列sse实现实时推送"><a class="markdownIt-Anchor" href="#消息队列sse实现实时推送">#</a> 消息队列 + SSE 实现实时推送</h3><ol><li>首先基于自定义注解 @messageHub 标注消费者方法，在程序运行初始化完 Bean 之后，实现对每个 Bean 进行循环检查，寻找带该注解的方法，找到后将其注册成消费者。</li><li>基于 Redis 的 stream 数据结构实现轻量级的消息队列，出于两点原因选择该方式，首先是本项目只需要使用消息队列实现任务状态和任务进度的推送，不需要严格意义上的高可用、低延迟，其次是 RabbitMQ 或者是 Kafka 服务的复杂性，一个健壮的项目并不是引入越多的中间件会越好，而是 Bug 越少越好，或者说出 Bug 概率越低越好。而 Redis 的 stream 借助了很多 Kafka 的设计思路，而且更轻量级。</li><li>消费者的注册，其实是非常简单的一件事，只需要带有 @messageHub 的，就创建一个线程监听注解中配置的消息地址，监听到消息后回调给该消费者，并返回 ACK 标记消息已被消费，如果在消费过程中出现故障，则会继续在 pending 队列中消费，一般会重试 4 次，如果消费不了，只能是放弃，因为这并不是一个十分重要的操作，丢失部分进度消息是用户能够接收的。</li><li>基于 SSE 实例实现消息实时推送，SSE 是使用流信息向浏览器进行信息推送，也就是信息不是一次性的，而是连续不断的，类似于视频的播放。SSE 是一个单向通信，只能服务器向浏览器发送，这里不选择 websocket 的原因和 2 一样，基于业务层面去考虑。</li></ol></li><li><h3 id="抽象数据源driver设计"><a class="markdownIt-Anchor" href="#抽象数据源driver设计">#</a> 抽象数据源 Driver 设计</h3><p>​       该项目是支持异构数据源同步的，所以会有很多种数据源，进行每种数据源连接是确实可以使用 if else 去定义，但是这并不好维护，而且可拓展性十分差，想象一下每次新增一个数据源就写多一个 if else，代码的可读性也会很差，所以这里会基于模板模式和工厂模式对数据源驱动的获取进行抽象。</p><p>​我们把数据源实例抽象为几个步骤，读写分离，然后分别定义为接口，将支持 JDBC 的数据源做一种实现，不支持 JDBC 的数据源也做一种实现。每种实现又有一个基础类去实现基础功能。</p><p>如下图所示：</p><p><img src="image1.png" alt="image1"></p><ol><li><p>DsDriverFactory 遵循工厂设计原则，每个数据源必要信息通过压缩算法返回一个唯一字符串 connectld，DsDriverFactory 向外暴露 getDsReader、getDsWriter 方法，通过解析 connectld 得到数据源 type，根据数据源 type 获取对应 Driver.class 并通过反射生成对应的驱动对象。</p></li><li><p>要新增数据源，比如新增一个 Mysql 的数据源，那么只需要继承 JDBC 这个类，JDBC 这个类已经实现了部分基本的功能，并制定了相应的执行流程，Mysql 继承之后，只需要把获取数据源 JdbcUrl 这种每个数据源都不同的抽象方法给实现了就行。</p></li><li><p>总体流程就是，用户提交了注册新数据源的信息之后，注册信息会存入数据库中，等到创建流转任务时，会获取数据源下的数据表，这时就调用 DsDriverFactory 获取数据源实例，并调用实例中的方法获取对应的数据 / 写入数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">PACKAGE_PREFIX</span> <span class="operator">=</span> <span class="string">&quot;com.datalinkx.driver.dsdriver.&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String <span class="title function_">getDriverClass</span><span class="params">(String driverName)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> PACKAGE_PREFIX + driverName.toLowerCase() + <span class="string">&quot;driver&quot;</span> + <span class="string">&quot;.&quot;</span> + ConnectIdUtils.toPascalCase(driverName) + <span class="string">&quot;Driver&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//getDriver 方法依赖于 ConnectIdUtils 和 getDriverClass 方法来确定应该加载和实例化哪个驱动类。</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IDsDriver <span class="title function_">getDriver</span><span class="params">(String connectId)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">dsType</span> <span class="operator">=</span> ConnectIdUtils.getDsType(connectId);</span><br><span class="line">        <span class="comment">//拼接地址，也就是包的地址，来获取对应的类名，并返回给DsServiceImpl类，让他知道用的是哪个实现类，这就是用来替代无数个if else的关键地方</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">driverClassName</span> <span class="operator">=</span> getDriverClass(dsType);</span><br><span class="line">        Class&lt;?&gt; driverClass = Class.forName(driverClassName);</span><br><span class="line">        Constructor&lt;?&gt; constructor = driverClass.getDeclaredConstructor(String.class);</span><br><span class="line">        <span class="keyword">return</span> (IDsDriver) constructor.newInstance(connectId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IStreamDriver <span class="title function_">getStreamDriver</span><span class="params">(String connectId)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">dsType</span> <span class="operator">=</span> ConnectIdUtils.getDsType(connectId);</span><br><span class="line">        <span class="type">String</span> <span class="variable">driverClassName</span> <span class="operator">=</span> getDriverClass(dsType);</span><br><span class="line">        Class&lt;?&gt; driverClass = Class.forName(driverClassName);</span><br><span class="line">        Constructor&lt;?&gt; constructor = driverClass.getDeclaredConstructor(String.class);</span><br><span class="line">        <span class="keyword">return</span> (IStreamDriver) constructor.newInstance(connectId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IDsReader <span class="title function_">getDsReader</span><span class="params">(String connectId)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * getDriver(connectId) 返回的是一个 IDsDriver 类型的对象，然后将其强制转换为 IDsReader 类型。这种转换能够成功执行的前提是：</span></span><br><span class="line"><span class="comment">         * IDsReader 接口继承自 IDsDriver 接口：如果 IDsReader 接口是 IDsDriver 接口的子接口，那么所有的 IDsReader 实现也必然是 IDsDriver 的实现。</span></span><br><span class="line"><span class="comment">         * 这种情况下，转换是合法的，因为 IDsReader 是 IDsDriver 的子类型。</span></span><br><span class="line"><span class="comment">         * 实现类的兼容性：调用 getDriver(connectId) 方法返回的具体实现类必须同时实现了 IDsDriver 和 IDsReader 接口。这是类型转换能够成功的关键。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> (IDsReader) getDriver(connectId);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InstantiationException | IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;driver load error&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NoSuchMethodException e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;driver load error&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;can not initialize driver&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IDsWriter <span class="title function_">getDsWriter</span><span class="params">(String connectId)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> (IDsWriter) getDriver(connectId);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InstantiationException | IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;driver load error&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NoSuchMethodException e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;driver load error&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;can not initialize driver&quot;</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><h3 id="xxl-job平台"><a class="markdownIt-Anchor" href="#xxl-job平台">#</a> XXL-Job 平台</h3><p>​xxl-job 是一个分布式任务调度平台，核心思想是将调度任务抽象成为调度器和执行器，平台本身不承担业务逻辑，只负责发起调度请求后，由执行器接收调度请求并执行任务，这里的任务被抽象为分散的 JobHandler。JobHandler 分散到各个业务代码中，依 netty 保持与调度器的交互，通过这种方式即可实现调度与任务相互解耦，从而提高系统整体的稳定性和拓展性。如下图：</p><p><img src="image2.png" alt="image2"></p><ol><li><p>调度器只需要借助 maven 打包 xxl-job 的 admin 即可，然后 java -jar 把 jar 包运行起来即可，或者是用 docker 直接部署，而执行器则放在项目中的 Job 模块下，使用 @XxlJob (“XXX”) 在方法上定义执行器即可</p></li><li><p>为了解决 xxl-job 登录的问题，这里会使用一个拦截器，拦截器逻辑非常朴实无华，在发请求之前，判断 cookieValue 是否存在，如果不存在就通过 datalinkx-client 的能力发起一次登录请求。登陆后保存 cookie 加到原请求中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Response <span class="title function_">intercept</span><span class="params">(Chain chain)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"><span class="type">Request</span> <span class="variable">request</span> <span class="operator">=</span> chain.request();</span><br><span class="line"></span><br><span class="line">Response response;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (StringUtils.isEmpty(cookieValue)) &#123;</span><br><span class="line">retrofit2.<span class="type">Response</span> <span class="variable">loginResp</span> <span class="operator">=</span> xxlLoginClient.login(username, passwd, <span class="string">&quot;on&quot;</span>).execute();</span><br><span class="line">cookieValue = loginResp.headers().get(SET_COOKIE_HEADER);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (StringUtils.isNotEmpty(cookieValue)) &#123;<span class="comment">//检查是否成功获取了 Cookie 值</span></span><br><span class="line"><span class="comment">//如果获取了 Cookie，则创建一个新的请求，将 Cookie 添加到请求头中</span></span><br><span class="line"><span class="type">Request</span> <span class="variable">newRequest</span> <span class="operator">=</span> chain.request().newBuilder()</span><br><span class="line">.addHeader(HEADER_COOKIE, cookieValue)</span><br><span class="line">.build();</span><br><span class="line"><span class="comment">//使用新的请求继续执行链式调用，获取响应</span></span><br><span class="line"><span class="comment">//当前拦截器已经完成了对请求的处理，将请求传递给拦截器链中的下一个拦截器继续处理</span></span><br><span class="line"><span class="comment">// 最后一个拦截器处理请求后，不再调用 proceed，而是直接将请求发送到服务器。服务器的响应返回，按相反顺序通过拦截器链。</span></span><br><span class="line">response = chain.proceed(newRequest);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">log.error(<span class="string">&quot;xxl-job login error&quot;</span>);</span><br><span class="line">response = chain.proceed(chain.request());</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> e;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (response.code() == <span class="number">200</span>) &#123;</span><br><span class="line"><span class="type">ResponseBody</span> <span class="variable">responseBody</span> <span class="operator">=</span> response.body();</span><br><span class="line"><span class="type">MediaType</span> <span class="variable">contentType</span> <span class="operator">=</span> responseBody != <span class="literal">null</span> ? responseBody.contentType() : <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">if</span> (contentType != <span class="literal">null</span> &amp;&amp; contentType.subtype().equals(<span class="string">&quot;json&quot;</span>)) &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">bodyString</span> <span class="operator">=</span> getBody(response);</span><br><span class="line">bodyString = bodyString.replaceFirst(<span class="string">&quot;\&quot;result\&quot;:\\s*\&quot;\\s*\&quot;&quot;</span>, <span class="string">&quot;\&quot;result\&quot;: null&quot;</span>);</span><br><span class="line"><span class="type">ResponseBody</span> <span class="variable">body</span> <span class="operator">=</span> ResponseBody.create(contentType, bodyString);</span><br><span class="line">response = response.newBuilder().body(body).build();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> response;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>流转任务的生命钩子函数设计</p><p>​通常的模板方法模式中会设计一个 abstract 的抽象方法，交给它的子类实现，这个方法称为模板方法。而钩子方法，是对于抽象方法或者接口中定义的方法的一个空实现，也是模板方法模式的一种实现方式。设计钩子方法的主要目的是干预执行流程，使得控制行为流程更加灵活，更符合实际业务的需求。在该项目流转任务的生命周期都是基于钩子方法 + 模板实现的，任务由 xxl-job 调度中心通过 netty 回调到 DataTransHandler 执行器中，执行器中注入任务触发类 DataTransferAction，由 doAction 开始一次任务的执行，而 FlinkAction 是继承的 AbstractDataTransferAction 实现各种模板和钩子方法:</p><p>​六个钩子方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">begin</span><span class="params">(D info)</span>;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">end</span><span class="params">(D info, <span class="type">int</span> status, String errmsg)</span>;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">beforeExec</span><span class="params">(U unit)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(U unit)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="type">boolean</span> <span class="title function_">checkResult</span><span class="params">(U unit)</span>;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">afterExec</span><span class="params">(U unit, <span class="type">boolean</span> success, String errorMsg)</span>;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> U <span class="title function_">convertExecUnit</span><span class="params">(D info)</span>;</span><br></pre></td></tr></table></figure><p>​doAction 串联生命周期：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doAction</span><span class="params">(T actionInfo)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 获取job详情</span></span><br><span class="line">    <span class="type">D</span> <span class="variable">detail</span> <span class="operator">=</span> getJobDetail(actionInfo);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">StringBuffer</span> <span class="variable">error</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line">        <span class="comment">// 准备执行job</span></span><br><span class="line">        begin(detail);</span><br><span class="line">        <span class="comment">// ....</span></span><br><span class="line">        <span class="comment">// 遍历执行启动flink任务</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 每个单元执行前的准备</span></span><br><span class="line">            <span class="keyword">if</span> (isStop()) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;logkill trigger&quot;</span>);</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">InterruptedException</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            beforeExec(unit)</span><br><span class="line">            <span class="comment">// 启动任务</span></span><br><span class="line">            execute(unit);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;user stop&quot;</span>, e);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;execute flink task error.&quot;</span>, e);</span><br><span class="line">            unitCount.decrementAndGet();</span><br><span class="line">            afterExec(unit, <span class="literal">false</span>, e.getMessage());</span><br><span class="line">            error.append(e.getMessage()).append(<span class="string">&quot;\r\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 整个Job结束后的处理</span></span><br><span class="line">        end(detail, error.length() == <span class="number">0</span> ? JOB_STATUS_SUCCESS : JOB_STATUS_ERROR, error.length() == <span class="number">0</span> ? <span class="string">&quot;success&quot;</span> : error.toString());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        log.error(<span class="string">&quot;Stop task by user.&quot;</span>);</span><br><span class="line">        JobUtils.cntx().setCanceled(<span class="literal">true</span>);</span><br><span class="line">        end(detail, JOB_STATUS_STOP, <span class="string">&quot;cancel the job&quot;</span>);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">        log.error(<span class="string">&quot;sync failed&quot;</span>, e);</span><br><span class="line">        end(detail, JOB_STATUS_ERROR, e.getMessage());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​这个方法可谓是整个 datalinkx 的灵魂，核心中的核心，最核心的代码，我们通过这个方法将一个流转任务的生命周期串联起来。<br>这样真正的实现类 DataTransferAction.java 只需要继承 AbstractDataTransferAction 实现各个具体的生命周期方法即可，Xx1-Job 回调来的方法直接调用 FlinkAction 继承的 doAction 方法即可将任务串联执行。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> project_intro </category>
          
          <category> datalinkx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> datalinkx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLM-concept</title>
      <link href="/2025/05/12/LLM-concept/"/>
      <url>/2025/05/12/LLM-concept/</url>
      
        <content type="html"><![CDATA[<h1 id="大模型基本概念"><a class="markdownIt-Anchor" href="#大模型基本概念">#</a> 大模型基本概念</h1><h2 id="目标"><a class="markdownIt-Anchor" href="#目标">#</a> 目标</h2><ul><li><p>语言模型就是对自然语言的概率分布进行建模，即 P (w1 w2 w3 … wn)，计算这些词构成的这句话成为合法的一句话的概率</p></li><li><p>计算下一个词的概率 P (wn | w1 w2 w3… wn-1)</p><p><img src="image1.png" alt="image"></p></li></ul><h2 id="发展历程"><a class="markdownIt-Anchor" href="#发展历程">#</a> 发展历程</h2><p>从 n-gram:</p><p><img src="image2.png" alt="image"></p><p>到 neural language model: 每个词都映射成一个低维向量</p><p><img src="image3.png" alt="image"></p><p>再到后面的 transformer 出现，transformer 的出现，NLP 进入了预训练微调阶段，也就是只需把预训练好的模型用特定任务的训练集去微调（fine-tune），即可对下游任务进行操作，这种模型是 PLM。</p><p>随着 OpenAI 发布的 1750 亿个参数（GPT-3），开启 LLM 时代</p><h2 id="问题发现"><a class="markdownIt-Anchor" href="#问题发现">#</a> 问题发现</h2><p>・大模型（如 GPT-3）参数量极大（1750 亿 +），传统 “预训练 + 微调” 范式成本过高（需为每个任务调整海量参数）。</p><ol><li><p>解决方案：<br>・开发新范式（ICL/Prompt），通过输入指令或示例直接引导模型，避免微调。</p><p>・但要让模型支持这种范式，必须在预训练阶段就赋予它相关能力（如理解指令、模仿示例）。</p></li><li><p>模型构建的关键：<br>・预训练阶段：用海量多样化数据（图书、网页、指令数据等）训练模型，使其隐式掌握 ICL/Prompt 所需的能力（如任务模式识别、指令遵循）。</p><p>・后续阶段（SFT+RLHF）：进一步优化模型对新范式的响应质量（如更精准的指令理解、更安全的输出）。</p></li><li><p>结论：<br>・新范式（ICL/Prompt）依赖特定训练的模型：只有通过大规模预训练（及后续优化）的模型，才能直接通过上下文或指令适配任务，而传统小模型无法做到这一点。</p></li></ol><h2 id="llm的构建流程"><a class="markdownIt-Anchor" href="#llm的构建流程">#</a> LLM 的构建流程</h2><ul><li>预训练： 利用海量训练数据构建多样化内容，构建基础模型 ——&gt; 对长文本建模，使模型具有语言生成能力</li><li>有监督微调 SFT：用少量高质量数据集，通过有监督训练使模型具有问答、写作的能力，数据包括：用户输入提示词和对应理想输出结果</li><li>奖励建模 RM：训练一个能够判断文本质量的裁判，对同个提示词，比较 SFT 生成的多个输出的质量</li><li>强化学习 RLHF (human feedback)：基于 RM，优化 SFT 模型</li></ul><p>SFT 相当于学生学会答题，RM 是评分老师，判断 answer 好坏，RLHF 是学生根据老师评分改进答题策略</p><h2 id="补充"><a class="markdownIt-Anchor" href="#补充">#</a> 补充</h2><p><strong>N-gram 模型详解</strong><br> N-gram 是一种基于统计的语言模型，用于预测或生成文本中的下一个词，其核心思想是：一个词的出现概率依赖于它前面的有限个词（n-1 个词）。它是自然语言处理（NLP）中最基础且广泛使用的模型之一。</p><p>N-gram 的定义：</p><p>・指文本中连续的 <em>n</em> 个词（或字符）组成的序列。</p><p>・例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">◦ Unigram (1-gram): &quot;the&quot;、&quot;cat&quot;、&quot;sat&quot;（单个词）。  </span><br><span class="line"></span><br><span class="line">◦ Bigram (2-gram): &quot;the cat&quot;、&quot;cat sat&quot;、&quot;sat on&quot;（两个连续词）。  </span><br><span class="line"></span><br><span class="line">◦ Trigram (3-gram): &quot;the cat sat&quot;、&quot;cat sat on&quot;（三个连续词）。  </span><br></pre></td></tr></table></figure><p>・核心假设：</p><p>・马尔可夫假设：当前词的概率仅依赖于前 <em>n-1</em> 个词，而非整个历史。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">◦ 例如，Bigram 模型认为 `P(sat | the cat)` ≈ `P(sat | cat)`，忽略更早的上下文。</span><br></pre></td></tr></table></figure><hr><p><strong>如何计算概率？</strong><br>N-gram 通过统计语料库中词序列的频率来估计概率：</p><p>计算  <code>P(sat | the cat)</code> ：</p><p>P(sat∣the cat)=Count(“the cat”)Count(“the cat sat”)</p><p>若语料中 “the cat” 出现 100 次，“the cat sat” 出现 30 次，则  <code>P(sat | the cat) = 0.3</code> 。</p><p><strong>N-gram 的优缺点</strong></p><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>简单高效，计算速度快。</td><td>无法捕捉长距离依赖（如 “The cat… sat” 相隔较远时）。</td></tr><tr><td>小规模数据即可训练。</td><td>数据稀疏性（罕见 n-gram 概率不准确）。</td></tr><tr><td>曾广泛用于机器翻译、拼写检查等任务。</td><td>无法理解语义（仅统计共现频率）。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>resume</title>
      <link href="/2025/05/11/resume/"/>
      <url>/2025/05/11/resume/</url>
      
        <content type="html"><![CDATA[<h1 id="Resume"><a href="#Resume" class="headerlink" title="Resume"></a>Resume</h1><h2 id="教育经历"><a href="#教育经历" class="headerlink" title="教育经历"></a>教育经历</h2><ul><li>SZU 计算机技术 硕士一等奖学金</li><li>GDUFE 计算机科学与技术 学业奖学金  CET-6 蓝桥杯省赛二等奖</li></ul><h2 id="实习经历"><a href="#实习经历" class="headerlink" title="实习经历"></a>实习经历</h2><ul><li>中金财富、望海康信、CVTE、迅策</li></ul><h2 id="项目经历"><a href="#项目经历" class="headerlink" title="项目经历"></a>项目经历</h2><ul><li><p>AI问答系统（实习项目、后台开发）</p><ul><li>项目描述：AI问答系统是基于LLM平台，搭建多轮记忆对话工作流的平台，主要包括文案审查、合同审查、普通对话等功能，为企业的营销部门、HR部门、法务部门提供合规性检查。</li><li>项目所用技术：Django + LLM平台 + Mysql</li><li>基于异步模式实现文档审核批注功能：基于定时任务扫描任务表执行文档审核功能，将同步审核任务异步化，避免接口响应时间过长</li><li>定时任务处理优化：基于线程池+Future类优化文档的审查过程，实现同个文档的多个分块并行执行，将文档审查耗时从平均时间30+s降低至20s</li><li>基于公司LLM应用平台搭建工作流，实现知识库搭建、用户意图识别、分支链路处理、结果返回等功能，通过优化Prompt、定义明确问题分类器、优化知识库分片等方式，提高大模型的处理能力</li></ul></li><li><p>智慧交通平台 （实习项目、后台开发）</p><ul><li>负责智慧交通平台道路信息服务的后端研发</li><li>项目所用技术：SpringBoot + Mybatis + Flink</li><li>参与问题定位开发：使用AOP并结合自定义注解获取全局请求与处理信息，增强API的可追踪性和调试效率</li><li>查询优化：完成3个模块数据表设计，同时基于索引下推机制执行联合索引改造，将回表次数优化为24次</li><li>道路模块开发：负责相关需求开发，基于内部CI&#x2F;CD平台自动化测试流水线，保证在提交测试前各接口单元测试覆盖率达到70%以上，核心链路全覆盖</li></ul></li><li><p>AI智能客服  后端开发</p><ul><li>项目描述：企业级AI智能客服，专为客户支持与服务场景打造。具备多轮对话、上下文理解和知识库检索能力，可处理业务咨询、工单查询及问题诊断，并能智能调用外部工具执行任务，如订单跟踪、网页查询。</li><li>项目技术栈：Spring Boot 3 + Spring AI + RAG + Tool Calling</li><li>多轮对话与持久化：通过Advisor特性实现对话上下文功能，基于ChatMemor与PostgreSQL实现记忆持久性</li><li>RAG向量存储：自定义VectorStore实现，结合EmbeddingModel将文本转换为语义向量并存储到PGvector向量数据库，支持语义相似度搜索和多维度过滤，提高了检索精准度</li><li>流式接口：实现了SSE流式响应接口，并通过CompletableFuture将AI推理任务异步化，避免阻塞主线程</li><li>自主规划智能体：基于ReAct模式构建了具备自主规划能力的分层AI智能体，能够分解任务、自主决策、选择工具、循环执行直到完成复杂任务，架构参考OpenManus，实现高扩展性和可维护性</li><li>AI工具调用：利用Spring AI的工具调用注解实现了多种工具调用功能，扩展了AI的能力边界</li></ul></li><li><p>异构数据源同步平台</p></li><li><p>项目描述：该系统是一个基于Flink的异构数据源流转服务，用来作为数据源之间的数据同步工具，通过抽象异构数据源驱动，同时屏蔽数据源间不同的通信协议，通过页面化配置数据同步任务的方式简化数据库同步流程，并实现定时调度。</p></li><li><p>项目技术栈：Flink + Chunjun + Xxl-Job + Redis + Retrofit2</p></li><li><p>数据源Driver设计：使用工厂设计模式、模板设计模式抽象化数据源的交互逻辑，同时支持灵活扩展数据源</p></li><li><p>流转进度推送：基于Redis stream搭建轻量级消息队列，同时结合SSE实例实现任务流转进度即时刷新</p></li><li><p>任务调度：借助Xxl-Job实现任务调度管理，确保异构数据源的定时同步和实时更新需求，支持批量任务管理</p></li><li><p>数据同步：使用钩子方法覆盖流转任务完整生命周期，并定义模板方法以高度可扩展的方式串联任务生命周期</p></li><li><p>生物大语言模型集成平台</p><ul><li>项目描述：生物语言模型集成平台是一个面向生物医学研究领域的工具类网站，旨在整合实验室研发的多种大语言模型，为研究人员提供便捷的模型调用和数据分析服务，并且还提供实验室研究成果展示等功能。本人负责部分模块后端开发，同时负责工作分配以及把控进度。</li><li>项目技术栈：SpringCloud + SpringBoot + Mybatis</li><li>服务拆分：基于跨语言与团队成员擅长技术的需求，将平台拆分为网关、模型处理、数据管理服务三个独立模块，并结合Nacos注册中心、Feign远程调用技术</li><li>登录与用户管理：利用Spring Cloud Gateway接口，解析和验证JWT令牌，并传递用户信息至下游服务，实现服务间用户信息共享。对用户密码采取BCrypt密码加密方式，有效保障用户账号安全</li><li>代码重构：分析所负责项目中多个相似的查询请求，通过自动化Mapper接口与实体类的映射，同时结合动态构建查询条件，实现了通用查询框架，提高了代码的复用性，减少了至少7个Mapper接口编码工作</li><li>异步执行与存储优化：将模型调用的同步执行操作，结合Mq技术，转化为异步操作，实现平均响应时间从2000ms+降低至300ms内，并基于MinIO部署专用文件存储服务器，实现文件转存，减少本地服务器压力</li></ul></li><li><p>生活服务平台（后端开发成员）</p><ul><li>项目描述：生活服务平台是以用户点评和商户信息为核心，主要实现用户登录、优惠券抢购、文章点赞、发布笔记等功能</li><li>项目技术栈：SpringBoot + Mybatis-Plus + Redis</li><li>登录拦截器：使用Redis存储用户Token实现分布式系统Session共享，搭建拦截链，解决Token刷新与认证</li><li>高频信息缓存预热：使用Redis缓存热门商户信息、文章笔记信息，并采用写删读建短超时的策略保证一致性</li><li>优惠券秒杀：使用乐观锁实现普通优惠券超卖问题，使用Redis+Lua脚本实现限购问题</li></ul></li></ul><h2 id="专业技能"><a href="#专业技能" class="headerlink" title="专业技能"></a>专业技能</h2><ul><li>熟悉Java基础编程，具有良好的面向对象编程思想、熟悉多线程、集合等知识</li><li>熟悉SpringBoot、SpringCloud、Mybatis等开发框架，了解微服务架构以及Nacos、Gateway等组件的使用</li><li>熟悉Mysql数据库的使用，对索引、锁机制、事务、日志、数据库范式有一定理解</li><li>熟悉Redis的使用，熟悉五种常见数据类型，对缓存持久化、缓存穿透、缓存击穿有一定理解</li><li>熟悉Linux系统以及常用命令的使用，对Docker、Kubernetes有一定的理解</li><li>熟悉计算机网络原理，对OSI七层模型、TCP&#x2F;UDP、HTTP&#x2F;HTTPS协议有一定理解</li><li>熟悉操作系统基础知识，对进程调度、内存管理、虚拟内存等有一定理解</li></ul>]]></content>
      
      
      <categories>
          
          <category> information </category>
          
      </categories>
      
      
        <tags>
            
            <tag> resume </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
